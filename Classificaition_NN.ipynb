{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhK93xQqXE8bWx04HYmh1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isi2000/PATTERN/blob/main/Classificaition_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "gZHWFXQdUh2r"
      },
      "outputs": [],
      "source": [
        "#data an and plotting \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#torch \n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms.functional as Ft\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "#working with dirs\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this line of code copies the files in the PATTERN repo\n",
        "#they are hd image so it may take a while\n",
        "!git clone https://github.com/Isi2000/PATTERN.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb-Q4uKaDccv",
        "outputId": "53c95bad-461a-4274-a387-10af4ab909ea"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PATTERN' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET"
      ],
      "metadata": {
        "id": "JGZytXHIB7d0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First the image paths and labels need to be storged in a pd DataFrame object to later be put in a custom made dataset "
      ],
      "metadata": {
        "id": "5vV305iXChEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {0: 'NORMAL', 1:'BACTERIAL', 2:'VIRAL',}\n",
        "\n",
        "data_dir_train = '/content/PATTERN/images/t_all'\n",
        "data_dir_test = '/content/PATTERN/images/test_all'\n",
        "data_dir_val = '/content/PATTERN/images/val_all'\n",
        "\n",
        "d_train = {'img_path': os.listdir(data_dir_train)}\n",
        "d_test = {'img_path': os.listdir(data_dir_test)}\n",
        "d_val = {'img_path': os.listdir(data_dir_val)}\n",
        "\n",
        "df_train = pd.DataFrame(d_train)\n",
        "df_test= pd.DataFrame(d_test)\n",
        "df_val = pd.DataFrame(d_val)\n",
        "\n",
        "df_train['label'] = 0\n",
        "df_test['label'] = 0\n",
        "df_val['label'] = 0\n",
        "\n",
        "#these lines work really well, BE CAREFUL BECAUSE IT GIVES A PD WARNING \n",
        "\n",
        "#DIOCANE\n",
        "df_train.loc[df_train['img_path'].str.contains('virus'), 'label'] = 2\n",
        "df_train.loc[df_train['img_path'].str.contains('bacte'), 'label'] = 1\n",
        "df_test.loc[df_test['img_path'].str.contains('virus'), 'label'] = 2\n",
        "df_test.loc[df_test['img_path'].str.contains('bacte'), 'label'] = 1\n",
        "df_val.loc[df_val['img_path'].str.contains('virus'), 'label'] = 2\n",
        "df_val.loc[df_val['img_path'].str.contains('bacte'), 'label'] = 1\n",
        "\n",
        "print(df_train.head())\n",
        "print(df_test.head())\n",
        "print(df_val.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXK3l7yuAH-A",
        "outputId": "0ebd06e5-79ef-454a-b027-42e774ac1c67"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        img_path  label\n",
            "0      person843_virus_1485.jpeg      2\n",
            "1     person41_bacteria_210.jpeg      1\n",
            "2  person1719_bacteria_4542.jpeg      1\n",
            "3     person27_bacteria_138.jpeg      1\n",
            "4     person1473_virus_2551.jpeg      2\n",
            "                         img_path  label\n",
            "0  NORMAL2-IM-0246-0001-0002.jpeg      0\n",
            "1       NORMAL2-IM-0327-0001.jpeg      0\n",
            "2     person117_bacteria_556.jpeg      1\n",
            "3       NORMAL2-IM-0120-0001.jpeg      0\n",
            "4               IM-0029-0001.jpeg      0\n",
            "                        img_path  label\n",
            "0  person1950_bacteria_4881.jpeg      1\n",
            "1  person1952_bacteria_4883.jpeg      1\n",
            "2  person1946_bacteria_4874.jpeg      1\n",
            "3  person1946_bacteria_4875.jpeg      1\n",
            "4      NORMAL2-IM-1436-0001.jpeg      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation and Custome Image Dataset. The class is written following to pytorch standards \n",
        "(for more info see https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ],
      "metadata": {
        "id": "S3hgRXZyB3Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stands for pixels, it's for resizing\n",
        "\n",
        "pix = 128\n",
        "\n",
        "transform_1 = transforms.Compose(\n",
        "    [transforms.ToPILImage(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Grayscale(),    \n",
        "     #the resizing must be fixed according to training perf\n",
        "     #224 is the standard size for resnets\n",
        "     transforms.Resize(size = (pix, pix), antialias = False),\n",
        "     transforms.Normalize(mean=[0.0], std=[1.0]),\n",
        "     ])\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, pd_df, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "train_ = CustomImageDataset(df_train, data_dir_train, transform = transform_1)\n",
        "test_ = CustomImageDataset(df_test, data_dir_test, transform = transform_1)\n",
        "val_ = CustomImageDataset(df_val, data_dir_val, transform = transform_1)"
      ],
      "metadata": {
        "id": "RnsmFhcMU8nu"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization of the dataset"
      ],
      "metadata": {
        "id": "uK_qEHGNDEWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotter(data):      \n",
        "    \"\"\"this plots 9 images from a dataset\"\"\"\n",
        "    figure = plt.figure(figsize = (8,8))\n",
        "    cols, rows = 5, 5\n",
        "    for i in range(1, cols * rows + 1):\n",
        "        sample_idx = torch.randint(len(data), size=(1,)).item()\n",
        "        img, label = Ft.to_pil_image(data[sample_idx][0]) , data[sample_idx][1]\n",
        "        #print(img.size)\n",
        "        figure.add_subplot(rows, cols, i)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(labels_map[label], fontsize = 10)\n",
        "        plt.imshow(img)        \n",
        "    plt.show()\n",
        "\n",
        "#plotter(train_)"
      ],
      "metadata": {
        "id": "rUUIrqVjBQGr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network and GPU"
      ],
      "metadata": {
        "id": "-_XmqFODIAFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below tells the program to use gpu if available. All the code written is designed to work even if the machine doesn't have a gpu"
      ],
      "metadata": {
        "id": "Dn4lKp0SIvAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Congratulations, you have a GPU!\")\n",
        "else:\n",
        "    print(\"PyTorch cannot see your GPU :(\")\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d66qn5oSDmuy",
        "outputId": "427da0f2-d141-4253-bf66-6d9eeeff81d7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Congratulations, you have a GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network class that defines the model used to classify images. This class inherits from pytorch class nn.Module"
      ],
      "metadata": {
        "id": "5LzGn9zEKDNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, filters: list, units: list, activation=F.leaky_relu, kernel=3):\n",
        "        \n",
        "        super().__init__()\n",
        "        filters = [1] + filters\n",
        "        #takes the last dim of the filters and divides the side by 2^#pools\n",
        "        units = [filters[-1]*(int(pix/8))*(int(pix/8))] + units + [3]\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.fc = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "\n",
        "        # define convolutions\n",
        "        for i, num_filters in enumerate(filters[1:]):\n",
        "            conv = nn.Conv2d(in_channels=filters[i], out_channels=num_filters, \n",
        "                             kernel_size=kernel, padding='same')\n",
        "            \n",
        "\n",
        "            self.convs.append(conv)\n",
        "            \n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "  \n",
        "        # define dense layers\n",
        "        for i, num_units in enumerate(units[1:]):\n",
        "            fc = nn.Linear(units[i], num_units)\n",
        "            self.fc.append(fc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.convs):\n",
        "            x = self.activation(layer(x))\n",
        "            \n",
        "            if i % 2 == 1:\n",
        "                # downsample by 2 on 2nd conv layer\n",
        "                x = self.max_pool(x)\n",
        "        \n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "        for layer in self.fc:\n",
        "            x = self.activation(layer(x))\n",
        "        \n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "dYjcllkhIf4D"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remember that u have to send it to cuda\n",
        "net = Net(filters=[16, 16, 32, 32, 64, 64], units=[128], kernel=3).to(torch.device('cuda'))\n",
        "\n",
        "#I have to set it to cpu, but it does not mean that my model uses cpu!!!\n",
        "summary(net, (1, pix, pix), device = 'cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiwG2B5wJA9s",
        "outputId": "2ecfab61-007a-4db1-bcb7-9716ca88569e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 128, 128]             160\n",
            "            Conv2d-2         [-1, 16, 128, 128]           2,320\n",
            "         MaxPool2d-3           [-1, 16, 64, 64]               0\n",
            "            Conv2d-4           [-1, 32, 64, 64]           4,640\n",
            "            Conv2d-5           [-1, 32, 64, 64]           9,248\n",
            "         MaxPool2d-6           [-1, 32, 32, 32]               0\n",
            "            Conv2d-7           [-1, 64, 32, 32]          18,496\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,928\n",
            "         MaxPool2d-9           [-1, 64, 16, 16]               0\n",
            "           Linear-10                  [-1, 128]       2,097,280\n",
            "           Linear-11                    [-1, 3]             387\n",
            "================================================================\n",
            "Total params: 2,169,459\n",
            "Trainable params: 2,169,459\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 7.88\n",
            "Params size (MB): 8.28\n",
            "Estimated Total Size (MB): 16.21\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-2012b380bb94>:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing"
      ],
      "metadata": {
        "id": "NXe9UXkJUToU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "#Adam may be too strong\n",
        "optimizer = optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "train_loader = DataLoader(train_, batch_size=50, shuffle=True)\n",
        "test_loader = DataLoader(test_, batch_size=50, shuffle=True)\n",
        "val_loader = DataLoader(val_, batch_size=10, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, freq=5):\n",
        "    model.train()\n",
        "    epoch_loss = 0    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # if batch_idx % freq == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "            \n",
        "    return epoch_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)  \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    return test_loss, (100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "q9EfRI2INCXN"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "num_epochs = 20\n",
        "best_params = net.state_dict()\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_losses.append(train(net, device, train_loader, optimizer, epoch))\n",
        "    test_loss, test_accuracy = test(net, device, test_loader)\n",
        "\n",
        "    # model selection\n",
        "    if test_accuracy >= best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_params = net.state_dict()\n",
        "\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Km7VZjCNQY-",
        "outputId": "4b3fb030-3844-4215-90af-34e2d6060a8d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-2012b380bb94>:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/5216 (0%)]\tLoss: 1.101192\n",
            "Train Epoch: 1 [50/5216 (1%)]\tLoss: 1.061301\n",
            "Train Epoch: 1 [100/5216 (2%)]\tLoss: 0.876827\n",
            "Train Epoch: 1 [150/5216 (3%)]\tLoss: 1.425424\n",
            "Train Epoch: 1 [200/5216 (4%)]\tLoss: 1.175755\n",
            "Train Epoch: 1 [250/5216 (5%)]\tLoss: 1.074340\n",
            "Train Epoch: 1 [300/5216 (6%)]\tLoss: 1.037116\n",
            "Train Epoch: 1 [350/5216 (7%)]\tLoss: 1.099887\n",
            "Train Epoch: 1 [400/5216 (8%)]\tLoss: 1.079384\n",
            "Train Epoch: 1 [450/5216 (9%)]\tLoss: 1.083394\n",
            "Train Epoch: 1 [500/5216 (10%)]\tLoss: 1.092935\n",
            "Train Epoch: 1 [550/5216 (10%)]\tLoss: 1.091013\n",
            "Train Epoch: 1 [600/5216 (11%)]\tLoss: 1.086475\n",
            "Train Epoch: 1 [650/5216 (12%)]\tLoss: 1.083498\n",
            "Train Epoch: 1 [700/5216 (13%)]\tLoss: 1.088125\n",
            "Train Epoch: 1 [750/5216 (14%)]\tLoss: 1.085060\n",
            "Train Epoch: 1 [800/5216 (15%)]\tLoss: 1.082073\n",
            "Train Epoch: 1 [850/5216 (16%)]\tLoss: 1.072846\n",
            "Train Epoch: 1 [900/5216 (17%)]\tLoss: 1.040151\n",
            "Train Epoch: 1 [950/5216 (18%)]\tLoss: 1.134405\n",
            "Train Epoch: 1 [1000/5216 (19%)]\tLoss: 0.976269\n",
            "Train Epoch: 1 [1050/5216 (20%)]\tLoss: 1.000554\n",
            "Train Epoch: 1 [1100/5216 (21%)]\tLoss: 1.082727\n",
            "Train Epoch: 1 [1150/5216 (22%)]\tLoss: 0.968102\n",
            "Train Epoch: 1 [1200/5216 (23%)]\tLoss: 1.083084\n",
            "Train Epoch: 1 [1250/5216 (24%)]\tLoss: 1.009749\n",
            "Train Epoch: 1 [1300/5216 (25%)]\tLoss: 1.051187\n",
            "Train Epoch: 1 [1350/5216 (26%)]\tLoss: 1.122085\n",
            "Train Epoch: 1 [1400/5216 (27%)]\tLoss: 1.061975\n",
            "Train Epoch: 1 [1450/5216 (28%)]\tLoss: 1.087518\n",
            "Train Epoch: 1 [1500/5216 (29%)]\tLoss: 1.078398\n",
            "Train Epoch: 1 [1550/5216 (30%)]\tLoss: 1.078463\n",
            "Train Epoch: 1 [1600/5216 (30%)]\tLoss: 1.058386\n",
            "Train Epoch: 1 [1650/5216 (31%)]\tLoss: 1.058739\n",
            "Train Epoch: 1 [1700/5216 (32%)]\tLoss: 1.079650\n",
            "Train Epoch: 1 [1750/5216 (33%)]\tLoss: 1.062455\n",
            "Train Epoch: 1 [1800/5216 (34%)]\tLoss: 1.023057\n",
            "Train Epoch: 1 [1850/5216 (35%)]\tLoss: 1.037528\n",
            "Train Epoch: 1 [1900/5216 (36%)]\tLoss: 1.080525\n",
            "Train Epoch: 1 [1950/5216 (37%)]\tLoss: 1.167699\n",
            "Train Epoch: 1 [2000/5216 (38%)]\tLoss: 0.992979\n",
            "Train Epoch: 1 [2050/5216 (39%)]\tLoss: 1.078261\n",
            "Train Epoch: 1 [2100/5216 (40%)]\tLoss: 1.063771\n",
            "Train Epoch: 1 [2150/5216 (41%)]\tLoss: 1.052337\n",
            "Train Epoch: 1 [2200/5216 (42%)]\tLoss: 0.975470\n",
            "Train Epoch: 1 [2250/5216 (43%)]\tLoss: 0.962015\n",
            "Train Epoch: 1 [2300/5216 (44%)]\tLoss: 1.150094\n",
            "Train Epoch: 1 [2350/5216 (45%)]\tLoss: 1.036264\n",
            "Train Epoch: 1 [2400/5216 (46%)]\tLoss: 0.959168\n",
            "Train Epoch: 1 [2450/5216 (47%)]\tLoss: 0.965823\n",
            "Train Epoch: 1 [2500/5216 (48%)]\tLoss: 1.069238\n",
            "Train Epoch: 1 [2550/5216 (49%)]\tLoss: 1.040791\n",
            "Train Epoch: 1 [2600/5216 (50%)]\tLoss: 1.061203\n",
            "Train Epoch: 1 [2650/5216 (50%)]\tLoss: 1.111417\n",
            "Train Epoch: 1 [2700/5216 (51%)]\tLoss: 1.067187\n",
            "Train Epoch: 1 [2750/5216 (52%)]\tLoss: 1.109593\n",
            "Train Epoch: 1 [2800/5216 (53%)]\tLoss: 1.033248\n",
            "Train Epoch: 1 [2850/5216 (54%)]\tLoss: 1.049603\n",
            "Train Epoch: 1 [2900/5216 (55%)]\tLoss: 0.997953\n",
            "Train Epoch: 1 [2950/5216 (56%)]\tLoss: 1.082980\n",
            "Train Epoch: 1 [3000/5216 (57%)]\tLoss: 1.084382\n",
            "Train Epoch: 1 [3050/5216 (58%)]\tLoss: 1.073496\n",
            "Train Epoch: 1 [3100/5216 (59%)]\tLoss: 1.029052\n",
            "Train Epoch: 1 [3150/5216 (60%)]\tLoss: 1.032373\n",
            "Train Epoch: 1 [3200/5216 (61%)]\tLoss: 1.023740\n",
            "Train Epoch: 1 [3250/5216 (62%)]\tLoss: 1.040318\n",
            "Train Epoch: 1 [3300/5216 (63%)]\tLoss: 1.015659\n",
            "Train Epoch: 1 [3350/5216 (64%)]\tLoss: 0.906412\n",
            "Train Epoch: 1 [3400/5216 (65%)]\tLoss: 1.092005\n",
            "Train Epoch: 1 [3450/5216 (66%)]\tLoss: 0.994119\n",
            "Train Epoch: 1 [3500/5216 (67%)]\tLoss: 0.941544\n",
            "Train Epoch: 1 [3550/5216 (68%)]\tLoss: 0.900094\n",
            "Train Epoch: 1 [3600/5216 (69%)]\tLoss: 0.993357\n",
            "Train Epoch: 1 [3650/5216 (70%)]\tLoss: 0.812752\n",
            "Train Epoch: 1 [3700/5216 (70%)]\tLoss: 0.974668\n",
            "Train Epoch: 1 [3750/5216 (71%)]\tLoss: 0.714523\n",
            "Train Epoch: 1 [3800/5216 (72%)]\tLoss: 1.007842\n",
            "Train Epoch: 1 [3850/5216 (73%)]\tLoss: 0.879918\n",
            "Train Epoch: 1 [3900/5216 (74%)]\tLoss: 0.950780\n",
            "Train Epoch: 1 [3950/5216 (75%)]\tLoss: 0.920653\n",
            "Train Epoch: 1 [4000/5216 (76%)]\tLoss: 0.994454\n",
            "Train Epoch: 1 [4050/5216 (77%)]\tLoss: 0.867395\n",
            "Train Epoch: 1 [4100/5216 (78%)]\tLoss: 0.826852\n",
            "Train Epoch: 1 [4150/5216 (79%)]\tLoss: 0.845511\n",
            "Train Epoch: 1 [4200/5216 (80%)]\tLoss: 0.937969\n",
            "Train Epoch: 1 [4250/5216 (81%)]\tLoss: 0.901629\n",
            "Train Epoch: 1 [4300/5216 (82%)]\tLoss: 0.965682\n",
            "Train Epoch: 1 [4350/5216 (83%)]\tLoss: 0.852182\n",
            "Train Epoch: 1 [4400/5216 (84%)]\tLoss: 0.814490\n",
            "Train Epoch: 1 [4450/5216 (85%)]\tLoss: 0.786549\n",
            "Train Epoch: 1 [4500/5216 (86%)]\tLoss: 0.924258\n",
            "Train Epoch: 1 [4550/5216 (87%)]\tLoss: 0.780704\n",
            "Train Epoch: 1 [4600/5216 (88%)]\tLoss: 0.886626\n",
            "Train Epoch: 1 [4650/5216 (89%)]\tLoss: 0.854622\n",
            "Train Epoch: 1 [4700/5216 (90%)]\tLoss: 0.949369\n",
            "Train Epoch: 1 [4750/5216 (90%)]\tLoss: 1.000011\n",
            "Train Epoch: 1 [4800/5216 (91%)]\tLoss: 1.009883\n",
            "Train Epoch: 1 [4850/5216 (92%)]\tLoss: 0.890948\n",
            "Train Epoch: 1 [4900/5216 (93%)]\tLoss: 0.866160\n",
            "Train Epoch: 1 [4950/5216 (94%)]\tLoss: 0.999972\n",
            "Train Epoch: 1 [5000/5216 (95%)]\tLoss: 0.854546\n",
            "Train Epoch: 1 [5050/5216 (96%)]\tLoss: 0.859041\n",
            "Train Epoch: 1 [5100/5216 (97%)]\tLoss: 0.896237\n",
            "Train Epoch: 1 [5150/5216 (98%)]\tLoss: 0.829459\n",
            "Train Epoch: 1 [1664/5216 (99%)]\tLoss: 0.970941\n",
            "\n",
            "Test set: Average loss: 0.9794, Accuracy: 311/624 (50%)\n",
            "\n",
            "Train Epoch: 2 [0/5216 (0%)]\tLoss: 0.878623\n",
            "Train Epoch: 2 [50/5216 (1%)]\tLoss: 0.958488\n",
            "Train Epoch: 2 [100/5216 (2%)]\tLoss: 0.916308\n",
            "Train Epoch: 2 [150/5216 (3%)]\tLoss: 0.872707\n",
            "Train Epoch: 2 [200/5216 (4%)]\tLoss: 0.838914\n",
            "Train Epoch: 2 [250/5216 (5%)]\tLoss: 0.897057\n",
            "Train Epoch: 2 [300/5216 (6%)]\tLoss: 0.959712\n",
            "Train Epoch: 2 [350/5216 (7%)]\tLoss: 1.037938\n",
            "Train Epoch: 2 [400/5216 (8%)]\tLoss: 0.929894\n",
            "Train Epoch: 2 [450/5216 (9%)]\tLoss: 0.900753\n",
            "Train Epoch: 2 [500/5216 (10%)]\tLoss: 0.820974\n",
            "Train Epoch: 2 [550/5216 (10%)]\tLoss: 0.792093\n",
            "Train Epoch: 2 [600/5216 (11%)]\tLoss: 0.993982\n",
            "Train Epoch: 2 [650/5216 (12%)]\tLoss: 0.957207\n",
            "Train Epoch: 2 [700/5216 (13%)]\tLoss: 0.966308\n",
            "Train Epoch: 2 [750/5216 (14%)]\tLoss: 0.887159\n",
            "Train Epoch: 2 [800/5216 (15%)]\tLoss: 0.659877\n",
            "Train Epoch: 2 [850/5216 (16%)]\tLoss: 0.851770\n",
            "Train Epoch: 2 [900/5216 (17%)]\tLoss: 0.901836\n",
            "Train Epoch: 2 [950/5216 (18%)]\tLoss: 0.824455\n",
            "Train Epoch: 2 [1000/5216 (19%)]\tLoss: 0.838566\n",
            "Train Epoch: 2 [1050/5216 (20%)]\tLoss: 0.777213\n",
            "Train Epoch: 2 [1100/5216 (21%)]\tLoss: 0.927089\n",
            "Train Epoch: 2 [1150/5216 (22%)]\tLoss: 1.014797\n",
            "Train Epoch: 2 [1200/5216 (23%)]\tLoss: 0.803601\n",
            "Train Epoch: 2 [1250/5216 (24%)]\tLoss: 0.839607\n",
            "Train Epoch: 2 [1300/5216 (25%)]\tLoss: 0.745660\n",
            "Train Epoch: 2 [1350/5216 (26%)]\tLoss: 0.809906\n",
            "Train Epoch: 2 [1400/5216 (27%)]\tLoss: 0.825559\n",
            "Train Epoch: 2 [1450/5216 (28%)]\tLoss: 0.701223\n",
            "Train Epoch: 2 [1500/5216 (29%)]\tLoss: 0.805641\n",
            "Train Epoch: 2 [1550/5216 (30%)]\tLoss: 0.913645\n",
            "Train Epoch: 2 [1600/5216 (30%)]\tLoss: 1.050855\n",
            "Train Epoch: 2 [1650/5216 (31%)]\tLoss: 0.915404\n",
            "Train Epoch: 2 [1700/5216 (32%)]\tLoss: 1.012437\n",
            "Train Epoch: 2 [1750/5216 (33%)]\tLoss: 1.010404\n",
            "Train Epoch: 2 [1800/5216 (34%)]\tLoss: 0.972515\n",
            "Train Epoch: 2 [1850/5216 (35%)]\tLoss: 0.900479\n",
            "Train Epoch: 2 [1900/5216 (36%)]\tLoss: 0.862588\n",
            "Train Epoch: 2 [1950/5216 (37%)]\tLoss: 0.930621\n",
            "Train Epoch: 2 [2000/5216 (38%)]\tLoss: 0.883953\n",
            "Train Epoch: 2 [2050/5216 (39%)]\tLoss: 0.821934\n",
            "Train Epoch: 2 [2100/5216 (40%)]\tLoss: 0.978779\n",
            "Train Epoch: 2 [2150/5216 (41%)]\tLoss: 0.879729\n",
            "Train Epoch: 2 [2200/5216 (42%)]\tLoss: 0.868848\n",
            "Train Epoch: 2 [2250/5216 (43%)]\tLoss: 1.082866\n",
            "Train Epoch: 2 [2300/5216 (44%)]\tLoss: 0.695345\n",
            "Train Epoch: 2 [2350/5216 (45%)]\tLoss: 0.839449\n",
            "Train Epoch: 2 [2400/5216 (46%)]\tLoss: 0.893171\n",
            "Train Epoch: 2 [2450/5216 (47%)]\tLoss: 0.847803\n",
            "Train Epoch: 2 [2500/5216 (48%)]\tLoss: 0.821570\n",
            "Train Epoch: 2 [2550/5216 (49%)]\tLoss: 0.841511\n",
            "Train Epoch: 2 [2600/5216 (50%)]\tLoss: 0.948241\n",
            "Train Epoch: 2 [2650/5216 (50%)]\tLoss: 0.835462\n",
            "Train Epoch: 2 [2700/5216 (51%)]\tLoss: 0.869558\n",
            "Train Epoch: 2 [2750/5216 (52%)]\tLoss: 0.962281\n",
            "Train Epoch: 2 [2800/5216 (53%)]\tLoss: 0.882701\n",
            "Train Epoch: 2 [2850/5216 (54%)]\tLoss: 0.882121\n",
            "Train Epoch: 2 [2900/5216 (55%)]\tLoss: 1.098332\n",
            "Train Epoch: 2 [2950/5216 (56%)]\tLoss: 0.904973\n",
            "Train Epoch: 2 [3000/5216 (57%)]\tLoss: 0.855741\n",
            "Train Epoch: 2 [3050/5216 (58%)]\tLoss: 0.798169\n",
            "Train Epoch: 2 [3100/5216 (59%)]\tLoss: 0.888196\n",
            "Train Epoch: 2 [3150/5216 (60%)]\tLoss: 0.795252\n",
            "Train Epoch: 2 [3200/5216 (61%)]\tLoss: 0.799619\n",
            "Train Epoch: 2 [3250/5216 (62%)]\tLoss: 0.781443\n",
            "Train Epoch: 2 [3300/5216 (63%)]\tLoss: 0.853228\n",
            "Train Epoch: 2 [3350/5216 (64%)]\tLoss: 0.825260\n",
            "Train Epoch: 2 [3400/5216 (65%)]\tLoss: 0.817017\n",
            "Train Epoch: 2 [3450/5216 (66%)]\tLoss: 1.078758\n",
            "Train Epoch: 2 [3500/5216 (67%)]\tLoss: 0.730229\n",
            "Train Epoch: 2 [3550/5216 (68%)]\tLoss: 0.777188\n",
            "Train Epoch: 2 [3600/5216 (69%)]\tLoss: 0.711005\n",
            "Train Epoch: 2 [3650/5216 (70%)]\tLoss: 0.932468\n",
            "Train Epoch: 2 [3700/5216 (70%)]\tLoss: 0.684557\n",
            "Train Epoch: 2 [3750/5216 (71%)]\tLoss: 0.652659\n",
            "Train Epoch: 2 [3800/5216 (72%)]\tLoss: 0.813531\n",
            "Train Epoch: 2 [3850/5216 (73%)]\tLoss: 0.889683\n",
            "Train Epoch: 2 [3900/5216 (74%)]\tLoss: 0.841507\n",
            "Train Epoch: 2 [3950/5216 (75%)]\tLoss: 0.729945\n",
            "Train Epoch: 2 [4000/5216 (76%)]\tLoss: 0.759543\n",
            "Train Epoch: 2 [4050/5216 (77%)]\tLoss: 0.957146\n",
            "Train Epoch: 2 [4100/5216 (78%)]\tLoss: 0.682999\n",
            "Train Epoch: 2 [4150/5216 (79%)]\tLoss: 0.822114\n",
            "Train Epoch: 2 [4200/5216 (80%)]\tLoss: 0.624695\n",
            "Train Epoch: 2 [4250/5216 (81%)]\tLoss: 0.854981\n",
            "Train Epoch: 2 [4300/5216 (82%)]\tLoss: 0.724387\n",
            "Train Epoch: 2 [4350/5216 (83%)]\tLoss: 0.636099\n",
            "Train Epoch: 2 [4400/5216 (84%)]\tLoss: 0.596822\n",
            "Train Epoch: 2 [4450/5216 (85%)]\tLoss: 0.804417\n",
            "Train Epoch: 2 [4500/5216 (86%)]\tLoss: 0.808393\n",
            "Train Epoch: 2 [4550/5216 (87%)]\tLoss: 0.778040\n",
            "Train Epoch: 2 [4600/5216 (88%)]\tLoss: 0.583977\n",
            "Train Epoch: 2 [4650/5216 (89%)]\tLoss: 0.731046\n",
            "Train Epoch: 2 [4700/5216 (90%)]\tLoss: 0.727975\n",
            "Train Epoch: 2 [4750/5216 (90%)]\tLoss: 0.729710\n",
            "Train Epoch: 2 [4800/5216 (91%)]\tLoss: 0.638292\n",
            "Train Epoch: 2 [4850/5216 (92%)]\tLoss: 0.764361\n",
            "Train Epoch: 2 [4900/5216 (93%)]\tLoss: 0.678156\n",
            "Train Epoch: 2 [4950/5216 (94%)]\tLoss: 0.740515\n",
            "Train Epoch: 2 [5000/5216 (95%)]\tLoss: 0.934883\n",
            "Train Epoch: 2 [5050/5216 (96%)]\tLoss: 0.668536\n",
            "Train Epoch: 2 [5100/5216 (97%)]\tLoss: 0.527860\n",
            "Train Epoch: 2 [5150/5216 (98%)]\tLoss: 0.570154\n",
            "Train Epoch: 2 [1664/5216 (99%)]\tLoss: 1.097042\n",
            "\n",
            "Test set: Average loss: 1.1620, Accuracy: 327/624 (52%)\n",
            "\n",
            "Train Epoch: 3 [0/5216 (0%)]\tLoss: 0.716324\n",
            "Train Epoch: 3 [50/5216 (1%)]\tLoss: 0.625852\n",
            "Train Epoch: 3 [100/5216 (2%)]\tLoss: 0.686422\n",
            "Train Epoch: 3 [150/5216 (3%)]\tLoss: 0.789306\n",
            "Train Epoch: 3 [200/5216 (4%)]\tLoss: 0.693858\n",
            "Train Epoch: 3 [250/5216 (5%)]\tLoss: 0.651890\n",
            "Train Epoch: 3 [300/5216 (6%)]\tLoss: 0.670678\n",
            "Train Epoch: 3 [350/5216 (7%)]\tLoss: 0.629711\n",
            "Train Epoch: 3 [400/5216 (8%)]\tLoss: 0.647729\n",
            "Train Epoch: 3 [450/5216 (9%)]\tLoss: 0.590879\n",
            "Train Epoch: 3 [500/5216 (10%)]\tLoss: 0.710702\n",
            "Train Epoch: 3 [550/5216 (10%)]\tLoss: 0.777670\n",
            "Train Epoch: 3 [600/5216 (11%)]\tLoss: 0.708143\n",
            "Train Epoch: 3 [650/5216 (12%)]\tLoss: 0.664961\n",
            "Train Epoch: 3 [700/5216 (13%)]\tLoss: 0.528613\n",
            "Train Epoch: 3 [750/5216 (14%)]\tLoss: 0.722686\n",
            "Train Epoch: 3 [800/5216 (15%)]\tLoss: 0.677439\n",
            "Train Epoch: 3 [850/5216 (16%)]\tLoss: 0.578025\n",
            "Train Epoch: 3 [900/5216 (17%)]\tLoss: 0.554723\n",
            "Train Epoch: 3 [950/5216 (18%)]\tLoss: 0.525959\n",
            "Train Epoch: 3 [1000/5216 (19%)]\tLoss: 0.488238\n",
            "Train Epoch: 3 [1050/5216 (20%)]\tLoss: 0.617033\n",
            "Train Epoch: 3 [1100/5216 (21%)]\tLoss: 0.563389\n",
            "Train Epoch: 3 [1150/5216 (22%)]\tLoss: 0.659318\n",
            "Train Epoch: 3 [1200/5216 (23%)]\tLoss: 0.682005\n",
            "Train Epoch: 3 [1250/5216 (24%)]\tLoss: 0.587599\n",
            "Train Epoch: 3 [1300/5216 (25%)]\tLoss: 0.755883\n",
            "Train Epoch: 3 [1350/5216 (26%)]\tLoss: 0.766462\n",
            "Train Epoch: 3 [1400/5216 (27%)]\tLoss: 0.475211\n",
            "Train Epoch: 3 [1450/5216 (28%)]\tLoss: 0.585298\n",
            "Train Epoch: 3 [1500/5216 (29%)]\tLoss: 0.599071\n",
            "Train Epoch: 3 [1550/5216 (30%)]\tLoss: 0.609097\n",
            "Train Epoch: 3 [1600/5216 (30%)]\tLoss: 0.822906\n",
            "Train Epoch: 3 [1650/5216 (31%)]\tLoss: 0.647726\n",
            "Train Epoch: 3 [1700/5216 (32%)]\tLoss: 0.623098\n",
            "Train Epoch: 3 [1750/5216 (33%)]\tLoss: 0.594947\n",
            "Train Epoch: 3 [1800/5216 (34%)]\tLoss: 0.662541\n",
            "Train Epoch: 3 [1850/5216 (35%)]\tLoss: 0.568509\n",
            "Train Epoch: 3 [1900/5216 (36%)]\tLoss: 0.596824\n",
            "Train Epoch: 3 [1950/5216 (37%)]\tLoss: 0.677558\n",
            "Train Epoch: 3 [2000/5216 (38%)]\tLoss: 0.482453\n",
            "Train Epoch: 3 [2050/5216 (39%)]\tLoss: 0.616147\n",
            "Train Epoch: 3 [2100/5216 (40%)]\tLoss: 0.624959\n",
            "Train Epoch: 3 [2150/5216 (41%)]\tLoss: 0.615853\n",
            "Train Epoch: 3 [2200/5216 (42%)]\tLoss: 0.632745\n",
            "Train Epoch: 3 [2250/5216 (43%)]\tLoss: 0.628249\n",
            "Train Epoch: 3 [2300/5216 (44%)]\tLoss: 0.561156\n",
            "Train Epoch: 3 [2350/5216 (45%)]\tLoss: 0.767674\n",
            "Train Epoch: 3 [2400/5216 (46%)]\tLoss: 0.655730\n",
            "Train Epoch: 3 [2450/5216 (47%)]\tLoss: 0.512603\n",
            "Train Epoch: 3 [2500/5216 (48%)]\tLoss: 0.486176\n",
            "Train Epoch: 3 [2550/5216 (49%)]\tLoss: 0.836031\n",
            "Train Epoch: 3 [2600/5216 (50%)]\tLoss: 0.792480\n",
            "Train Epoch: 3 [2650/5216 (50%)]\tLoss: 0.479462\n",
            "Train Epoch: 3 [2700/5216 (51%)]\tLoss: 0.621359\n",
            "Train Epoch: 3 [2750/5216 (52%)]\tLoss: 0.783300\n",
            "Train Epoch: 3 [2800/5216 (53%)]\tLoss: 0.702974\n",
            "Train Epoch: 3 [2850/5216 (54%)]\tLoss: 0.676760\n",
            "Train Epoch: 3 [2900/5216 (55%)]\tLoss: 0.616805\n",
            "Train Epoch: 3 [2950/5216 (56%)]\tLoss: 0.486622\n",
            "Train Epoch: 3 [3000/5216 (57%)]\tLoss: 0.693175\n",
            "Train Epoch: 3 [3050/5216 (58%)]\tLoss: 0.440513\n",
            "Train Epoch: 3 [3100/5216 (59%)]\tLoss: 0.620340\n",
            "Train Epoch: 3 [3150/5216 (60%)]\tLoss: 0.648414\n",
            "Train Epoch: 3 [3200/5216 (61%)]\tLoss: 0.673264\n",
            "Train Epoch: 3 [3250/5216 (62%)]\tLoss: 0.603632\n",
            "Train Epoch: 3 [3300/5216 (63%)]\tLoss: 0.538632\n",
            "Train Epoch: 3 [3350/5216 (64%)]\tLoss: 0.693858\n",
            "Train Epoch: 3 [3400/5216 (65%)]\tLoss: 0.481522\n",
            "Train Epoch: 3 [3450/5216 (66%)]\tLoss: 0.443720\n",
            "Train Epoch: 3 [3500/5216 (67%)]\tLoss: 0.460830\n",
            "Train Epoch: 3 [3550/5216 (68%)]\tLoss: 0.554860\n",
            "Train Epoch: 3 [3600/5216 (69%)]\tLoss: 0.644929\n",
            "Train Epoch: 3 [3650/5216 (70%)]\tLoss: 0.468429\n",
            "Train Epoch: 3 [3700/5216 (70%)]\tLoss: 0.542809\n",
            "Train Epoch: 3 [3750/5216 (71%)]\tLoss: 0.509515\n",
            "Train Epoch: 3 [3800/5216 (72%)]\tLoss: 0.651091\n",
            "Train Epoch: 3 [3850/5216 (73%)]\tLoss: 0.549375\n",
            "Train Epoch: 3 [3900/5216 (74%)]\tLoss: 0.406677\n",
            "Train Epoch: 3 [3950/5216 (75%)]\tLoss: 0.718672\n",
            "Train Epoch: 3 [4000/5216 (76%)]\tLoss: 0.574602\n",
            "Train Epoch: 3 [4050/5216 (77%)]\tLoss: 0.432120\n",
            "Train Epoch: 3 [4100/5216 (78%)]\tLoss: 0.486109\n",
            "Train Epoch: 3 [4150/5216 (79%)]\tLoss: 0.794052\n",
            "Train Epoch: 3 [4200/5216 (80%)]\tLoss: 0.558448\n",
            "Train Epoch: 3 [4250/5216 (81%)]\tLoss: 0.443042\n",
            "Train Epoch: 3 [4300/5216 (82%)]\tLoss: 0.383631\n",
            "Train Epoch: 3 [4350/5216 (83%)]\tLoss: 0.588947\n",
            "Train Epoch: 3 [4400/5216 (84%)]\tLoss: 0.479299\n",
            "Train Epoch: 3 [4450/5216 (85%)]\tLoss: 0.639192\n",
            "Train Epoch: 3 [4500/5216 (86%)]\tLoss: 0.555374\n",
            "Train Epoch: 3 [4550/5216 (87%)]\tLoss: 0.548712\n",
            "Train Epoch: 3 [4600/5216 (88%)]\tLoss: 0.642801\n",
            "Train Epoch: 3 [4650/5216 (89%)]\tLoss: 0.560107\n",
            "Train Epoch: 3 [4700/5216 (90%)]\tLoss: 0.401502\n",
            "Train Epoch: 3 [4750/5216 (90%)]\tLoss: 0.457719\n",
            "Train Epoch: 3 [4800/5216 (91%)]\tLoss: 0.492363\n",
            "Train Epoch: 3 [4850/5216 (92%)]\tLoss: 0.548925\n",
            "Train Epoch: 3 [4900/5216 (93%)]\tLoss: 0.601780\n",
            "Train Epoch: 3 [4950/5216 (94%)]\tLoss: 0.515825\n",
            "Train Epoch: 3 [5000/5216 (95%)]\tLoss: 0.421818\n",
            "Train Epoch: 3 [5050/5216 (96%)]\tLoss: 0.398503\n",
            "Train Epoch: 3 [5100/5216 (97%)]\tLoss: 0.481205\n",
            "Train Epoch: 3 [5150/5216 (98%)]\tLoss: 0.699404\n",
            "Train Epoch: 3 [1664/5216 (99%)]\tLoss: 0.453193\n",
            "\n",
            "Test set: Average loss: 0.9198, Accuracy: 435/624 (70%)\n",
            "\n",
            "Train Epoch: 4 [0/5216 (0%)]\tLoss: 0.492669\n",
            "Train Epoch: 4 [50/5216 (1%)]\tLoss: 0.529740\n",
            "Train Epoch: 4 [100/5216 (2%)]\tLoss: 0.596481\n",
            "Train Epoch: 4 [150/5216 (3%)]\tLoss: 0.580272\n",
            "Train Epoch: 4 [200/5216 (4%)]\tLoss: 0.544075\n",
            "Train Epoch: 4 [250/5216 (5%)]\tLoss: 0.594278\n",
            "Train Epoch: 4 [300/5216 (6%)]\tLoss: 0.565619\n",
            "Train Epoch: 4 [350/5216 (7%)]\tLoss: 0.657712\n",
            "Train Epoch: 4 [400/5216 (8%)]\tLoss: 0.632695\n",
            "Train Epoch: 4 [450/5216 (9%)]\tLoss: 0.457319\n",
            "Train Epoch: 4 [500/5216 (10%)]\tLoss: 0.695810\n",
            "Train Epoch: 4 [550/5216 (10%)]\tLoss: 0.445667\n",
            "Train Epoch: 4 [600/5216 (11%)]\tLoss: 0.527299\n",
            "Train Epoch: 4 [650/5216 (12%)]\tLoss: 0.443844\n",
            "Train Epoch: 4 [700/5216 (13%)]\tLoss: 0.480773\n",
            "Train Epoch: 4 [750/5216 (14%)]\tLoss: 0.466532\n",
            "Train Epoch: 4 [800/5216 (15%)]\tLoss: 0.408247\n",
            "Train Epoch: 4 [850/5216 (16%)]\tLoss: 0.456235\n",
            "Train Epoch: 4 [900/5216 (17%)]\tLoss: 0.541132\n",
            "Train Epoch: 4 [950/5216 (18%)]\tLoss: 0.542864\n",
            "Train Epoch: 4 [1000/5216 (19%)]\tLoss: 0.402543\n",
            "Train Epoch: 4 [1050/5216 (20%)]\tLoss: 0.452770\n",
            "Train Epoch: 4 [1100/5216 (21%)]\tLoss: 0.471880\n",
            "Train Epoch: 4 [1150/5216 (22%)]\tLoss: 0.471540\n",
            "Train Epoch: 4 [1200/5216 (23%)]\tLoss: 0.647829\n",
            "Train Epoch: 4 [1250/5216 (24%)]\tLoss: 0.525877\n",
            "Train Epoch: 4 [1300/5216 (25%)]\tLoss: 0.636380\n",
            "Train Epoch: 4 [1350/5216 (26%)]\tLoss: 0.347037\n",
            "Train Epoch: 4 [1400/5216 (27%)]\tLoss: 0.497391\n",
            "Train Epoch: 4 [1450/5216 (28%)]\tLoss: 0.514966\n",
            "Train Epoch: 4 [1500/5216 (29%)]\tLoss: 0.597221\n",
            "Train Epoch: 4 [1550/5216 (30%)]\tLoss: 0.389304\n",
            "Train Epoch: 4 [1600/5216 (30%)]\tLoss: 0.562225\n",
            "Train Epoch: 4 [1650/5216 (31%)]\tLoss: 0.714791\n",
            "Train Epoch: 4 [1700/5216 (32%)]\tLoss: 0.592136\n",
            "Train Epoch: 4 [1750/5216 (33%)]\tLoss: 0.380570\n",
            "Train Epoch: 4 [1800/5216 (34%)]\tLoss: 0.616112\n",
            "Train Epoch: 4 [1850/5216 (35%)]\tLoss: 0.546647\n",
            "Train Epoch: 4 [1900/5216 (36%)]\tLoss: 0.551728\n",
            "Train Epoch: 4 [1950/5216 (37%)]\tLoss: 0.554016\n",
            "Train Epoch: 4 [2000/5216 (38%)]\tLoss: 0.535424\n",
            "Train Epoch: 4 [2050/5216 (39%)]\tLoss: 0.574138\n",
            "Train Epoch: 4 [2100/5216 (40%)]\tLoss: 0.536503\n",
            "Train Epoch: 4 [2150/5216 (41%)]\tLoss: 0.429830\n",
            "Train Epoch: 4 [2200/5216 (42%)]\tLoss: 0.696344\n",
            "Train Epoch: 4 [2250/5216 (43%)]\tLoss: 0.609407\n",
            "Train Epoch: 4 [2300/5216 (44%)]\tLoss: 0.471930\n",
            "Train Epoch: 4 [2350/5216 (45%)]\tLoss: 0.468031\n",
            "Train Epoch: 4 [2400/5216 (46%)]\tLoss: 0.472920\n",
            "Train Epoch: 4 [2450/5216 (47%)]\tLoss: 0.555771\n",
            "Train Epoch: 4 [2500/5216 (48%)]\tLoss: 0.567227\n",
            "Train Epoch: 4 [2550/5216 (49%)]\tLoss: 0.549278\n",
            "Train Epoch: 4 [2600/5216 (50%)]\tLoss: 0.415704\n",
            "Train Epoch: 4 [2650/5216 (50%)]\tLoss: 0.476372\n",
            "Train Epoch: 4 [2700/5216 (51%)]\tLoss: 0.334021\n",
            "Train Epoch: 4 [2750/5216 (52%)]\tLoss: 0.566765\n",
            "Train Epoch: 4 [2800/5216 (53%)]\tLoss: 0.582536\n",
            "Train Epoch: 4 [2850/5216 (54%)]\tLoss: 0.562758\n",
            "Train Epoch: 4 [2900/5216 (55%)]\tLoss: 0.479365\n",
            "Train Epoch: 4 [2950/5216 (56%)]\tLoss: 0.445373\n",
            "Train Epoch: 4 [3000/5216 (57%)]\tLoss: 0.416284\n",
            "Train Epoch: 4 [3050/5216 (58%)]\tLoss: 0.536292\n",
            "Train Epoch: 4 [3100/5216 (59%)]\tLoss: 0.624410\n",
            "Train Epoch: 4 [3150/5216 (60%)]\tLoss: 0.581433\n",
            "Train Epoch: 4 [3200/5216 (61%)]\tLoss: 0.495596\n",
            "Train Epoch: 4 [3250/5216 (62%)]\tLoss: 0.375515\n",
            "Train Epoch: 4 [3300/5216 (63%)]\tLoss: 0.342778\n",
            "Train Epoch: 4 [3350/5216 (64%)]\tLoss: 0.372905\n",
            "Train Epoch: 4 [3400/5216 (65%)]\tLoss: 0.512025\n",
            "Train Epoch: 4 [3450/5216 (66%)]\tLoss: 0.433201\n",
            "Train Epoch: 4 [3500/5216 (67%)]\tLoss: 0.481802\n",
            "Train Epoch: 4 [3550/5216 (68%)]\tLoss: 0.487026\n",
            "Train Epoch: 4 [3600/5216 (69%)]\tLoss: 0.405612\n",
            "Train Epoch: 4 [3650/5216 (70%)]\tLoss: 0.599987\n",
            "Train Epoch: 4 [3700/5216 (70%)]\tLoss: 0.523311\n",
            "Train Epoch: 4 [3750/5216 (71%)]\tLoss: 0.621254\n",
            "Train Epoch: 4 [3800/5216 (72%)]\tLoss: 0.667743\n",
            "Train Epoch: 4 [3850/5216 (73%)]\tLoss: 0.539636\n",
            "Train Epoch: 4 [3900/5216 (74%)]\tLoss: 0.442431\n",
            "Train Epoch: 4 [3950/5216 (75%)]\tLoss: 0.506995\n",
            "Train Epoch: 4 [4000/5216 (76%)]\tLoss: 0.384218\n",
            "Train Epoch: 4 [4050/5216 (77%)]\tLoss: 0.463151\n",
            "Train Epoch: 4 [4100/5216 (78%)]\tLoss: 0.554960\n",
            "Train Epoch: 4 [4150/5216 (79%)]\tLoss: 0.641398\n",
            "Train Epoch: 4 [4200/5216 (80%)]\tLoss: 0.507821\n",
            "Train Epoch: 4 [4250/5216 (81%)]\tLoss: 0.405966\n",
            "Train Epoch: 4 [4300/5216 (82%)]\tLoss: 0.377070\n",
            "Train Epoch: 4 [4350/5216 (83%)]\tLoss: 0.435027\n",
            "Train Epoch: 4 [4400/5216 (84%)]\tLoss: 0.495124\n",
            "Train Epoch: 4 [4450/5216 (85%)]\tLoss: 0.426968\n",
            "Train Epoch: 4 [4500/5216 (86%)]\tLoss: 0.527795\n",
            "Train Epoch: 4 [4550/5216 (87%)]\tLoss: 0.598198\n",
            "Train Epoch: 4 [4600/5216 (88%)]\tLoss: 0.457745\n",
            "Train Epoch: 4 [4650/5216 (89%)]\tLoss: 0.612204\n",
            "Train Epoch: 4 [4700/5216 (90%)]\tLoss: 0.561716\n",
            "Train Epoch: 4 [4750/5216 (90%)]\tLoss: 0.546547\n",
            "Train Epoch: 4 [4800/5216 (91%)]\tLoss: 0.366519\n",
            "Train Epoch: 4 [4850/5216 (92%)]\tLoss: 0.403802\n",
            "Train Epoch: 4 [4900/5216 (93%)]\tLoss: 0.478682\n",
            "Train Epoch: 4 [4950/5216 (94%)]\tLoss: 0.489155\n",
            "Train Epoch: 4 [5000/5216 (95%)]\tLoss: 0.467976\n",
            "Train Epoch: 4 [5050/5216 (96%)]\tLoss: 0.510170\n",
            "Train Epoch: 4 [5100/5216 (97%)]\tLoss: 0.462341\n",
            "Train Epoch: 4 [5150/5216 (98%)]\tLoss: 0.497586\n",
            "Train Epoch: 4 [1664/5216 (99%)]\tLoss: 0.644321\n",
            "\n",
            "Test set: Average loss: 1.1722, Accuracy: 392/624 (63%)\n",
            "\n",
            "Train Epoch: 5 [0/5216 (0%)]\tLoss: 0.446122\n",
            "Train Epoch: 5 [50/5216 (1%)]\tLoss: 0.556222\n",
            "Train Epoch: 5 [100/5216 (2%)]\tLoss: 0.540305\n",
            "Train Epoch: 5 [150/5216 (3%)]\tLoss: 0.427516\n",
            "Train Epoch: 5 [200/5216 (4%)]\tLoss: 0.389129\n",
            "Train Epoch: 5 [250/5216 (5%)]\tLoss: 0.513235\n",
            "Train Epoch: 5 [300/5216 (6%)]\tLoss: 0.503000\n",
            "Train Epoch: 5 [350/5216 (7%)]\tLoss: 0.486183\n",
            "Train Epoch: 5 [400/5216 (8%)]\tLoss: 0.540129\n",
            "Train Epoch: 5 [450/5216 (9%)]\tLoss: 0.401435\n",
            "Train Epoch: 5 [500/5216 (10%)]\tLoss: 0.438684\n",
            "Train Epoch: 5 [550/5216 (10%)]\tLoss: 0.549320\n",
            "Train Epoch: 5 [600/5216 (11%)]\tLoss: 0.552181\n",
            "Train Epoch: 5 [650/5216 (12%)]\tLoss: 0.452435\n",
            "Train Epoch: 5 [700/5216 (13%)]\tLoss: 0.539872\n",
            "Train Epoch: 5 [750/5216 (14%)]\tLoss: 0.549146\n",
            "Train Epoch: 5 [800/5216 (15%)]\tLoss: 0.495271\n",
            "Train Epoch: 5 [850/5216 (16%)]\tLoss: 0.563262\n",
            "Train Epoch: 5 [900/5216 (17%)]\tLoss: 0.656446\n",
            "Train Epoch: 5 [950/5216 (18%)]\tLoss: 0.381332\n",
            "Train Epoch: 5 [1000/5216 (19%)]\tLoss: 0.554682\n",
            "Train Epoch: 5 [1050/5216 (20%)]\tLoss: 0.413550\n",
            "Train Epoch: 5 [1100/5216 (21%)]\tLoss: 0.429559\n",
            "Train Epoch: 5 [1150/5216 (22%)]\tLoss: 0.356747\n",
            "Train Epoch: 5 [1200/5216 (23%)]\tLoss: 0.399329\n",
            "Train Epoch: 5 [1250/5216 (24%)]\tLoss: 0.568959\n",
            "Train Epoch: 5 [1300/5216 (25%)]\tLoss: 0.323395\n",
            "Train Epoch: 5 [1350/5216 (26%)]\tLoss: 0.361883\n",
            "Train Epoch: 5 [1400/5216 (27%)]\tLoss: 0.524597\n",
            "Train Epoch: 5 [1450/5216 (28%)]\tLoss: 0.414172\n",
            "Train Epoch: 5 [1500/5216 (29%)]\tLoss: 0.483791\n",
            "Train Epoch: 5 [1550/5216 (30%)]\tLoss: 0.397608\n",
            "Train Epoch: 5 [1600/5216 (30%)]\tLoss: 0.404491\n",
            "Train Epoch: 5 [1650/5216 (31%)]\tLoss: 0.342962\n",
            "Train Epoch: 5 [1700/5216 (32%)]\tLoss: 0.494023\n",
            "Train Epoch: 5 [1750/5216 (33%)]\tLoss: 0.545483\n",
            "Train Epoch: 5 [1800/5216 (34%)]\tLoss: 0.309674\n",
            "Train Epoch: 5 [1850/5216 (35%)]\tLoss: 0.524231\n",
            "Train Epoch: 5 [1900/5216 (36%)]\tLoss: 0.446806\n",
            "Train Epoch: 5 [1950/5216 (37%)]\tLoss: 0.385571\n",
            "Train Epoch: 5 [2000/5216 (38%)]\tLoss: 0.365164\n",
            "Train Epoch: 5 [2050/5216 (39%)]\tLoss: 0.538091\n",
            "Train Epoch: 5 [2100/5216 (40%)]\tLoss: 0.587294\n",
            "Train Epoch: 5 [2150/5216 (41%)]\tLoss: 0.404056\n",
            "Train Epoch: 5 [2200/5216 (42%)]\tLoss: 0.397295\n",
            "Train Epoch: 5 [2250/5216 (43%)]\tLoss: 0.542125\n",
            "Train Epoch: 5 [2300/5216 (44%)]\tLoss: 0.450360\n",
            "Train Epoch: 5 [2350/5216 (45%)]\tLoss: 0.598082\n",
            "Train Epoch: 5 [2400/5216 (46%)]\tLoss: 0.496325\n",
            "Train Epoch: 5 [2450/5216 (47%)]\tLoss: 0.334891\n",
            "Train Epoch: 5 [2500/5216 (48%)]\tLoss: 0.585344\n",
            "Train Epoch: 5 [2550/5216 (49%)]\tLoss: 0.560501\n",
            "Train Epoch: 5 [2600/5216 (50%)]\tLoss: 0.358994\n",
            "Train Epoch: 5 [2650/5216 (50%)]\tLoss: 0.492318\n",
            "Train Epoch: 5 [2700/5216 (51%)]\tLoss: 0.456121\n",
            "Train Epoch: 5 [2750/5216 (52%)]\tLoss: 0.452287\n",
            "Train Epoch: 5 [2800/5216 (53%)]\tLoss: 0.339976\n",
            "Train Epoch: 5 [2850/5216 (54%)]\tLoss: 0.470819\n",
            "Train Epoch: 5 [2900/5216 (55%)]\tLoss: 0.535855\n",
            "Train Epoch: 5 [2950/5216 (56%)]\tLoss: 0.575912\n",
            "Train Epoch: 5 [3000/5216 (57%)]\tLoss: 0.478116\n",
            "Train Epoch: 5 [3050/5216 (58%)]\tLoss: 0.486118\n",
            "Train Epoch: 5 [3100/5216 (59%)]\tLoss: 0.436522\n",
            "Train Epoch: 5 [3150/5216 (60%)]\tLoss: 0.523325\n",
            "Train Epoch: 5 [3200/5216 (61%)]\tLoss: 0.521964\n",
            "Train Epoch: 5 [3250/5216 (62%)]\tLoss: 0.443370\n",
            "Train Epoch: 5 [3300/5216 (63%)]\tLoss: 0.365362\n",
            "Train Epoch: 5 [3350/5216 (64%)]\tLoss: 0.363442\n",
            "Train Epoch: 5 [3400/5216 (65%)]\tLoss: 0.566521\n",
            "Train Epoch: 5 [3450/5216 (66%)]\tLoss: 0.332597\n",
            "Train Epoch: 5 [3500/5216 (67%)]\tLoss: 0.533944\n",
            "Train Epoch: 5 [3550/5216 (68%)]\tLoss: 0.433383\n",
            "Train Epoch: 5 [3600/5216 (69%)]\tLoss: 0.420708\n",
            "Train Epoch: 5 [3650/5216 (70%)]\tLoss: 0.485900\n",
            "Train Epoch: 5 [3700/5216 (70%)]\tLoss: 0.463041\n",
            "Train Epoch: 5 [3750/5216 (71%)]\tLoss: 0.379597\n",
            "Train Epoch: 5 [3800/5216 (72%)]\tLoss: 0.457086\n",
            "Train Epoch: 5 [3850/5216 (73%)]\tLoss: 0.362568\n",
            "Train Epoch: 5 [3900/5216 (74%)]\tLoss: 0.520216\n",
            "Train Epoch: 5 [3950/5216 (75%)]\tLoss: 0.419237\n",
            "Train Epoch: 5 [4000/5216 (76%)]\tLoss: 0.541279\n",
            "Train Epoch: 5 [4050/5216 (77%)]\tLoss: 0.375124\n",
            "Train Epoch: 5 [4100/5216 (78%)]\tLoss: 0.422393\n",
            "Train Epoch: 5 [4150/5216 (79%)]\tLoss: 0.613601\n",
            "Train Epoch: 5 [4200/5216 (80%)]\tLoss: 0.298320\n",
            "Train Epoch: 5 [4250/5216 (81%)]\tLoss: 0.664748\n",
            "Train Epoch: 5 [4300/5216 (82%)]\tLoss: 0.525399\n",
            "Train Epoch: 5 [4350/5216 (83%)]\tLoss: 0.477719\n",
            "Train Epoch: 5 [4400/5216 (84%)]\tLoss: 0.381811\n",
            "Train Epoch: 5 [4450/5216 (85%)]\tLoss: 0.441931\n",
            "Train Epoch: 5 [4500/5216 (86%)]\tLoss: 0.540825\n",
            "Train Epoch: 5 [4550/5216 (87%)]\tLoss: 0.442051\n",
            "Train Epoch: 5 [4600/5216 (88%)]\tLoss: 0.387382\n",
            "Train Epoch: 5 [4650/5216 (89%)]\tLoss: 0.667559\n",
            "Train Epoch: 5 [4700/5216 (90%)]\tLoss: 0.547731\n",
            "Train Epoch: 5 [4750/5216 (90%)]\tLoss: 0.417003\n",
            "Train Epoch: 5 [4800/5216 (91%)]\tLoss: 0.366047\n",
            "Train Epoch: 5 [4850/5216 (92%)]\tLoss: 0.410711\n",
            "Train Epoch: 5 [4900/5216 (93%)]\tLoss: 0.384836\n",
            "Train Epoch: 5 [4950/5216 (94%)]\tLoss: 0.500441\n",
            "Train Epoch: 5 [5000/5216 (95%)]\tLoss: 0.439790\n",
            "Train Epoch: 5 [5050/5216 (96%)]\tLoss: 0.481936\n",
            "Train Epoch: 5 [5100/5216 (97%)]\tLoss: 0.467117\n",
            "Train Epoch: 5 [5150/5216 (98%)]\tLoss: 0.487202\n",
            "Train Epoch: 5 [1664/5216 (99%)]\tLoss: 0.420380\n",
            "\n",
            "Test set: Average loss: 1.2059, Accuracy: 413/624 (66%)\n",
            "\n",
            "Train Epoch: 6 [0/5216 (0%)]\tLoss: 0.501745\n",
            "Train Epoch: 6 [50/5216 (1%)]\tLoss: 0.467874\n",
            "Train Epoch: 6 [100/5216 (2%)]\tLoss: 0.349054\n",
            "Train Epoch: 6 [150/5216 (3%)]\tLoss: 0.282103\n",
            "Train Epoch: 6 [200/5216 (4%)]\tLoss: 0.440726\n",
            "Train Epoch: 6 [250/5216 (5%)]\tLoss: 0.568283\n",
            "Train Epoch: 6 [300/5216 (6%)]\tLoss: 0.409593\n",
            "Train Epoch: 6 [350/5216 (7%)]\tLoss: 0.486467\n",
            "Train Epoch: 6 [400/5216 (8%)]\tLoss: 0.398802\n",
            "Train Epoch: 6 [450/5216 (9%)]\tLoss: 0.602289\n",
            "Train Epoch: 6 [500/5216 (10%)]\tLoss: 0.297449\n",
            "Train Epoch: 6 [550/5216 (10%)]\tLoss: 0.330211\n",
            "Train Epoch: 6 [600/5216 (11%)]\tLoss: 0.361389\n",
            "Train Epoch: 6 [650/5216 (12%)]\tLoss: 0.570798\n",
            "Train Epoch: 6 [700/5216 (13%)]\tLoss: 0.496596\n",
            "Train Epoch: 6 [750/5216 (14%)]\tLoss: 0.487802\n",
            "Train Epoch: 6 [800/5216 (15%)]\tLoss: 0.438201\n",
            "Train Epoch: 6 [850/5216 (16%)]\tLoss: 0.429647\n",
            "Train Epoch: 6 [900/5216 (17%)]\tLoss: 0.534412\n",
            "Train Epoch: 6 [950/5216 (18%)]\tLoss: 0.398459\n",
            "Train Epoch: 6 [1000/5216 (19%)]\tLoss: 0.483153\n",
            "Train Epoch: 6 [1050/5216 (20%)]\tLoss: 0.349523\n",
            "Train Epoch: 6 [1100/5216 (21%)]\tLoss: 0.474535\n",
            "Train Epoch: 6 [1150/5216 (22%)]\tLoss: 0.349613\n",
            "Train Epoch: 6 [1200/5216 (23%)]\tLoss: 0.319910\n",
            "Train Epoch: 6 [1250/5216 (24%)]\tLoss: 0.391140\n",
            "Train Epoch: 6 [1300/5216 (25%)]\tLoss: 0.314082\n",
            "Train Epoch: 6 [1350/5216 (26%)]\tLoss: 0.536283\n",
            "Train Epoch: 6 [1400/5216 (27%)]\tLoss: 0.724004\n",
            "Train Epoch: 6 [1450/5216 (28%)]\tLoss: 0.368558\n",
            "Train Epoch: 6 [1500/5216 (29%)]\tLoss: 0.577138\n",
            "Train Epoch: 6 [1550/5216 (30%)]\tLoss: 0.450119\n",
            "Train Epoch: 6 [1600/5216 (30%)]\tLoss: 0.317807\n",
            "Train Epoch: 6 [1650/5216 (31%)]\tLoss: 0.519127\n",
            "Train Epoch: 6 [1700/5216 (32%)]\tLoss: 0.386286\n",
            "Train Epoch: 6 [1750/5216 (33%)]\tLoss: 0.486073\n",
            "Train Epoch: 6 [1800/5216 (34%)]\tLoss: 0.435765\n",
            "Train Epoch: 6 [1850/5216 (35%)]\tLoss: 0.456325\n",
            "Train Epoch: 6 [1900/5216 (36%)]\tLoss: 0.495670\n",
            "Train Epoch: 6 [1950/5216 (37%)]\tLoss: 0.408760\n",
            "Train Epoch: 6 [2000/5216 (38%)]\tLoss: 0.584957\n",
            "Train Epoch: 6 [2050/5216 (39%)]\tLoss: 0.317460\n",
            "Train Epoch: 6 [2100/5216 (40%)]\tLoss: 0.587354\n",
            "Train Epoch: 6 [2150/5216 (41%)]\tLoss: 0.475234\n",
            "Train Epoch: 6 [2200/5216 (42%)]\tLoss: 0.545664\n",
            "Train Epoch: 6 [2250/5216 (43%)]\tLoss: 0.806432\n",
            "Train Epoch: 6 [2300/5216 (44%)]\tLoss: 0.601456\n",
            "Train Epoch: 6 [2350/5216 (45%)]\tLoss: 0.378564\n",
            "Train Epoch: 6 [2400/5216 (46%)]\tLoss: 0.437330\n",
            "Train Epoch: 6 [2450/5216 (47%)]\tLoss: 0.403834\n",
            "Train Epoch: 6 [2500/5216 (48%)]\tLoss: 0.404720\n",
            "Train Epoch: 6 [2550/5216 (49%)]\tLoss: 0.323858\n",
            "Train Epoch: 6 [2600/5216 (50%)]\tLoss: 0.465259\n",
            "Train Epoch: 6 [2650/5216 (50%)]\tLoss: 0.368647\n",
            "Train Epoch: 6 [2700/5216 (51%)]\tLoss: 0.352708\n",
            "Train Epoch: 6 [2750/5216 (52%)]\tLoss: 0.265066\n",
            "Train Epoch: 6 [2800/5216 (53%)]\tLoss: 0.438549\n",
            "Train Epoch: 6 [2850/5216 (54%)]\tLoss: 0.470359\n",
            "Train Epoch: 6 [2900/5216 (55%)]\tLoss: 0.517031\n",
            "Train Epoch: 6 [2950/5216 (56%)]\tLoss: 0.419951\n",
            "Train Epoch: 6 [3000/5216 (57%)]\tLoss: 0.416502\n",
            "Train Epoch: 6 [3050/5216 (58%)]\tLoss: 0.578113\n",
            "Train Epoch: 6 [3100/5216 (59%)]\tLoss: 0.435785\n",
            "Train Epoch: 6 [3150/5216 (60%)]\tLoss: 0.322499\n",
            "Train Epoch: 6 [3200/5216 (61%)]\tLoss: 0.524630\n",
            "Train Epoch: 6 [3250/5216 (62%)]\tLoss: 0.427828\n",
            "Train Epoch: 6 [3300/5216 (63%)]\tLoss: 0.398084\n",
            "Train Epoch: 6 [3350/5216 (64%)]\tLoss: 0.375257\n",
            "Train Epoch: 6 [3400/5216 (65%)]\tLoss: 0.376319\n",
            "Train Epoch: 6 [3450/5216 (66%)]\tLoss: 0.376654\n",
            "Train Epoch: 6 [3500/5216 (67%)]\tLoss: 0.487871\n",
            "Train Epoch: 6 [3550/5216 (68%)]\tLoss: 0.485557\n",
            "Train Epoch: 6 [3600/5216 (69%)]\tLoss: 0.603406\n",
            "Train Epoch: 6 [3650/5216 (70%)]\tLoss: 0.397183\n",
            "Train Epoch: 6 [3700/5216 (70%)]\tLoss: 0.379343\n",
            "Train Epoch: 6 [3750/5216 (71%)]\tLoss: 0.584336\n",
            "Train Epoch: 6 [3800/5216 (72%)]\tLoss: 0.368971\n",
            "Train Epoch: 6 [3850/5216 (73%)]\tLoss: 0.511904\n",
            "Train Epoch: 6 [3900/5216 (74%)]\tLoss: 0.405117\n",
            "Train Epoch: 6 [3950/5216 (75%)]\tLoss: 0.528682\n",
            "Train Epoch: 6 [4000/5216 (76%)]\tLoss: 0.400072\n",
            "Train Epoch: 6 [4050/5216 (77%)]\tLoss: 0.684078\n",
            "Train Epoch: 6 [4100/5216 (78%)]\tLoss: 0.559716\n",
            "Train Epoch: 6 [4150/5216 (79%)]\tLoss: 0.529231\n",
            "Train Epoch: 6 [4200/5216 (80%)]\tLoss: 0.419620\n",
            "Train Epoch: 6 [4250/5216 (81%)]\tLoss: 0.365957\n",
            "Train Epoch: 6 [4300/5216 (82%)]\tLoss: 0.348386\n",
            "Train Epoch: 6 [4350/5216 (83%)]\tLoss: 0.435432\n",
            "Train Epoch: 6 [4400/5216 (84%)]\tLoss: 0.452153\n",
            "Train Epoch: 6 [4450/5216 (85%)]\tLoss: 0.374118\n",
            "Train Epoch: 6 [4500/5216 (86%)]\tLoss: 0.469443\n",
            "Train Epoch: 6 [4550/5216 (87%)]\tLoss: 0.483723\n",
            "Train Epoch: 6 [4600/5216 (88%)]\tLoss: 0.553060\n",
            "Train Epoch: 6 [4650/5216 (89%)]\tLoss: 0.405157\n",
            "Train Epoch: 6 [4700/5216 (90%)]\tLoss: 0.385114\n",
            "Train Epoch: 6 [4750/5216 (90%)]\tLoss: 0.481196\n",
            "Train Epoch: 6 [4800/5216 (91%)]\tLoss: 0.456312\n",
            "Train Epoch: 6 [4850/5216 (92%)]\tLoss: 0.429471\n",
            "Train Epoch: 6 [4900/5216 (93%)]\tLoss: 0.563896\n",
            "Train Epoch: 6 [4950/5216 (94%)]\tLoss: 0.325000\n",
            "Train Epoch: 6 [5000/5216 (95%)]\tLoss: 0.397039\n",
            "Train Epoch: 6 [5050/5216 (96%)]\tLoss: 0.399055\n",
            "Train Epoch: 6 [5100/5216 (97%)]\tLoss: 0.450157\n",
            "Train Epoch: 6 [5150/5216 (98%)]\tLoss: 0.367470\n",
            "Train Epoch: 6 [1664/5216 (99%)]\tLoss: 0.428172\n",
            "\n",
            "Test set: Average loss: 1.1539, Accuracy: 423/624 (68%)\n",
            "\n",
            "Train Epoch: 7 [0/5216 (0%)]\tLoss: 0.266503\n",
            "Train Epoch: 7 [50/5216 (1%)]\tLoss: 0.478530\n",
            "Train Epoch: 7 [100/5216 (2%)]\tLoss: 0.492213\n",
            "Train Epoch: 7 [150/5216 (3%)]\tLoss: 0.375359\n",
            "Train Epoch: 7 [200/5216 (4%)]\tLoss: 0.378623\n",
            "Train Epoch: 7 [250/5216 (5%)]\tLoss: 0.470218\n",
            "Train Epoch: 7 [300/5216 (6%)]\tLoss: 0.352869\n",
            "Train Epoch: 7 [350/5216 (7%)]\tLoss: 0.232468\n",
            "Train Epoch: 7 [400/5216 (8%)]\tLoss: 0.498989\n",
            "Train Epoch: 7 [450/5216 (9%)]\tLoss: 0.661533\n",
            "Train Epoch: 7 [500/5216 (10%)]\tLoss: 0.355910\n",
            "Train Epoch: 7 [550/5216 (10%)]\tLoss: 0.532652\n",
            "Train Epoch: 7 [600/5216 (11%)]\tLoss: 0.396059\n",
            "Train Epoch: 7 [650/5216 (12%)]\tLoss: 0.450521\n",
            "Train Epoch: 7 [700/5216 (13%)]\tLoss: 0.373368\n",
            "Train Epoch: 7 [750/5216 (14%)]\tLoss: 0.321550\n",
            "Train Epoch: 7 [800/5216 (15%)]\tLoss: 0.274615\n",
            "Train Epoch: 7 [850/5216 (16%)]\tLoss: 0.285419\n",
            "Train Epoch: 7 [900/5216 (17%)]\tLoss: 0.461098\n",
            "Train Epoch: 7 [950/5216 (18%)]\tLoss: 0.398205\n",
            "Train Epoch: 7 [1000/5216 (19%)]\tLoss: 0.530695\n",
            "Train Epoch: 7 [1050/5216 (20%)]\tLoss: 0.290573\n",
            "Train Epoch: 7 [1100/5216 (21%)]\tLoss: 0.454763\n",
            "Train Epoch: 7 [1150/5216 (22%)]\tLoss: 0.442748\n",
            "Train Epoch: 7 [1200/5216 (23%)]\tLoss: 0.471137\n",
            "Train Epoch: 7 [1250/5216 (24%)]\tLoss: 0.385631\n",
            "Train Epoch: 7 [1300/5216 (25%)]\tLoss: 0.350476\n",
            "Train Epoch: 7 [1350/5216 (26%)]\tLoss: 0.457541\n",
            "Train Epoch: 7 [1400/5216 (27%)]\tLoss: 0.428758\n",
            "Train Epoch: 7 [1450/5216 (28%)]\tLoss: 0.403021\n",
            "Train Epoch: 7 [1500/5216 (29%)]\tLoss: 0.237116\n",
            "Train Epoch: 7 [1550/5216 (30%)]\tLoss: 0.368171\n",
            "Train Epoch: 7 [1600/5216 (30%)]\tLoss: 0.402475\n",
            "Train Epoch: 7 [1650/5216 (31%)]\tLoss: 0.282432\n",
            "Train Epoch: 7 [1700/5216 (32%)]\tLoss: 0.461996\n",
            "Train Epoch: 7 [1750/5216 (33%)]\tLoss: 0.582162\n",
            "Train Epoch: 7 [1800/5216 (34%)]\tLoss: 0.369224\n",
            "Train Epoch: 7 [1850/5216 (35%)]\tLoss: 0.374412\n",
            "Train Epoch: 7 [1900/5216 (36%)]\tLoss: 0.387888\n",
            "Train Epoch: 7 [1950/5216 (37%)]\tLoss: 0.442935\n",
            "Train Epoch: 7 [2000/5216 (38%)]\tLoss: 0.289766\n",
            "Train Epoch: 7 [2050/5216 (39%)]\tLoss: 0.519132\n",
            "Train Epoch: 7 [2100/5216 (40%)]\tLoss: 0.397002\n",
            "Train Epoch: 7 [2150/5216 (41%)]\tLoss: 0.388663\n",
            "Train Epoch: 7 [2200/5216 (42%)]\tLoss: 0.493204\n",
            "Train Epoch: 7 [2250/5216 (43%)]\tLoss: 0.335795\n",
            "Train Epoch: 7 [2300/5216 (44%)]\tLoss: 0.346624\n",
            "Train Epoch: 7 [2350/5216 (45%)]\tLoss: 0.434079\n",
            "Train Epoch: 7 [2400/5216 (46%)]\tLoss: 0.444812\n",
            "Train Epoch: 7 [2450/5216 (47%)]\tLoss: 0.429317\n",
            "Train Epoch: 7 [2500/5216 (48%)]\tLoss: 0.367826\n",
            "Train Epoch: 7 [2550/5216 (49%)]\tLoss: 0.429853\n",
            "Train Epoch: 7 [2600/5216 (50%)]\tLoss: 0.435892\n",
            "Train Epoch: 7 [2650/5216 (50%)]\tLoss: 0.254574\n",
            "Train Epoch: 7 [2700/5216 (51%)]\tLoss: 0.384906\n",
            "Train Epoch: 7 [2750/5216 (52%)]\tLoss: 0.557072\n",
            "Train Epoch: 7 [2800/5216 (53%)]\tLoss: 0.464653\n",
            "Train Epoch: 7 [2850/5216 (54%)]\tLoss: 0.325221\n",
            "Train Epoch: 7 [2900/5216 (55%)]\tLoss: 0.316602\n",
            "Train Epoch: 7 [2950/5216 (56%)]\tLoss: 0.525611\n",
            "Train Epoch: 7 [3000/5216 (57%)]\tLoss: 0.400530\n",
            "Train Epoch: 7 [3050/5216 (58%)]\tLoss: 0.487313\n",
            "Train Epoch: 7 [3100/5216 (59%)]\tLoss: 0.362063\n",
            "Train Epoch: 7 [3150/5216 (60%)]\tLoss: 0.313038\n",
            "Train Epoch: 7 [3200/5216 (61%)]\tLoss: 0.305458\n",
            "Train Epoch: 7 [3250/5216 (62%)]\tLoss: 0.371284\n",
            "Train Epoch: 7 [3300/5216 (63%)]\tLoss: 0.398168\n",
            "Train Epoch: 7 [3350/5216 (64%)]\tLoss: 0.392135\n",
            "Train Epoch: 7 [3400/5216 (65%)]\tLoss: 0.277777\n",
            "Train Epoch: 7 [3450/5216 (66%)]\tLoss: 0.365225\n",
            "Train Epoch: 7 [3500/5216 (67%)]\tLoss: 0.555825\n",
            "Train Epoch: 7 [3550/5216 (68%)]\tLoss: 0.419025\n",
            "Train Epoch: 7 [3600/5216 (69%)]\tLoss: 0.411858\n",
            "Train Epoch: 7 [3650/5216 (70%)]\tLoss: 0.420174\n",
            "Train Epoch: 7 [3700/5216 (70%)]\tLoss: 0.450061\n",
            "Train Epoch: 7 [3750/5216 (71%)]\tLoss: 0.503745\n",
            "Train Epoch: 7 [3800/5216 (72%)]\tLoss: 0.345568\n",
            "Train Epoch: 7 [3850/5216 (73%)]\tLoss: 0.426285\n",
            "Train Epoch: 7 [3900/5216 (74%)]\tLoss: 0.471018\n",
            "Train Epoch: 7 [3950/5216 (75%)]\tLoss: 0.293625\n",
            "Train Epoch: 7 [4000/5216 (76%)]\tLoss: 0.421490\n",
            "Train Epoch: 7 [4050/5216 (77%)]\tLoss: 0.450033\n",
            "Train Epoch: 7 [4100/5216 (78%)]\tLoss: 0.418390\n",
            "Train Epoch: 7 [4150/5216 (79%)]\tLoss: 0.588025\n",
            "Train Epoch: 7 [4200/5216 (80%)]\tLoss: 0.347755\n",
            "Train Epoch: 7 [4250/5216 (81%)]\tLoss: 0.549658\n",
            "Train Epoch: 7 [4300/5216 (82%)]\tLoss: 0.428894\n",
            "Train Epoch: 7 [4350/5216 (83%)]\tLoss: 0.372259\n",
            "Train Epoch: 7 [4400/5216 (84%)]\tLoss: 0.445142\n",
            "Train Epoch: 7 [4450/5216 (85%)]\tLoss: 0.327081\n",
            "Train Epoch: 7 [4500/5216 (86%)]\tLoss: 0.507754\n",
            "Train Epoch: 7 [4550/5216 (87%)]\tLoss: 0.508278\n",
            "Train Epoch: 7 [4600/5216 (88%)]\tLoss: 0.299988\n",
            "Train Epoch: 7 [4650/5216 (89%)]\tLoss: 0.500484\n",
            "Train Epoch: 7 [4700/5216 (90%)]\tLoss: 0.451524\n",
            "Train Epoch: 7 [4750/5216 (90%)]\tLoss: 0.304918\n",
            "Train Epoch: 7 [4800/5216 (91%)]\tLoss: 0.271731\n",
            "Train Epoch: 7 [4850/5216 (92%)]\tLoss: 0.474385\n",
            "Train Epoch: 7 [4900/5216 (93%)]\tLoss: 0.446137\n",
            "Train Epoch: 7 [4950/5216 (94%)]\tLoss: 0.693150\n",
            "Train Epoch: 7 [5000/5216 (95%)]\tLoss: 0.345320\n",
            "Train Epoch: 7 [5050/5216 (96%)]\tLoss: 0.237850\n",
            "Train Epoch: 7 [5100/5216 (97%)]\tLoss: 0.483828\n",
            "Train Epoch: 7 [5150/5216 (98%)]\tLoss: 0.327181\n",
            "Train Epoch: 7 [1664/5216 (99%)]\tLoss: 0.470372\n",
            "\n",
            "Test set: Average loss: 1.2056, Accuracy: 430/624 (69%)\n",
            "\n",
            "Train Epoch: 8 [0/5216 (0%)]\tLoss: 0.480667\n",
            "Train Epoch: 8 [50/5216 (1%)]\tLoss: 0.359535\n",
            "Train Epoch: 8 [100/5216 (2%)]\tLoss: 0.333035\n",
            "Train Epoch: 8 [150/5216 (3%)]\tLoss: 0.468573\n",
            "Train Epoch: 8 [200/5216 (4%)]\tLoss: 0.424327\n",
            "Train Epoch: 8 [250/5216 (5%)]\tLoss: 0.328172\n",
            "Train Epoch: 8 [300/5216 (6%)]\tLoss: 0.401001\n",
            "Train Epoch: 8 [350/5216 (7%)]\tLoss: 0.361715\n",
            "Train Epoch: 8 [400/5216 (8%)]\tLoss: 0.392897\n",
            "Train Epoch: 8 [450/5216 (9%)]\tLoss: 0.551659\n",
            "Train Epoch: 8 [500/5216 (10%)]\tLoss: 0.360156\n",
            "Train Epoch: 8 [550/5216 (10%)]\tLoss: 0.335535\n",
            "Train Epoch: 8 [600/5216 (11%)]\tLoss: 0.393818\n",
            "Train Epoch: 8 [650/5216 (12%)]\tLoss: 0.317614\n",
            "Train Epoch: 8 [700/5216 (13%)]\tLoss: 0.370684\n",
            "Train Epoch: 8 [750/5216 (14%)]\tLoss: 0.393318\n",
            "Train Epoch: 8 [800/5216 (15%)]\tLoss: 0.254862\n",
            "Train Epoch: 8 [850/5216 (16%)]\tLoss: 0.444328\n",
            "Train Epoch: 8 [900/5216 (17%)]\tLoss: 0.400442\n",
            "Train Epoch: 8 [950/5216 (18%)]\tLoss: 0.538915\n",
            "Train Epoch: 8 [1000/5216 (19%)]\tLoss: 0.312229\n",
            "Train Epoch: 8 [1050/5216 (20%)]\tLoss: 0.388192\n",
            "Train Epoch: 8 [1100/5216 (21%)]\tLoss: 0.468398\n",
            "Train Epoch: 8 [1150/5216 (22%)]\tLoss: 0.421701\n",
            "Train Epoch: 8 [1200/5216 (23%)]\tLoss: 0.338181\n",
            "Train Epoch: 8 [1250/5216 (24%)]\tLoss: 0.521224\n",
            "Train Epoch: 8 [1300/5216 (25%)]\tLoss: 0.498108\n",
            "Train Epoch: 8 [1350/5216 (26%)]\tLoss: 0.351825\n",
            "Train Epoch: 8 [1400/5216 (27%)]\tLoss: 0.419877\n",
            "Train Epoch: 8 [1450/5216 (28%)]\tLoss: 0.315337\n",
            "Train Epoch: 8 [1500/5216 (29%)]\tLoss: 0.441594\n",
            "Train Epoch: 8 [1550/5216 (30%)]\tLoss: 0.458396\n",
            "Train Epoch: 8 [1600/5216 (30%)]\tLoss: 0.338741\n",
            "Train Epoch: 8 [1650/5216 (31%)]\tLoss: 0.374314\n",
            "Train Epoch: 8 [1700/5216 (32%)]\tLoss: 0.320169\n",
            "Train Epoch: 8 [1750/5216 (33%)]\tLoss: 0.334376\n",
            "Train Epoch: 8 [1800/5216 (34%)]\tLoss: 0.294371\n",
            "Train Epoch: 8 [1850/5216 (35%)]\tLoss: 0.342827\n",
            "Train Epoch: 8 [1900/5216 (36%)]\tLoss: 0.376631\n",
            "Train Epoch: 8 [1950/5216 (37%)]\tLoss: 0.349174\n",
            "Train Epoch: 8 [2000/5216 (38%)]\tLoss: 0.291346\n",
            "Train Epoch: 8 [2050/5216 (39%)]\tLoss: 0.401659\n",
            "Train Epoch: 8 [2100/5216 (40%)]\tLoss: 0.358847\n",
            "Train Epoch: 8 [2150/5216 (41%)]\tLoss: 0.380472\n",
            "Train Epoch: 8 [2200/5216 (42%)]\tLoss: 0.401178\n",
            "Train Epoch: 8 [2250/5216 (43%)]\tLoss: 0.408585\n",
            "Train Epoch: 8 [2300/5216 (44%)]\tLoss: 0.514762\n",
            "Train Epoch: 8 [2350/5216 (45%)]\tLoss: 0.349835\n",
            "Train Epoch: 8 [2400/5216 (46%)]\tLoss: 0.197507\n",
            "Train Epoch: 8 [2450/5216 (47%)]\tLoss: 0.417108\n",
            "Train Epoch: 8 [2500/5216 (48%)]\tLoss: 0.334830\n",
            "Train Epoch: 8 [2550/5216 (49%)]\tLoss: 0.336185\n",
            "Train Epoch: 8 [2600/5216 (50%)]\tLoss: 0.394427\n",
            "Train Epoch: 8 [2650/5216 (50%)]\tLoss: 0.463081\n",
            "Train Epoch: 8 [2700/5216 (51%)]\tLoss: 0.607871\n",
            "Train Epoch: 8 [2750/5216 (52%)]\tLoss: 0.280004\n",
            "Train Epoch: 8 [2800/5216 (53%)]\tLoss: 0.351669\n",
            "Train Epoch: 8 [2850/5216 (54%)]\tLoss: 0.338490\n",
            "Train Epoch: 8 [2900/5216 (55%)]\tLoss: 0.184527\n",
            "Train Epoch: 8 [2950/5216 (56%)]\tLoss: 0.412235\n",
            "Train Epoch: 8 [3000/5216 (57%)]\tLoss: 0.429593\n",
            "Train Epoch: 8 [3050/5216 (58%)]\tLoss: 0.309893\n",
            "Train Epoch: 8 [3100/5216 (59%)]\tLoss: 0.312294\n",
            "Train Epoch: 8 [3150/5216 (60%)]\tLoss: 0.374650\n",
            "Train Epoch: 8 [3200/5216 (61%)]\tLoss: 0.617149\n",
            "Train Epoch: 8 [3250/5216 (62%)]\tLoss: 0.550466\n",
            "Train Epoch: 8 [3300/5216 (63%)]\tLoss: 0.376350\n",
            "Train Epoch: 8 [3350/5216 (64%)]\tLoss: 0.408910\n",
            "Train Epoch: 8 [3400/5216 (65%)]\tLoss: 0.395347\n",
            "Train Epoch: 8 [3450/5216 (66%)]\tLoss: 0.375692\n",
            "Train Epoch: 8 [3500/5216 (67%)]\tLoss: 0.386754\n",
            "Train Epoch: 8 [3550/5216 (68%)]\tLoss: 0.272804\n",
            "Train Epoch: 8 [3600/5216 (69%)]\tLoss: 0.281416\n",
            "Train Epoch: 8 [3650/5216 (70%)]\tLoss: 0.424816\n",
            "Train Epoch: 8 [3700/5216 (70%)]\tLoss: 0.356161\n",
            "Train Epoch: 8 [3750/5216 (71%)]\tLoss: 0.566674\n",
            "Train Epoch: 8 [3800/5216 (72%)]\tLoss: 0.430720\n",
            "Train Epoch: 8 [3850/5216 (73%)]\tLoss: 0.236989\n",
            "Train Epoch: 8 [3900/5216 (74%)]\tLoss: 0.446783\n",
            "Train Epoch: 8 [3950/5216 (75%)]\tLoss: 0.511267\n",
            "Train Epoch: 8 [4000/5216 (76%)]\tLoss: 0.393012\n",
            "Train Epoch: 8 [4050/5216 (77%)]\tLoss: 0.233258\n",
            "Train Epoch: 8 [4100/5216 (78%)]\tLoss: 0.325081\n",
            "Train Epoch: 8 [4150/5216 (79%)]\tLoss: 0.352906\n",
            "Train Epoch: 8 [4200/5216 (80%)]\tLoss: 0.482953\n",
            "Train Epoch: 8 [4250/5216 (81%)]\tLoss: 0.361749\n",
            "Train Epoch: 8 [4300/5216 (82%)]\tLoss: 0.465832\n",
            "Train Epoch: 8 [4350/5216 (83%)]\tLoss: 0.400796\n",
            "Train Epoch: 8 [4400/5216 (84%)]\tLoss: 0.472116\n",
            "Train Epoch: 8 [4450/5216 (85%)]\tLoss: 0.239626\n",
            "Train Epoch: 8 [4500/5216 (86%)]\tLoss: 0.361527\n",
            "Train Epoch: 8 [4550/5216 (87%)]\tLoss: 0.416830\n",
            "Train Epoch: 8 [4600/5216 (88%)]\tLoss: 0.409000\n",
            "Train Epoch: 8 [4650/5216 (89%)]\tLoss: 0.457701\n",
            "Train Epoch: 8 [4700/5216 (90%)]\tLoss: 0.406280\n",
            "Train Epoch: 8 [4750/5216 (90%)]\tLoss: 0.251251\n",
            "Train Epoch: 8 [4800/5216 (91%)]\tLoss: 0.309521\n",
            "Train Epoch: 8 [4850/5216 (92%)]\tLoss: 0.409498\n",
            "Train Epoch: 8 [4900/5216 (93%)]\tLoss: 0.495637\n",
            "Train Epoch: 8 [4950/5216 (94%)]\tLoss: 0.242674\n",
            "Train Epoch: 8 [5000/5216 (95%)]\tLoss: 0.329409\n",
            "Train Epoch: 8 [5050/5216 (96%)]\tLoss: 0.444728\n",
            "Train Epoch: 8 [5100/5216 (97%)]\tLoss: 0.404702\n",
            "Train Epoch: 8 [5150/5216 (98%)]\tLoss: 0.546180\n",
            "Train Epoch: 8 [1664/5216 (99%)]\tLoss: 0.431679\n",
            "\n",
            "Test set: Average loss: 1.6520, Accuracy: 378/624 (61%)\n",
            "\n",
            "Train Epoch: 9 [0/5216 (0%)]\tLoss: 0.397676\n",
            "Train Epoch: 9 [50/5216 (1%)]\tLoss: 0.388282\n",
            "Train Epoch: 9 [100/5216 (2%)]\tLoss: 0.220224\n",
            "Train Epoch: 9 [150/5216 (3%)]\tLoss: 0.348722\n",
            "Train Epoch: 9 [200/5216 (4%)]\tLoss: 0.304445\n",
            "Train Epoch: 9 [250/5216 (5%)]\tLoss: 0.224913\n",
            "Train Epoch: 9 [300/5216 (6%)]\tLoss: 0.360981\n",
            "Train Epoch: 9 [350/5216 (7%)]\tLoss: 0.278798\n",
            "Train Epoch: 9 [400/5216 (8%)]\tLoss: 0.379798\n",
            "Train Epoch: 9 [450/5216 (9%)]\tLoss: 0.247284\n",
            "Train Epoch: 9 [500/5216 (10%)]\tLoss: 0.538983\n",
            "Train Epoch: 9 [550/5216 (10%)]\tLoss: 0.333405\n",
            "Train Epoch: 9 [600/5216 (11%)]\tLoss: 0.203998\n",
            "Train Epoch: 9 [650/5216 (12%)]\tLoss: 0.279325\n",
            "Train Epoch: 9 [700/5216 (13%)]\tLoss: 0.403181\n",
            "Train Epoch: 9 [750/5216 (14%)]\tLoss: 0.259317\n",
            "Train Epoch: 9 [800/5216 (15%)]\tLoss: 0.258021\n",
            "Train Epoch: 9 [850/5216 (16%)]\tLoss: 0.460730\n",
            "Train Epoch: 9 [900/5216 (17%)]\tLoss: 0.324474\n",
            "Train Epoch: 9 [950/5216 (18%)]\tLoss: 0.394744\n",
            "Train Epoch: 9 [1000/5216 (19%)]\tLoss: 0.505126\n",
            "Train Epoch: 9 [1050/5216 (20%)]\tLoss: 0.422143\n",
            "Train Epoch: 9 [1100/5216 (21%)]\tLoss: 0.446173\n",
            "Train Epoch: 9 [1150/5216 (22%)]\tLoss: 0.373071\n",
            "Train Epoch: 9 [1200/5216 (23%)]\tLoss: 0.412174\n",
            "Train Epoch: 9 [1250/5216 (24%)]\tLoss: 0.285653\n",
            "Train Epoch: 9 [1300/5216 (25%)]\tLoss: 0.384129\n",
            "Train Epoch: 9 [1350/5216 (26%)]\tLoss: 0.338836\n",
            "Train Epoch: 9 [1400/5216 (27%)]\tLoss: 0.330378\n",
            "Train Epoch: 9 [1450/5216 (28%)]\tLoss: 0.342166\n",
            "Train Epoch: 9 [1500/5216 (29%)]\tLoss: 0.518951\n",
            "Train Epoch: 9 [1550/5216 (30%)]\tLoss: 0.423202\n",
            "Train Epoch: 9 [1600/5216 (30%)]\tLoss: 0.363738\n",
            "Train Epoch: 9 [1650/5216 (31%)]\tLoss: 0.240688\n",
            "Train Epoch: 9 [1700/5216 (32%)]\tLoss: 0.485592\n",
            "Train Epoch: 9 [1750/5216 (33%)]\tLoss: 0.405989\n",
            "Train Epoch: 9 [1800/5216 (34%)]\tLoss: 0.382929\n",
            "Train Epoch: 9 [1850/5216 (35%)]\tLoss: 0.293102\n",
            "Train Epoch: 9 [1900/5216 (36%)]\tLoss: 0.327500\n",
            "Train Epoch: 9 [1950/5216 (37%)]\tLoss: 0.282677\n",
            "Train Epoch: 9 [2000/5216 (38%)]\tLoss: 0.419952\n",
            "Train Epoch: 9 [2050/5216 (39%)]\tLoss: 0.471149\n",
            "Train Epoch: 9 [2100/5216 (40%)]\tLoss: 0.237520\n",
            "Train Epoch: 9 [2150/5216 (41%)]\tLoss: 0.446535\n",
            "Train Epoch: 9 [2200/5216 (42%)]\tLoss: 0.297256\n",
            "Train Epoch: 9 [2250/5216 (43%)]\tLoss: 0.322013\n",
            "Train Epoch: 9 [2300/5216 (44%)]\tLoss: 0.310066\n",
            "Train Epoch: 9 [2350/5216 (45%)]\tLoss: 0.390019\n",
            "Train Epoch: 9 [2400/5216 (46%)]\tLoss: 0.362957\n",
            "Train Epoch: 9 [2450/5216 (47%)]\tLoss: 0.468368\n",
            "Train Epoch: 9 [2500/5216 (48%)]\tLoss: 0.475525\n",
            "Train Epoch: 9 [2550/5216 (49%)]\tLoss: 0.265369\n",
            "Train Epoch: 9 [2600/5216 (50%)]\tLoss: 0.210860\n",
            "Train Epoch: 9 [2650/5216 (50%)]\tLoss: 0.257712\n",
            "Train Epoch: 9 [2700/5216 (51%)]\tLoss: 0.388769\n",
            "Train Epoch: 9 [2750/5216 (52%)]\tLoss: 0.410199\n",
            "Train Epoch: 9 [2800/5216 (53%)]\tLoss: 0.426129\n",
            "Train Epoch: 9 [2850/5216 (54%)]\tLoss: 0.428630\n",
            "Train Epoch: 9 [2900/5216 (55%)]\tLoss: 0.431588\n",
            "Train Epoch: 9 [2950/5216 (56%)]\tLoss: 0.231604\n",
            "Train Epoch: 9 [3000/5216 (57%)]\tLoss: 0.222457\n",
            "Train Epoch: 9 [3050/5216 (58%)]\tLoss: 0.321900\n",
            "Train Epoch: 9 [3100/5216 (59%)]\tLoss: 0.276556\n",
            "Train Epoch: 9 [3150/5216 (60%)]\tLoss: 0.432176\n",
            "Train Epoch: 9 [3200/5216 (61%)]\tLoss: 0.554103\n",
            "Train Epoch: 9 [3250/5216 (62%)]\tLoss: 0.287074\n",
            "Train Epoch: 9 [3300/5216 (63%)]\tLoss: 0.426280\n",
            "Train Epoch: 9 [3350/5216 (64%)]\tLoss: 0.396688\n",
            "Train Epoch: 9 [3400/5216 (65%)]\tLoss: 0.425205\n",
            "Train Epoch: 9 [3450/5216 (66%)]\tLoss: 0.296014\n",
            "Train Epoch: 9 [3500/5216 (67%)]\tLoss: 0.349196\n",
            "Train Epoch: 9 [3550/5216 (68%)]\tLoss: 0.440446\n",
            "Train Epoch: 9 [3600/5216 (69%)]\tLoss: 0.246553\n",
            "Train Epoch: 9 [3650/5216 (70%)]\tLoss: 0.343224\n",
            "Train Epoch: 9 [3700/5216 (70%)]\tLoss: 0.300314\n",
            "Train Epoch: 9 [3750/5216 (71%)]\tLoss: 0.400304\n",
            "Train Epoch: 9 [3800/5216 (72%)]\tLoss: 0.375207\n",
            "Train Epoch: 9 [3850/5216 (73%)]\tLoss: 0.465747\n",
            "Train Epoch: 9 [3900/5216 (74%)]\tLoss: 0.316003\n",
            "Train Epoch: 9 [3950/5216 (75%)]\tLoss: 0.370883\n",
            "Train Epoch: 9 [4000/5216 (76%)]\tLoss: 0.307526\n",
            "Train Epoch: 9 [4050/5216 (77%)]\tLoss: 0.347926\n",
            "Train Epoch: 9 [4100/5216 (78%)]\tLoss: 0.409517\n",
            "Train Epoch: 9 [4150/5216 (79%)]\tLoss: 0.273475\n",
            "Train Epoch: 9 [4200/5216 (80%)]\tLoss: 0.420323\n",
            "Train Epoch: 9 [4250/5216 (81%)]\tLoss: 0.598579\n",
            "Train Epoch: 9 [4300/5216 (82%)]\tLoss: 0.320117\n",
            "Train Epoch: 9 [4350/5216 (83%)]\tLoss: 0.318647\n",
            "Train Epoch: 9 [4400/5216 (84%)]\tLoss: 0.450857\n",
            "Train Epoch: 9 [4450/5216 (85%)]\tLoss: 0.524935\n",
            "Train Epoch: 9 [4500/5216 (86%)]\tLoss: 0.262766\n",
            "Train Epoch: 9 [4550/5216 (87%)]\tLoss: 0.288852\n",
            "Train Epoch: 9 [4600/5216 (88%)]\tLoss: 0.326687\n",
            "Train Epoch: 9 [4650/5216 (89%)]\tLoss: 0.362264\n",
            "Train Epoch: 9 [4700/5216 (90%)]\tLoss: 0.390453\n",
            "Train Epoch: 9 [4750/5216 (90%)]\tLoss: 0.297017\n",
            "Train Epoch: 9 [4800/5216 (91%)]\tLoss: 0.518408\n",
            "Train Epoch: 9 [4850/5216 (92%)]\tLoss: 0.396111\n",
            "Train Epoch: 9 [4900/5216 (93%)]\tLoss: 0.484034\n",
            "Train Epoch: 9 [4950/5216 (94%)]\tLoss: 0.292666\n",
            "Train Epoch: 9 [5000/5216 (95%)]\tLoss: 0.312025\n",
            "Train Epoch: 9 [5050/5216 (96%)]\tLoss: 0.394577\n",
            "Train Epoch: 9 [5100/5216 (97%)]\tLoss: 0.285377\n",
            "Train Epoch: 9 [5150/5216 (98%)]\tLoss: 0.310469\n",
            "Train Epoch: 9 [1664/5216 (99%)]\tLoss: 0.239618\n",
            "\n",
            "Test set: Average loss: 1.4084, Accuracy: 419/624 (67%)\n",
            "\n",
            "Train Epoch: 10 [0/5216 (0%)]\tLoss: 0.346591\n",
            "Train Epoch: 10 [50/5216 (1%)]\tLoss: 0.253840\n",
            "Train Epoch: 10 [100/5216 (2%)]\tLoss: 0.289657\n",
            "Train Epoch: 10 [150/5216 (3%)]\tLoss: 0.340573\n",
            "Train Epoch: 10 [200/5216 (4%)]\tLoss: 0.309839\n",
            "Train Epoch: 10 [250/5216 (5%)]\tLoss: 0.239206\n",
            "Train Epoch: 10 [300/5216 (6%)]\tLoss: 0.279913\n",
            "Train Epoch: 10 [350/5216 (7%)]\tLoss: 0.346184\n",
            "Train Epoch: 10 [400/5216 (8%)]\tLoss: 0.360840\n",
            "Train Epoch: 10 [450/5216 (9%)]\tLoss: 0.309973\n",
            "Train Epoch: 10 [500/5216 (10%)]\tLoss: 0.420722\n",
            "Train Epoch: 10 [550/5216 (10%)]\tLoss: 0.308329\n",
            "Train Epoch: 10 [600/5216 (11%)]\tLoss: 0.360365\n",
            "Train Epoch: 10 [650/5216 (12%)]\tLoss: 0.292867\n",
            "Train Epoch: 10 [700/5216 (13%)]\tLoss: 0.186055\n",
            "Train Epoch: 10 [750/5216 (14%)]\tLoss: 0.242740\n",
            "Train Epoch: 10 [800/5216 (15%)]\tLoss: 0.314837\n",
            "Train Epoch: 10 [850/5216 (16%)]\tLoss: 0.361347\n",
            "Train Epoch: 10 [900/5216 (17%)]\tLoss: 0.318324\n",
            "Train Epoch: 10 [950/5216 (18%)]\tLoss: 0.485172\n",
            "Train Epoch: 10 [1000/5216 (19%)]\tLoss: 0.273907\n",
            "Train Epoch: 10 [1050/5216 (20%)]\tLoss: 0.363448\n",
            "Train Epoch: 10 [1100/5216 (21%)]\tLoss: 0.391526\n",
            "Train Epoch: 10 [1150/5216 (22%)]\tLoss: 0.270605\n",
            "Train Epoch: 10 [1200/5216 (23%)]\tLoss: 0.376347\n",
            "Train Epoch: 10 [1250/5216 (24%)]\tLoss: 0.319689\n",
            "Train Epoch: 10 [1300/5216 (25%)]\tLoss: 0.313896\n",
            "Train Epoch: 10 [1350/5216 (26%)]\tLoss: 0.299499\n",
            "Train Epoch: 10 [1400/5216 (27%)]\tLoss: 0.271665\n",
            "Train Epoch: 10 [1450/5216 (28%)]\tLoss: 0.380088\n",
            "Train Epoch: 10 [1500/5216 (29%)]\tLoss: 0.287833\n",
            "Train Epoch: 10 [1550/5216 (30%)]\tLoss: 0.293821\n",
            "Train Epoch: 10 [1600/5216 (30%)]\tLoss: 0.452842\n",
            "Train Epoch: 10 [1650/5216 (31%)]\tLoss: 0.245134\n",
            "Train Epoch: 10 [1700/5216 (32%)]\tLoss: 0.377129\n",
            "Train Epoch: 10 [1750/5216 (33%)]\tLoss: 0.332944\n",
            "Train Epoch: 10 [1800/5216 (34%)]\tLoss: 0.260362\n",
            "Train Epoch: 10 [1850/5216 (35%)]\tLoss: 0.432858\n",
            "Train Epoch: 10 [1900/5216 (36%)]\tLoss: 0.489581\n",
            "Train Epoch: 10 [1950/5216 (37%)]\tLoss: 0.311957\n",
            "Train Epoch: 10 [2000/5216 (38%)]\tLoss: 0.358328\n",
            "Train Epoch: 10 [2050/5216 (39%)]\tLoss: 0.167373\n",
            "Train Epoch: 10 [2100/5216 (40%)]\tLoss: 0.305167\n",
            "Train Epoch: 10 [2150/5216 (41%)]\tLoss: 0.455386\n",
            "Train Epoch: 10 [2200/5216 (42%)]\tLoss: 0.289699\n",
            "Train Epoch: 10 [2250/5216 (43%)]\tLoss: 0.407131\n",
            "Train Epoch: 10 [2300/5216 (44%)]\tLoss: 0.289145\n",
            "Train Epoch: 10 [2350/5216 (45%)]\tLoss: 0.340513\n",
            "Train Epoch: 10 [2400/5216 (46%)]\tLoss: 0.364919\n",
            "Train Epoch: 10 [2450/5216 (47%)]\tLoss: 0.397274\n",
            "Train Epoch: 10 [2500/5216 (48%)]\tLoss: 0.338224\n",
            "Train Epoch: 10 [2550/5216 (49%)]\tLoss: 0.309796\n",
            "Train Epoch: 10 [2600/5216 (50%)]\tLoss: 0.421127\n",
            "Train Epoch: 10 [2650/5216 (50%)]\tLoss: 0.359520\n",
            "Train Epoch: 10 [2700/5216 (51%)]\tLoss: 0.346642\n",
            "Train Epoch: 10 [2750/5216 (52%)]\tLoss: 0.336314\n",
            "Train Epoch: 10 [2800/5216 (53%)]\tLoss: 0.244653\n",
            "Train Epoch: 10 [2850/5216 (54%)]\tLoss: 0.241864\n",
            "Train Epoch: 10 [2900/5216 (55%)]\tLoss: 0.341513\n",
            "Train Epoch: 10 [2950/5216 (56%)]\tLoss: 0.191080\n",
            "Train Epoch: 10 [3000/5216 (57%)]\tLoss: 0.318184\n",
            "Train Epoch: 10 [3050/5216 (58%)]\tLoss: 0.354087\n",
            "Train Epoch: 10 [3100/5216 (59%)]\tLoss: 0.341902\n",
            "Train Epoch: 10 [3150/5216 (60%)]\tLoss: 0.291679\n",
            "Train Epoch: 10 [3200/5216 (61%)]\tLoss: 0.294580\n",
            "Train Epoch: 10 [3250/5216 (62%)]\tLoss: 0.358957\n",
            "Train Epoch: 10 [3300/5216 (63%)]\tLoss: 0.325075\n",
            "Train Epoch: 10 [3350/5216 (64%)]\tLoss: 0.327041\n",
            "Train Epoch: 10 [3400/5216 (65%)]\tLoss: 0.413208\n",
            "Train Epoch: 10 [3450/5216 (66%)]\tLoss: 0.329432\n",
            "Train Epoch: 10 [3500/5216 (67%)]\tLoss: 0.206038\n",
            "Train Epoch: 10 [3550/5216 (68%)]\tLoss: 0.322637\n",
            "Train Epoch: 10 [3600/5216 (69%)]\tLoss: 0.273996\n",
            "Train Epoch: 10 [3650/5216 (70%)]\tLoss: 0.234788\n",
            "Train Epoch: 10 [3700/5216 (70%)]\tLoss: 0.323057\n",
            "Train Epoch: 10 [3750/5216 (71%)]\tLoss: 0.432553\n",
            "Train Epoch: 10 [3800/5216 (72%)]\tLoss: 0.371936\n",
            "Train Epoch: 10 [3850/5216 (73%)]\tLoss: 0.266825\n",
            "Train Epoch: 10 [3900/5216 (74%)]\tLoss: 0.324129\n",
            "Train Epoch: 10 [3950/5216 (75%)]\tLoss: 0.431021\n",
            "Train Epoch: 10 [4000/5216 (76%)]\tLoss: 0.333420\n",
            "Train Epoch: 10 [4050/5216 (77%)]\tLoss: 0.320265\n",
            "Train Epoch: 10 [4100/5216 (78%)]\tLoss: 0.283721\n",
            "Train Epoch: 10 [4150/5216 (79%)]\tLoss: 0.354297\n",
            "Train Epoch: 10 [4200/5216 (80%)]\tLoss: 0.242757\n",
            "Train Epoch: 10 [4250/5216 (81%)]\tLoss: 0.373951\n",
            "Train Epoch: 10 [4300/5216 (82%)]\tLoss: 0.376158\n",
            "Train Epoch: 10 [4350/5216 (83%)]\tLoss: 0.269111\n",
            "Train Epoch: 10 [4400/5216 (84%)]\tLoss: 0.261190\n",
            "Train Epoch: 10 [4450/5216 (85%)]\tLoss: 0.252318\n",
            "Train Epoch: 10 [4500/5216 (86%)]\tLoss: 0.232240\n",
            "Train Epoch: 10 [4550/5216 (87%)]\tLoss: 0.296355\n",
            "Train Epoch: 10 [4600/5216 (88%)]\tLoss: 0.276683\n",
            "Train Epoch: 10 [4650/5216 (89%)]\tLoss: 0.326025\n",
            "Train Epoch: 10 [4700/5216 (90%)]\tLoss: 0.170230\n",
            "Train Epoch: 10 [4750/5216 (90%)]\tLoss: 0.375189\n",
            "Train Epoch: 10 [4800/5216 (91%)]\tLoss: 0.423026\n",
            "Train Epoch: 10 [4850/5216 (92%)]\tLoss: 0.315937\n",
            "Train Epoch: 10 [4900/5216 (93%)]\tLoss: 0.358250\n",
            "Train Epoch: 10 [4950/5216 (94%)]\tLoss: 0.254775\n",
            "Train Epoch: 10 [5000/5216 (95%)]\tLoss: 0.320817\n",
            "Train Epoch: 10 [5050/5216 (96%)]\tLoss: 0.352879\n",
            "Train Epoch: 10 [5100/5216 (97%)]\tLoss: 0.241555\n",
            "Train Epoch: 10 [5150/5216 (98%)]\tLoss: 0.443305\n",
            "Train Epoch: 10 [1664/5216 (99%)]\tLoss: 0.297882\n",
            "\n",
            "Test set: Average loss: 1.9515, Accuracy: 336/624 (54%)\n",
            "\n",
            "Train Epoch: 11 [0/5216 (0%)]\tLoss: 0.355379\n",
            "Train Epoch: 11 [50/5216 (1%)]\tLoss: 0.280408\n",
            "Train Epoch: 11 [100/5216 (2%)]\tLoss: 0.309651\n",
            "Train Epoch: 11 [150/5216 (3%)]\tLoss: 0.290172\n",
            "Train Epoch: 11 [200/5216 (4%)]\tLoss: 0.280848\n",
            "Train Epoch: 11 [250/5216 (5%)]\tLoss: 0.311844\n",
            "Train Epoch: 11 [300/5216 (6%)]\tLoss: 0.277536\n",
            "Train Epoch: 11 [350/5216 (7%)]\tLoss: 0.257055\n",
            "Train Epoch: 11 [400/5216 (8%)]\tLoss: 0.300121\n",
            "Train Epoch: 11 [450/5216 (9%)]\tLoss: 0.303308\n",
            "Train Epoch: 11 [500/5216 (10%)]\tLoss: 0.368934\n",
            "Train Epoch: 11 [550/5216 (10%)]\tLoss: 0.335617\n",
            "Train Epoch: 11 [600/5216 (11%)]\tLoss: 0.300939\n",
            "Train Epoch: 11 [650/5216 (12%)]\tLoss: 0.254332\n",
            "Train Epoch: 11 [700/5216 (13%)]\tLoss: 0.222528\n",
            "Train Epoch: 11 [750/5216 (14%)]\tLoss: 0.145232\n",
            "Train Epoch: 11 [800/5216 (15%)]\tLoss: 0.166628\n",
            "Train Epoch: 11 [850/5216 (16%)]\tLoss: 0.315741\n",
            "Train Epoch: 11 [900/5216 (17%)]\tLoss: 0.301434\n",
            "Train Epoch: 11 [950/5216 (18%)]\tLoss: 0.260709\n",
            "Train Epoch: 11 [1000/5216 (19%)]\tLoss: 0.352761\n",
            "Train Epoch: 11 [1050/5216 (20%)]\tLoss: 0.348989\n",
            "Train Epoch: 11 [1100/5216 (21%)]\tLoss: 0.461191\n",
            "Train Epoch: 11 [1150/5216 (22%)]\tLoss: 0.148905\n",
            "Train Epoch: 11 [1200/5216 (23%)]\tLoss: 0.305116\n",
            "Train Epoch: 11 [1250/5216 (24%)]\tLoss: 0.309814\n",
            "Train Epoch: 11 [1300/5216 (25%)]\tLoss: 0.301485\n",
            "Train Epoch: 11 [1350/5216 (26%)]\tLoss: 0.223119\n",
            "Train Epoch: 11 [1400/5216 (27%)]\tLoss: 0.329153\n",
            "Train Epoch: 11 [1450/5216 (28%)]\tLoss: 0.431497\n",
            "Train Epoch: 11 [1500/5216 (29%)]\tLoss: 0.214557\n",
            "Train Epoch: 11 [1550/5216 (30%)]\tLoss: 0.408381\n",
            "Train Epoch: 11 [1600/5216 (30%)]\tLoss: 0.452871\n",
            "Train Epoch: 11 [1650/5216 (31%)]\tLoss: 0.335703\n",
            "Train Epoch: 11 [1700/5216 (32%)]\tLoss: 0.311327\n",
            "Train Epoch: 11 [1750/5216 (33%)]\tLoss: 0.220527\n",
            "Train Epoch: 11 [1800/5216 (34%)]\tLoss: 0.324024\n",
            "Train Epoch: 11 [1850/5216 (35%)]\tLoss: 0.348641\n",
            "Train Epoch: 11 [1900/5216 (36%)]\tLoss: 0.261978\n",
            "Train Epoch: 11 [1950/5216 (37%)]\tLoss: 0.318810\n",
            "Train Epoch: 11 [2000/5216 (38%)]\tLoss: 0.251283\n",
            "Train Epoch: 11 [2050/5216 (39%)]\tLoss: 0.190609\n",
            "Train Epoch: 11 [2100/5216 (40%)]\tLoss: 0.310728\n",
            "Train Epoch: 11 [2150/5216 (41%)]\tLoss: 0.263134\n",
            "Train Epoch: 11 [2200/5216 (42%)]\tLoss: 0.228083\n",
            "Train Epoch: 11 [2250/5216 (43%)]\tLoss: 0.182399\n",
            "Train Epoch: 11 [2300/5216 (44%)]\tLoss: 0.402916\n",
            "Train Epoch: 11 [2350/5216 (45%)]\tLoss: 0.347460\n",
            "Train Epoch: 11 [2400/5216 (46%)]\tLoss: 0.242553\n",
            "Train Epoch: 11 [2450/5216 (47%)]\tLoss: 0.220665\n",
            "Train Epoch: 11 [2500/5216 (48%)]\tLoss: 0.307195\n",
            "Train Epoch: 11 [2550/5216 (49%)]\tLoss: 0.201751\n",
            "Train Epoch: 11 [2600/5216 (50%)]\tLoss: 0.312834\n",
            "Train Epoch: 11 [2650/5216 (50%)]\tLoss: 0.197979\n",
            "Train Epoch: 11 [2700/5216 (51%)]\tLoss: 0.203369\n",
            "Train Epoch: 11 [2750/5216 (52%)]\tLoss: 0.353058\n",
            "Train Epoch: 11 [2800/5216 (53%)]\tLoss: 0.201476\n",
            "Train Epoch: 11 [2850/5216 (54%)]\tLoss: 0.229649\n",
            "Train Epoch: 11 [2900/5216 (55%)]\tLoss: 0.236223\n",
            "Train Epoch: 11 [2950/5216 (56%)]\tLoss: 0.180513\n",
            "Train Epoch: 11 [3000/5216 (57%)]\tLoss: 0.388800\n",
            "Train Epoch: 11 [3050/5216 (58%)]\tLoss: 0.387970\n",
            "Train Epoch: 11 [3100/5216 (59%)]\tLoss: 0.248690\n",
            "Train Epoch: 11 [3150/5216 (60%)]\tLoss: 0.426903\n",
            "Train Epoch: 11 [3200/5216 (61%)]\tLoss: 0.390496\n",
            "Train Epoch: 11 [3250/5216 (62%)]\tLoss: 0.163479\n",
            "Train Epoch: 11 [3300/5216 (63%)]\tLoss: 0.285195\n",
            "Train Epoch: 11 [3350/5216 (64%)]\tLoss: 0.253796\n",
            "Train Epoch: 11 [3400/5216 (65%)]\tLoss: 0.448355\n",
            "Train Epoch: 11 [3450/5216 (66%)]\tLoss: 0.283949\n",
            "Train Epoch: 11 [3500/5216 (67%)]\tLoss: 0.242227\n",
            "Train Epoch: 11 [3550/5216 (68%)]\tLoss: 0.249111\n",
            "Train Epoch: 11 [3600/5216 (69%)]\tLoss: 0.325731\n",
            "Train Epoch: 11 [3650/5216 (70%)]\tLoss: 0.340008\n",
            "Train Epoch: 11 [3700/5216 (70%)]\tLoss: 0.510150\n",
            "Train Epoch: 11 [3750/5216 (71%)]\tLoss: 0.235775\n",
            "Train Epoch: 11 [3800/5216 (72%)]\tLoss: 0.196885\n",
            "Train Epoch: 11 [3850/5216 (73%)]\tLoss: 0.281301\n",
            "Train Epoch: 11 [3900/5216 (74%)]\tLoss: 0.238003\n",
            "Train Epoch: 11 [3950/5216 (75%)]\tLoss: 0.394717\n",
            "Train Epoch: 11 [4000/5216 (76%)]\tLoss: 0.304654\n",
            "Train Epoch: 11 [4050/5216 (77%)]\tLoss: 0.353319\n",
            "Train Epoch: 11 [4100/5216 (78%)]\tLoss: 0.343696\n",
            "Train Epoch: 11 [4150/5216 (79%)]\tLoss: 0.233391\n",
            "Train Epoch: 11 [4200/5216 (80%)]\tLoss: 0.289020\n",
            "Train Epoch: 11 [4250/5216 (81%)]\tLoss: 0.115698\n",
            "Train Epoch: 11 [4300/5216 (82%)]\tLoss: 0.246278\n",
            "Train Epoch: 11 [4350/5216 (83%)]\tLoss: 0.363819\n",
            "Train Epoch: 11 [4400/5216 (84%)]\tLoss: 0.196921\n",
            "Train Epoch: 11 [4450/5216 (85%)]\tLoss: 0.324326\n",
            "Train Epoch: 11 [4500/5216 (86%)]\tLoss: 0.235968\n",
            "Train Epoch: 11 [4550/5216 (87%)]\tLoss: 0.253923\n",
            "Train Epoch: 11 [4600/5216 (88%)]\tLoss: 0.343729\n",
            "Train Epoch: 11 [4650/5216 (89%)]\tLoss: 0.397712\n",
            "Train Epoch: 11 [4700/5216 (90%)]\tLoss: 0.223957\n",
            "Train Epoch: 11 [4750/5216 (90%)]\tLoss: 0.406574\n",
            "Train Epoch: 11 [4800/5216 (91%)]\tLoss: 0.367051\n",
            "Train Epoch: 11 [4850/5216 (92%)]\tLoss: 0.190603\n",
            "Train Epoch: 11 [4900/5216 (93%)]\tLoss: 0.217765\n",
            "Train Epoch: 11 [4950/5216 (94%)]\tLoss: 0.251067\n",
            "Train Epoch: 11 [5000/5216 (95%)]\tLoss: 0.370372\n",
            "Train Epoch: 11 [5050/5216 (96%)]\tLoss: 0.180809\n",
            "Train Epoch: 11 [5100/5216 (97%)]\tLoss: 0.372033\n",
            "Train Epoch: 11 [5150/5216 (98%)]\tLoss: 0.224471\n",
            "Train Epoch: 11 [1664/5216 (99%)]\tLoss: 0.429712\n",
            "\n",
            "Test set: Average loss: 1.6339, Accuracy: 404/624 (65%)\n",
            "\n",
            "Train Epoch: 12 [0/5216 (0%)]\tLoss: 0.222288\n",
            "Train Epoch: 12 [50/5216 (1%)]\tLoss: 0.167911\n",
            "Train Epoch: 12 [100/5216 (2%)]\tLoss: 0.329298\n",
            "Train Epoch: 12 [150/5216 (3%)]\tLoss: 0.253603\n",
            "Train Epoch: 12 [200/5216 (4%)]\tLoss: 0.252870\n",
            "Train Epoch: 12 [250/5216 (5%)]\tLoss: 0.346700\n",
            "Train Epoch: 12 [300/5216 (6%)]\tLoss: 0.300562\n",
            "Train Epoch: 12 [350/5216 (7%)]\tLoss: 0.220076\n",
            "Train Epoch: 12 [400/5216 (8%)]\tLoss: 0.246510\n",
            "Train Epoch: 12 [450/5216 (9%)]\tLoss: 0.408893\n",
            "Train Epoch: 12 [500/5216 (10%)]\tLoss: 0.279015\n",
            "Train Epoch: 12 [550/5216 (10%)]\tLoss: 0.174593\n",
            "Train Epoch: 12 [600/5216 (11%)]\tLoss: 0.232873\n",
            "Train Epoch: 12 [650/5216 (12%)]\tLoss: 0.346139\n",
            "Train Epoch: 12 [700/5216 (13%)]\tLoss: 0.245419\n",
            "Train Epoch: 12 [750/5216 (14%)]\tLoss: 0.308715\n",
            "Train Epoch: 12 [800/5216 (15%)]\tLoss: 0.282122\n",
            "Train Epoch: 12 [850/5216 (16%)]\tLoss: 0.283201\n",
            "Train Epoch: 12 [900/5216 (17%)]\tLoss: 0.129501\n",
            "Train Epoch: 12 [950/5216 (18%)]\tLoss: 0.230375\n",
            "Train Epoch: 12 [1000/5216 (19%)]\tLoss: 0.177128\n",
            "Train Epoch: 12 [1050/5216 (20%)]\tLoss: 0.280587\n",
            "Train Epoch: 12 [1100/5216 (21%)]\tLoss: 0.135016\n",
            "Train Epoch: 12 [1150/5216 (22%)]\tLoss: 0.230457\n",
            "Train Epoch: 12 [1200/5216 (23%)]\tLoss: 0.334143\n",
            "Train Epoch: 12 [1250/5216 (24%)]\tLoss: 0.182915\n",
            "Train Epoch: 12 [1300/5216 (25%)]\tLoss: 0.182529\n",
            "Train Epoch: 12 [1350/5216 (26%)]\tLoss: 0.248278\n",
            "Train Epoch: 12 [1400/5216 (27%)]\tLoss: 0.299330\n",
            "Train Epoch: 12 [1450/5216 (28%)]\tLoss: 0.224644\n",
            "Train Epoch: 12 [1500/5216 (29%)]\tLoss: 0.424958\n",
            "Train Epoch: 12 [1550/5216 (30%)]\tLoss: 0.248280\n",
            "Train Epoch: 12 [1600/5216 (30%)]\tLoss: 0.286728\n",
            "Train Epoch: 12 [1650/5216 (31%)]\tLoss: 0.262610\n",
            "Train Epoch: 12 [1700/5216 (32%)]\tLoss: 0.171697\n",
            "Train Epoch: 12 [1750/5216 (33%)]\tLoss: 0.328373\n",
            "Train Epoch: 12 [1800/5216 (34%)]\tLoss: 0.240360\n",
            "Train Epoch: 12 [1850/5216 (35%)]\tLoss: 0.287179\n",
            "Train Epoch: 12 [1900/5216 (36%)]\tLoss: 0.340801\n",
            "Train Epoch: 12 [1950/5216 (37%)]\tLoss: 0.263075\n",
            "Train Epoch: 12 [2000/5216 (38%)]\tLoss: 0.361319\n",
            "Train Epoch: 12 [2050/5216 (39%)]\tLoss: 0.254123\n",
            "Train Epoch: 12 [2100/5216 (40%)]\tLoss: 0.281608\n",
            "Train Epoch: 12 [2150/5216 (41%)]\tLoss: 0.276559\n",
            "Train Epoch: 12 [2200/5216 (42%)]\tLoss: 0.232809\n",
            "Train Epoch: 12 [2250/5216 (43%)]\tLoss: 0.211173\n",
            "Train Epoch: 12 [2300/5216 (44%)]\tLoss: 0.207779\n",
            "Train Epoch: 12 [2350/5216 (45%)]\tLoss: 0.168266\n",
            "Train Epoch: 12 [2400/5216 (46%)]\tLoss: 0.239452\n",
            "Train Epoch: 12 [2450/5216 (47%)]\tLoss: 0.221511\n",
            "Train Epoch: 12 [2500/5216 (48%)]\tLoss: 0.278591\n",
            "Train Epoch: 12 [2550/5216 (49%)]\tLoss: 0.143538\n",
            "Train Epoch: 12 [2600/5216 (50%)]\tLoss: 0.219682\n",
            "Train Epoch: 12 [2650/5216 (50%)]\tLoss: 0.220172\n",
            "Train Epoch: 12 [2700/5216 (51%)]\tLoss: 0.170997\n",
            "Train Epoch: 12 [2750/5216 (52%)]\tLoss: 0.160586\n",
            "Train Epoch: 12 [2800/5216 (53%)]\tLoss: 0.552298\n",
            "Train Epoch: 12 [2850/5216 (54%)]\tLoss: 0.302859\n",
            "Train Epoch: 12 [2900/5216 (55%)]\tLoss: 0.198664\n",
            "Train Epoch: 12 [2950/5216 (56%)]\tLoss: 0.282668\n",
            "Train Epoch: 12 [3000/5216 (57%)]\tLoss: 0.307310\n",
            "Train Epoch: 12 [3050/5216 (58%)]\tLoss: 0.169916\n",
            "Train Epoch: 12 [3100/5216 (59%)]\tLoss: 0.193764\n",
            "Train Epoch: 12 [3150/5216 (60%)]\tLoss: 0.217317\n",
            "Train Epoch: 12 [3200/5216 (61%)]\tLoss: 0.249298\n",
            "Train Epoch: 12 [3250/5216 (62%)]\tLoss: 0.277148\n",
            "Train Epoch: 12 [3300/5216 (63%)]\tLoss: 0.256270\n",
            "Train Epoch: 12 [3350/5216 (64%)]\tLoss: 0.346861\n",
            "Train Epoch: 12 [3400/5216 (65%)]\tLoss: 0.266534\n",
            "Train Epoch: 12 [3450/5216 (66%)]\tLoss: 0.268209\n",
            "Train Epoch: 12 [3500/5216 (67%)]\tLoss: 0.148917\n",
            "Train Epoch: 12 [3550/5216 (68%)]\tLoss: 0.206735\n",
            "Train Epoch: 12 [3600/5216 (69%)]\tLoss: 0.161482\n",
            "Train Epoch: 12 [3650/5216 (70%)]\tLoss: 0.201561\n",
            "Train Epoch: 12 [3700/5216 (70%)]\tLoss: 0.360321\n",
            "Train Epoch: 12 [3750/5216 (71%)]\tLoss: 0.358778\n",
            "Train Epoch: 12 [3800/5216 (72%)]\tLoss: 0.204233\n",
            "Train Epoch: 12 [3850/5216 (73%)]\tLoss: 0.155546\n",
            "Train Epoch: 12 [3900/5216 (74%)]\tLoss: 0.200243\n",
            "Train Epoch: 12 [3950/5216 (75%)]\tLoss: 0.203245\n",
            "Train Epoch: 12 [4000/5216 (76%)]\tLoss: 0.107992\n",
            "Train Epoch: 12 [4050/5216 (77%)]\tLoss: 0.310124\n",
            "Train Epoch: 12 [4100/5216 (78%)]\tLoss: 0.286349\n",
            "Train Epoch: 12 [4150/5216 (79%)]\tLoss: 0.312637\n",
            "Train Epoch: 12 [4200/5216 (80%)]\tLoss: 0.258100\n",
            "Train Epoch: 12 [4250/5216 (81%)]\tLoss: 0.190891\n",
            "Train Epoch: 12 [4300/5216 (82%)]\tLoss: 0.186252\n",
            "Train Epoch: 12 [4350/5216 (83%)]\tLoss: 0.244709\n",
            "Train Epoch: 12 [4400/5216 (84%)]\tLoss: 0.248457\n",
            "Train Epoch: 12 [4450/5216 (85%)]\tLoss: 0.230661\n",
            "Train Epoch: 12 [4500/5216 (86%)]\tLoss: 0.361086\n",
            "Train Epoch: 12 [4550/5216 (87%)]\tLoss: 0.317872\n",
            "Train Epoch: 12 [4600/5216 (88%)]\tLoss: 0.292862\n",
            "Train Epoch: 12 [4650/5216 (89%)]\tLoss: 0.279372\n",
            "Train Epoch: 12 [4700/5216 (90%)]\tLoss: 0.156237\n",
            "Train Epoch: 12 [4750/5216 (90%)]\tLoss: 0.339494\n",
            "Train Epoch: 12 [4800/5216 (91%)]\tLoss: 0.130574\n",
            "Train Epoch: 12 [4850/5216 (92%)]\tLoss: 0.238381\n",
            "Train Epoch: 12 [4900/5216 (93%)]\tLoss: 0.319018\n",
            "Train Epoch: 12 [4950/5216 (94%)]\tLoss: 0.374625\n",
            "Train Epoch: 12 [5000/5216 (95%)]\tLoss: 0.179586\n",
            "Train Epoch: 12 [5050/5216 (96%)]\tLoss: 0.234331\n",
            "Train Epoch: 12 [5100/5216 (97%)]\tLoss: 0.271150\n",
            "Train Epoch: 12 [5150/5216 (98%)]\tLoss: 0.272643\n",
            "Train Epoch: 12 [1664/5216 (99%)]\tLoss: 0.290957\n",
            "\n",
            "Test set: Average loss: 2.0288, Accuracy: 399/624 (64%)\n",
            "\n",
            "Train Epoch: 13 [0/5216 (0%)]\tLoss: 0.221277\n",
            "Train Epoch: 13 [50/5216 (1%)]\tLoss: 0.212869\n",
            "Train Epoch: 13 [100/5216 (2%)]\tLoss: 0.160894\n",
            "Train Epoch: 13 [150/5216 (3%)]\tLoss: 0.219856\n",
            "Train Epoch: 13 [200/5216 (4%)]\tLoss: 0.215578\n",
            "Train Epoch: 13 [250/5216 (5%)]\tLoss: 0.140827\n",
            "Train Epoch: 13 [300/5216 (6%)]\tLoss: 0.337190\n",
            "Train Epoch: 13 [350/5216 (7%)]\tLoss: 0.172581\n",
            "Train Epoch: 13 [400/5216 (8%)]\tLoss: 0.168616\n",
            "Train Epoch: 13 [450/5216 (9%)]\tLoss: 0.117262\n",
            "Train Epoch: 13 [500/5216 (10%)]\tLoss: 0.245531\n",
            "Train Epoch: 13 [550/5216 (10%)]\tLoss: 0.257309\n",
            "Train Epoch: 13 [600/5216 (11%)]\tLoss: 0.321811\n",
            "Train Epoch: 13 [650/5216 (12%)]\tLoss: 0.233048\n",
            "Train Epoch: 13 [700/5216 (13%)]\tLoss: 0.207905\n",
            "Train Epoch: 13 [750/5216 (14%)]\tLoss: 0.302052\n",
            "Train Epoch: 13 [800/5216 (15%)]\tLoss: 0.230818\n",
            "Train Epoch: 13 [850/5216 (16%)]\tLoss: 0.223042\n",
            "Train Epoch: 13 [900/5216 (17%)]\tLoss: 0.236358\n",
            "Train Epoch: 13 [950/5216 (18%)]\tLoss: 0.228029\n",
            "Train Epoch: 13 [1000/5216 (19%)]\tLoss: 0.203346\n",
            "Train Epoch: 13 [1050/5216 (20%)]\tLoss: 0.172310\n",
            "Train Epoch: 13 [1100/5216 (21%)]\tLoss: 0.143976\n",
            "Train Epoch: 13 [1150/5216 (22%)]\tLoss: 0.213091\n",
            "Train Epoch: 13 [1200/5216 (23%)]\tLoss: 0.239931\n",
            "Train Epoch: 13 [1250/5216 (24%)]\tLoss: 0.139643\n",
            "Train Epoch: 13 [1300/5216 (25%)]\tLoss: 0.173547\n",
            "Train Epoch: 13 [1350/5216 (26%)]\tLoss: 0.243468\n",
            "Train Epoch: 13 [1400/5216 (27%)]\tLoss: 0.125172\n",
            "Train Epoch: 13 [1450/5216 (28%)]\tLoss: 0.391986\n",
            "Train Epoch: 13 [1500/5216 (29%)]\tLoss: 0.395739\n",
            "Train Epoch: 13 [1550/5216 (30%)]\tLoss: 0.136514\n",
            "Train Epoch: 13 [1600/5216 (30%)]\tLoss: 0.189910\n",
            "Train Epoch: 13 [1650/5216 (31%)]\tLoss: 0.473152\n",
            "Train Epoch: 13 [1700/5216 (32%)]\tLoss: 0.112706\n",
            "Train Epoch: 13 [1750/5216 (33%)]\tLoss: 0.217388\n",
            "Train Epoch: 13 [1800/5216 (34%)]\tLoss: 0.144148\n",
            "Train Epoch: 13 [1850/5216 (35%)]\tLoss: 0.137461\n",
            "Train Epoch: 13 [1900/5216 (36%)]\tLoss: 0.168269\n",
            "Train Epoch: 13 [1950/5216 (37%)]\tLoss: 0.191336\n",
            "Train Epoch: 13 [2000/5216 (38%)]\tLoss: 0.250307\n",
            "Train Epoch: 13 [2050/5216 (39%)]\tLoss: 0.229023\n",
            "Train Epoch: 13 [2100/5216 (40%)]\tLoss: 0.187208\n",
            "Train Epoch: 13 [2150/5216 (41%)]\tLoss: 0.234484\n",
            "Train Epoch: 13 [2200/5216 (42%)]\tLoss: 0.099804\n",
            "Train Epoch: 13 [2250/5216 (43%)]\tLoss: 0.148129\n",
            "Train Epoch: 13 [2300/5216 (44%)]\tLoss: 0.246585\n",
            "Train Epoch: 13 [2350/5216 (45%)]\tLoss: 0.150655\n",
            "Train Epoch: 13 [2400/5216 (46%)]\tLoss: 0.299908\n",
            "Train Epoch: 13 [2450/5216 (47%)]\tLoss: 0.200036\n",
            "Train Epoch: 13 [2500/5216 (48%)]\tLoss: 0.240472\n",
            "Train Epoch: 13 [2550/5216 (49%)]\tLoss: 0.205487\n",
            "Train Epoch: 13 [2600/5216 (50%)]\tLoss: 0.128953\n",
            "Train Epoch: 13 [2650/5216 (50%)]\tLoss: 0.224752\n",
            "Train Epoch: 13 [2700/5216 (51%)]\tLoss: 0.218494\n",
            "Train Epoch: 13 [2750/5216 (52%)]\tLoss: 0.150350\n",
            "Train Epoch: 13 [2800/5216 (53%)]\tLoss: 0.241213\n",
            "Train Epoch: 13 [2850/5216 (54%)]\tLoss: 0.129598\n",
            "Train Epoch: 13 [2900/5216 (55%)]\tLoss: 0.387894\n",
            "Train Epoch: 13 [2950/5216 (56%)]\tLoss: 0.199462\n",
            "Train Epoch: 13 [3000/5216 (57%)]\tLoss: 0.168698\n",
            "Train Epoch: 13 [3050/5216 (58%)]\tLoss: 0.204177\n",
            "Train Epoch: 13 [3100/5216 (59%)]\tLoss: 0.106869\n",
            "Train Epoch: 13 [3150/5216 (60%)]\tLoss: 0.148446\n",
            "Train Epoch: 13 [3200/5216 (61%)]\tLoss: 0.231421\n",
            "Train Epoch: 13 [3250/5216 (62%)]\tLoss: 0.102843\n",
            "Train Epoch: 13 [3300/5216 (63%)]\tLoss: 0.145356\n",
            "Train Epoch: 13 [3350/5216 (64%)]\tLoss: 0.207858\n",
            "Train Epoch: 13 [3400/5216 (65%)]\tLoss: 0.109510\n",
            "Train Epoch: 13 [3450/5216 (66%)]\tLoss: 0.134648\n",
            "Train Epoch: 13 [3500/5216 (67%)]\tLoss: 0.441207\n",
            "Train Epoch: 13 [3550/5216 (68%)]\tLoss: 0.231564\n",
            "Train Epoch: 13 [3600/5216 (69%)]\tLoss: 0.315957\n",
            "Train Epoch: 13 [3650/5216 (70%)]\tLoss: 0.213142\n",
            "Train Epoch: 13 [3700/5216 (70%)]\tLoss: 0.224123\n",
            "Train Epoch: 13 [3750/5216 (71%)]\tLoss: 0.140355\n",
            "Train Epoch: 13 [3800/5216 (72%)]\tLoss: 0.131992\n",
            "Train Epoch: 13 [3850/5216 (73%)]\tLoss: 0.306744\n",
            "Train Epoch: 13 [3900/5216 (74%)]\tLoss: 0.262160\n",
            "Train Epoch: 13 [3950/5216 (75%)]\tLoss: 0.162703\n",
            "Train Epoch: 13 [4000/5216 (76%)]\tLoss: 0.151641\n",
            "Train Epoch: 13 [4050/5216 (77%)]\tLoss: 0.152965\n",
            "Train Epoch: 13 [4100/5216 (78%)]\tLoss: 0.242847\n",
            "Train Epoch: 13 [4150/5216 (79%)]\tLoss: 0.247238\n",
            "Train Epoch: 13 [4200/5216 (80%)]\tLoss: 0.226673\n",
            "Train Epoch: 13 [4250/5216 (81%)]\tLoss: 0.160538\n",
            "Train Epoch: 13 [4300/5216 (82%)]\tLoss: 0.241876\n",
            "Train Epoch: 13 [4350/5216 (83%)]\tLoss: 0.241942\n",
            "Train Epoch: 13 [4400/5216 (84%)]\tLoss: 0.263632\n",
            "Train Epoch: 13 [4450/5216 (85%)]\tLoss: 0.107743\n",
            "Train Epoch: 13 [4500/5216 (86%)]\tLoss: 0.355662\n",
            "Train Epoch: 13 [4550/5216 (87%)]\tLoss: 0.139406\n",
            "Train Epoch: 13 [4600/5216 (88%)]\tLoss: 0.279352\n",
            "Train Epoch: 13 [4650/5216 (89%)]\tLoss: 0.179949\n",
            "Train Epoch: 13 [4700/5216 (90%)]\tLoss: 0.216179\n",
            "Train Epoch: 13 [4750/5216 (90%)]\tLoss: 0.228004\n",
            "Train Epoch: 13 [4800/5216 (91%)]\tLoss: 0.431364\n",
            "Train Epoch: 13 [4850/5216 (92%)]\tLoss: 0.254396\n",
            "Train Epoch: 13 [4900/5216 (93%)]\tLoss: 0.180078\n",
            "Train Epoch: 13 [4950/5216 (94%)]\tLoss: 0.175338\n",
            "Train Epoch: 13 [5000/5216 (95%)]\tLoss: 0.175712\n",
            "Train Epoch: 13 [5050/5216 (96%)]\tLoss: 0.225982\n",
            "Train Epoch: 13 [5100/5216 (97%)]\tLoss: 0.196729\n",
            "Train Epoch: 13 [5150/5216 (98%)]\tLoss: 0.183292\n",
            "Train Epoch: 13 [1664/5216 (99%)]\tLoss: 0.238649\n",
            "\n",
            "Test set: Average loss: 1.6545, Accuracy: 403/624 (65%)\n",
            "\n",
            "Train Epoch: 14 [0/5216 (0%)]\tLoss: 0.211954\n",
            "Train Epoch: 14 [50/5216 (1%)]\tLoss: 0.155405\n",
            "Train Epoch: 14 [100/5216 (2%)]\tLoss: 0.142435\n",
            "Train Epoch: 14 [150/5216 (3%)]\tLoss: 0.196205\n",
            "Train Epoch: 14 [200/5216 (4%)]\tLoss: 0.100063\n",
            "Train Epoch: 14 [250/5216 (5%)]\tLoss: 0.117999\n",
            "Train Epoch: 14 [300/5216 (6%)]\tLoss: 0.131069\n",
            "Train Epoch: 14 [350/5216 (7%)]\tLoss: 0.150338\n",
            "Train Epoch: 14 [400/5216 (8%)]\tLoss: 0.182295\n",
            "Train Epoch: 14 [450/5216 (9%)]\tLoss: 0.169035\n",
            "Train Epoch: 14 [500/5216 (10%)]\tLoss: 0.111818\n",
            "Train Epoch: 14 [550/5216 (10%)]\tLoss: 0.179702\n",
            "Train Epoch: 14 [600/5216 (11%)]\tLoss: 0.162387\n",
            "Train Epoch: 14 [650/5216 (12%)]\tLoss: 0.075304\n",
            "Train Epoch: 14 [700/5216 (13%)]\tLoss: 0.057036\n",
            "Train Epoch: 14 [750/5216 (14%)]\tLoss: 0.111780\n",
            "Train Epoch: 14 [800/5216 (15%)]\tLoss: 0.147234\n",
            "Train Epoch: 14 [850/5216 (16%)]\tLoss: 0.066201\n",
            "Train Epoch: 14 [900/5216 (17%)]\tLoss: 0.112271\n",
            "Train Epoch: 14 [950/5216 (18%)]\tLoss: 0.103073\n",
            "Train Epoch: 14 [1000/5216 (19%)]\tLoss: 0.160984\n",
            "Train Epoch: 14 [1050/5216 (20%)]\tLoss: 0.194994\n",
            "Train Epoch: 14 [1100/5216 (21%)]\tLoss: 0.151050\n",
            "Train Epoch: 14 [1150/5216 (22%)]\tLoss: 0.200453\n",
            "Train Epoch: 14 [1200/5216 (23%)]\tLoss: 0.099850\n",
            "Train Epoch: 14 [1250/5216 (24%)]\tLoss: 0.180042\n",
            "Train Epoch: 14 [1300/5216 (25%)]\tLoss: 0.259968\n",
            "Train Epoch: 14 [1350/5216 (26%)]\tLoss: 0.140123\n",
            "Train Epoch: 14 [1400/5216 (27%)]\tLoss: 0.207056\n",
            "Train Epoch: 14 [1450/5216 (28%)]\tLoss: 0.298130\n",
            "Train Epoch: 14 [1500/5216 (29%)]\tLoss: 0.082029\n",
            "Train Epoch: 14 [1550/5216 (30%)]\tLoss: 0.218604\n",
            "Train Epoch: 14 [1600/5216 (30%)]\tLoss: 0.151520\n",
            "Train Epoch: 14 [1650/5216 (31%)]\tLoss: 0.123728\n",
            "Train Epoch: 14 [1700/5216 (32%)]\tLoss: 0.104394\n",
            "Train Epoch: 14 [1750/5216 (33%)]\tLoss: 0.223815\n",
            "Train Epoch: 14 [1800/5216 (34%)]\tLoss: 0.230934\n",
            "Train Epoch: 14 [1850/5216 (35%)]\tLoss: 0.153660\n",
            "Train Epoch: 14 [1900/5216 (36%)]\tLoss: 0.176122\n",
            "Train Epoch: 14 [1950/5216 (37%)]\tLoss: 0.101074\n",
            "Train Epoch: 14 [2000/5216 (38%)]\tLoss: 0.283588\n",
            "Train Epoch: 14 [2050/5216 (39%)]\tLoss: 0.135117\n",
            "Train Epoch: 14 [2100/5216 (40%)]\tLoss: 0.134205\n",
            "Train Epoch: 14 [2150/5216 (41%)]\tLoss: 0.213538\n",
            "Train Epoch: 14 [2200/5216 (42%)]\tLoss: 0.320711\n",
            "Train Epoch: 14 [2250/5216 (43%)]\tLoss: 0.177401\n",
            "Train Epoch: 14 [2300/5216 (44%)]\tLoss: 0.127208\n",
            "Train Epoch: 14 [2350/5216 (45%)]\tLoss: 0.135308\n",
            "Train Epoch: 14 [2400/5216 (46%)]\tLoss: 0.150707\n",
            "Train Epoch: 14 [2450/5216 (47%)]\tLoss: 0.099064\n",
            "Train Epoch: 14 [2500/5216 (48%)]\tLoss: 0.150361\n",
            "Train Epoch: 14 [2550/5216 (49%)]\tLoss: 0.117737\n",
            "Train Epoch: 14 [2600/5216 (50%)]\tLoss: 0.113671\n",
            "Train Epoch: 14 [2650/5216 (50%)]\tLoss: 0.080494\n",
            "Train Epoch: 14 [2700/5216 (51%)]\tLoss: 0.177652\n",
            "Train Epoch: 14 [2750/5216 (52%)]\tLoss: 0.209481\n",
            "Train Epoch: 14 [2800/5216 (53%)]\tLoss: 0.196593\n",
            "Train Epoch: 14 [2850/5216 (54%)]\tLoss: 0.058217\n",
            "Train Epoch: 14 [2900/5216 (55%)]\tLoss: 0.305004\n",
            "Train Epoch: 14 [2950/5216 (56%)]\tLoss: 0.081703\n",
            "Train Epoch: 14 [3000/5216 (57%)]\tLoss: 0.195173\n",
            "Train Epoch: 14 [3050/5216 (58%)]\tLoss: 0.189817\n",
            "Train Epoch: 14 [3100/5216 (59%)]\tLoss: 0.144849\n",
            "Train Epoch: 14 [3150/5216 (60%)]\tLoss: 0.241003\n",
            "Train Epoch: 14 [3200/5216 (61%)]\tLoss: 0.135889\n",
            "Train Epoch: 14 [3250/5216 (62%)]\tLoss: 0.140163\n",
            "Train Epoch: 14 [3300/5216 (63%)]\tLoss: 0.145000\n",
            "Train Epoch: 14 [3350/5216 (64%)]\tLoss: 0.158425\n",
            "Train Epoch: 14 [3400/5216 (65%)]\tLoss: 0.140429\n",
            "Train Epoch: 14 [3450/5216 (66%)]\tLoss: 0.173545\n",
            "Train Epoch: 14 [3500/5216 (67%)]\tLoss: 0.145777\n",
            "Train Epoch: 14 [3550/5216 (68%)]\tLoss: 0.180876\n",
            "Train Epoch: 14 [3600/5216 (69%)]\tLoss: 0.164973\n",
            "Train Epoch: 14 [3650/5216 (70%)]\tLoss: 0.119603\n",
            "Train Epoch: 14 [3700/5216 (70%)]\tLoss: 0.111685\n",
            "Train Epoch: 14 [3750/5216 (71%)]\tLoss: 0.142482\n",
            "Train Epoch: 14 [3800/5216 (72%)]\tLoss: 0.114128\n",
            "Train Epoch: 14 [3850/5216 (73%)]\tLoss: 0.110563\n",
            "Train Epoch: 14 [3900/5216 (74%)]\tLoss: 0.084489\n",
            "Train Epoch: 14 [3950/5216 (75%)]\tLoss: 0.067524\n",
            "Train Epoch: 14 [4000/5216 (76%)]\tLoss: 0.112010\n",
            "Train Epoch: 14 [4050/5216 (77%)]\tLoss: 0.157362\n",
            "Train Epoch: 14 [4100/5216 (78%)]\tLoss: 0.087192\n",
            "Train Epoch: 14 [4150/5216 (79%)]\tLoss: 0.300519\n",
            "Train Epoch: 14 [4200/5216 (80%)]\tLoss: 0.162168\n",
            "Train Epoch: 14 [4250/5216 (81%)]\tLoss: 0.073037\n",
            "Train Epoch: 14 [4300/5216 (82%)]\tLoss: 0.198685\n",
            "Train Epoch: 14 [4350/5216 (83%)]\tLoss: 0.107050\n",
            "Train Epoch: 14 [4400/5216 (84%)]\tLoss: 0.145746\n",
            "Train Epoch: 14 [4450/5216 (85%)]\tLoss: 0.094428\n",
            "Train Epoch: 14 [4500/5216 (86%)]\tLoss: 0.166752\n",
            "Train Epoch: 14 [4550/5216 (87%)]\tLoss: 0.181570\n",
            "Train Epoch: 14 [4600/5216 (88%)]\tLoss: 0.083603\n",
            "Train Epoch: 14 [4650/5216 (89%)]\tLoss: 0.177292\n",
            "Train Epoch: 14 [4700/5216 (90%)]\tLoss: 0.120709\n",
            "Train Epoch: 14 [4750/5216 (90%)]\tLoss: 0.196974\n",
            "Train Epoch: 14 [4800/5216 (91%)]\tLoss: 0.108391\n",
            "Train Epoch: 14 [4850/5216 (92%)]\tLoss: 0.122177\n",
            "Train Epoch: 14 [4900/5216 (93%)]\tLoss: 0.063627\n",
            "Train Epoch: 14 [4950/5216 (94%)]\tLoss: 0.031765\n",
            "Train Epoch: 14 [5000/5216 (95%)]\tLoss: 0.262880\n",
            "Train Epoch: 14 [5050/5216 (96%)]\tLoss: 0.238222\n",
            "Train Epoch: 14 [5100/5216 (97%)]\tLoss: 0.213162\n",
            "Train Epoch: 14 [5150/5216 (98%)]\tLoss: 0.063799\n",
            "Train Epoch: 14 [1664/5216 (99%)]\tLoss: 0.226643\n",
            "\n",
            "Test set: Average loss: 2.1090, Accuracy: 434/624 (70%)\n",
            "\n",
            "Train Epoch: 15 [0/5216 (0%)]\tLoss: 0.111671\n",
            "Train Epoch: 15 [50/5216 (1%)]\tLoss: 0.136036\n",
            "Train Epoch: 15 [100/5216 (2%)]\tLoss: 0.155088\n",
            "Train Epoch: 15 [150/5216 (3%)]\tLoss: 0.116184\n",
            "Train Epoch: 15 [200/5216 (4%)]\tLoss: 0.156836\n",
            "Train Epoch: 15 [250/5216 (5%)]\tLoss: 0.146944\n",
            "Train Epoch: 15 [300/5216 (6%)]\tLoss: 0.357373\n",
            "Train Epoch: 15 [350/5216 (7%)]\tLoss: 0.045201\n",
            "Train Epoch: 15 [400/5216 (8%)]\tLoss: 0.159031\n",
            "Train Epoch: 15 [450/5216 (9%)]\tLoss: 0.172263\n",
            "Train Epoch: 15 [500/5216 (10%)]\tLoss: 0.268676\n",
            "Train Epoch: 15 [550/5216 (10%)]\tLoss: 0.050964\n",
            "Train Epoch: 15 [600/5216 (11%)]\tLoss: 0.054360\n",
            "Train Epoch: 15 [650/5216 (12%)]\tLoss: 0.247730\n",
            "Train Epoch: 15 [700/5216 (13%)]\tLoss: 0.398917\n",
            "Train Epoch: 15 [750/5216 (14%)]\tLoss: 0.130430\n",
            "Train Epoch: 15 [800/5216 (15%)]\tLoss: 0.204186\n",
            "Train Epoch: 15 [850/5216 (16%)]\tLoss: 0.193774\n",
            "Train Epoch: 15 [900/5216 (17%)]\tLoss: 0.093088\n",
            "Train Epoch: 15 [950/5216 (18%)]\tLoss: 0.085022\n",
            "Train Epoch: 15 [1000/5216 (19%)]\tLoss: 0.094992\n",
            "Train Epoch: 15 [1050/5216 (20%)]\tLoss: 0.136825\n",
            "Train Epoch: 15 [1100/5216 (21%)]\tLoss: 0.077268\n",
            "Train Epoch: 15 [1150/5216 (22%)]\tLoss: 0.238705\n",
            "Train Epoch: 15 [1200/5216 (23%)]\tLoss: 0.244294\n",
            "Train Epoch: 15 [1250/5216 (24%)]\tLoss: 0.088184\n",
            "Train Epoch: 15 [1300/5216 (25%)]\tLoss: 0.213612\n",
            "Train Epoch: 15 [1350/5216 (26%)]\tLoss: 0.150448\n",
            "Train Epoch: 15 [1400/5216 (27%)]\tLoss: 0.115009\n",
            "Train Epoch: 15 [1450/5216 (28%)]\tLoss: 0.200928\n",
            "Train Epoch: 15 [1500/5216 (29%)]\tLoss: 0.111610\n",
            "Train Epoch: 15 [1550/5216 (30%)]\tLoss: 0.161141\n",
            "Train Epoch: 15 [1600/5216 (30%)]\tLoss: 0.052795\n",
            "Train Epoch: 15 [1650/5216 (31%)]\tLoss: 0.032638\n",
            "Train Epoch: 15 [1700/5216 (32%)]\tLoss: 0.136416\n",
            "Train Epoch: 15 [1750/5216 (33%)]\tLoss: 0.145331\n",
            "Train Epoch: 15 [1800/5216 (34%)]\tLoss: 0.131711\n",
            "Train Epoch: 15 [1850/5216 (35%)]\tLoss: 0.166136\n",
            "Train Epoch: 15 [1900/5216 (36%)]\tLoss: 0.085083\n",
            "Train Epoch: 15 [1950/5216 (37%)]\tLoss: 0.068611\n",
            "Train Epoch: 15 [2000/5216 (38%)]\tLoss: 0.090953\n",
            "Train Epoch: 15 [2050/5216 (39%)]\tLoss: 0.174361\n",
            "Train Epoch: 15 [2100/5216 (40%)]\tLoss: 0.100541\n",
            "Train Epoch: 15 [2150/5216 (41%)]\tLoss: 0.137273\n",
            "Train Epoch: 15 [2200/5216 (42%)]\tLoss: 0.237149\n",
            "Train Epoch: 15 [2250/5216 (43%)]\tLoss: 0.047463\n",
            "Train Epoch: 15 [2300/5216 (44%)]\tLoss: 0.071791\n",
            "Train Epoch: 15 [2350/5216 (45%)]\tLoss: 0.071539\n",
            "Train Epoch: 15 [2400/5216 (46%)]\tLoss: 0.109172\n",
            "Train Epoch: 15 [2450/5216 (47%)]\tLoss: 0.291887\n",
            "Train Epoch: 15 [2500/5216 (48%)]\tLoss: 0.124800\n",
            "Train Epoch: 15 [2550/5216 (49%)]\tLoss: 0.029383\n",
            "Train Epoch: 15 [2600/5216 (50%)]\tLoss: 0.093877\n",
            "Train Epoch: 15 [2650/5216 (50%)]\tLoss: 0.170387\n",
            "Train Epoch: 15 [2700/5216 (51%)]\tLoss: 0.332763\n",
            "Train Epoch: 15 [2750/5216 (52%)]\tLoss: 0.185860\n",
            "Train Epoch: 15 [2800/5216 (53%)]\tLoss: 0.054792\n",
            "Train Epoch: 15 [2850/5216 (54%)]\tLoss: 0.123221\n",
            "Train Epoch: 15 [2900/5216 (55%)]\tLoss: 0.169947\n",
            "Train Epoch: 15 [2950/5216 (56%)]\tLoss: 0.079467\n",
            "Train Epoch: 15 [3000/5216 (57%)]\tLoss: 0.171822\n",
            "Train Epoch: 15 [3050/5216 (58%)]\tLoss: 0.054830\n",
            "Train Epoch: 15 [3100/5216 (59%)]\tLoss: 0.049269\n",
            "Train Epoch: 15 [3150/5216 (60%)]\tLoss: 0.065229\n",
            "Train Epoch: 15 [3200/5216 (61%)]\tLoss: 0.106658\n",
            "Train Epoch: 15 [3250/5216 (62%)]\tLoss: 0.104055\n",
            "Train Epoch: 15 [3300/5216 (63%)]\tLoss: 0.098013\n",
            "Train Epoch: 15 [3350/5216 (64%)]\tLoss: 0.327215\n",
            "Train Epoch: 15 [3400/5216 (65%)]\tLoss: 0.159895\n",
            "Train Epoch: 15 [3450/5216 (66%)]\tLoss: 0.067469\n",
            "Train Epoch: 15 [3500/5216 (67%)]\tLoss: 0.116429\n",
            "Train Epoch: 15 [3550/5216 (68%)]\tLoss: 0.135048\n",
            "Train Epoch: 15 [3600/5216 (69%)]\tLoss: 0.132907\n",
            "Train Epoch: 15 [3650/5216 (70%)]\tLoss: 0.108586\n",
            "Train Epoch: 15 [3700/5216 (70%)]\tLoss: 0.117689\n",
            "Train Epoch: 15 [3750/5216 (71%)]\tLoss: 0.145413\n",
            "Train Epoch: 15 [3800/5216 (72%)]\tLoss: 0.120538\n",
            "Train Epoch: 15 [3850/5216 (73%)]\tLoss: 0.058346\n",
            "Train Epoch: 15 [3900/5216 (74%)]\tLoss: 0.083192\n",
            "Train Epoch: 15 [3950/5216 (75%)]\tLoss: 0.156872\n",
            "Train Epoch: 15 [4000/5216 (76%)]\tLoss: 0.133568\n",
            "Train Epoch: 15 [4050/5216 (77%)]\tLoss: 0.119211\n",
            "Train Epoch: 15 [4100/5216 (78%)]\tLoss: 0.118938\n",
            "Train Epoch: 15 [4150/5216 (79%)]\tLoss: 0.067677\n",
            "Train Epoch: 15 [4200/5216 (80%)]\tLoss: 0.254743\n",
            "Train Epoch: 15 [4250/5216 (81%)]\tLoss: 0.135470\n",
            "Train Epoch: 15 [4300/5216 (82%)]\tLoss: 0.025667\n",
            "Train Epoch: 15 [4350/5216 (83%)]\tLoss: 0.158581\n",
            "Train Epoch: 15 [4400/5216 (84%)]\tLoss: 0.060578\n",
            "Train Epoch: 15 [4450/5216 (85%)]\tLoss: 0.249418\n",
            "Train Epoch: 15 [4500/5216 (86%)]\tLoss: 0.069881\n",
            "Train Epoch: 15 [4550/5216 (87%)]\tLoss: 0.067402\n",
            "Train Epoch: 15 [4600/5216 (88%)]\tLoss: 0.131996\n",
            "Train Epoch: 15 [4650/5216 (89%)]\tLoss: 0.105288\n",
            "Train Epoch: 15 [4700/5216 (90%)]\tLoss: 0.113947\n",
            "Train Epoch: 15 [4750/5216 (90%)]\tLoss: 0.219242\n",
            "Train Epoch: 15 [4800/5216 (91%)]\tLoss: 0.080675\n",
            "Train Epoch: 15 [4850/5216 (92%)]\tLoss: 0.120941\n",
            "Train Epoch: 15 [4900/5216 (93%)]\tLoss: 0.246653\n",
            "Train Epoch: 15 [4950/5216 (94%)]\tLoss: 0.185737\n",
            "Train Epoch: 15 [5000/5216 (95%)]\tLoss: 0.176842\n",
            "Train Epoch: 15 [5050/5216 (96%)]\tLoss: 0.218927\n",
            "Train Epoch: 15 [5100/5216 (97%)]\tLoss: 0.231046\n",
            "Train Epoch: 15 [5150/5216 (98%)]\tLoss: 0.104048\n",
            "Train Epoch: 15 [1664/5216 (99%)]\tLoss: 0.184977\n",
            "\n",
            "Test set: Average loss: 1.8997, Accuracy: 431/624 (69%)\n",
            "\n",
            "Train Epoch: 16 [0/5216 (0%)]\tLoss: 0.042502\n",
            "Train Epoch: 16 [50/5216 (1%)]\tLoss: 0.169780\n",
            "Train Epoch: 16 [100/5216 (2%)]\tLoss: 0.045989\n",
            "Train Epoch: 16 [150/5216 (3%)]\tLoss: 0.065244\n",
            "Train Epoch: 16 [200/5216 (4%)]\tLoss: 0.193219\n",
            "Train Epoch: 16 [250/5216 (5%)]\tLoss: 0.070074\n",
            "Train Epoch: 16 [300/5216 (6%)]\tLoss: 0.036228\n",
            "Train Epoch: 16 [350/5216 (7%)]\tLoss: 0.130504\n",
            "Train Epoch: 16 [400/5216 (8%)]\tLoss: 0.116608\n",
            "Train Epoch: 16 [450/5216 (9%)]\tLoss: 0.137820\n",
            "Train Epoch: 16 [500/5216 (10%)]\tLoss: 0.100143\n",
            "Train Epoch: 16 [550/5216 (10%)]\tLoss: 0.149590\n",
            "Train Epoch: 16 [600/5216 (11%)]\tLoss: 0.038208\n",
            "Train Epoch: 16 [650/5216 (12%)]\tLoss: 0.063502\n",
            "Train Epoch: 16 [700/5216 (13%)]\tLoss: 0.116953\n",
            "Train Epoch: 16 [750/5216 (14%)]\tLoss: 0.122159\n",
            "Train Epoch: 16 [800/5216 (15%)]\tLoss: 0.164588\n",
            "Train Epoch: 16 [850/5216 (16%)]\tLoss: 0.177155\n",
            "Train Epoch: 16 [900/5216 (17%)]\tLoss: 0.128454\n",
            "Train Epoch: 16 [950/5216 (18%)]\tLoss: 0.139511\n",
            "Train Epoch: 16 [1000/5216 (19%)]\tLoss: 0.080266\n",
            "Train Epoch: 16 [1050/5216 (20%)]\tLoss: 0.115754\n",
            "Train Epoch: 16 [1100/5216 (21%)]\tLoss: 0.183056\n",
            "Train Epoch: 16 [1150/5216 (22%)]\tLoss: 0.204342\n",
            "Train Epoch: 16 [1200/5216 (23%)]\tLoss: 0.040567\n",
            "Train Epoch: 16 [1250/5216 (24%)]\tLoss: 0.081296\n",
            "Train Epoch: 16 [1300/5216 (25%)]\tLoss: 0.260437\n",
            "Train Epoch: 16 [1350/5216 (26%)]\tLoss: 0.075339\n",
            "Train Epoch: 16 [1400/5216 (27%)]\tLoss: 0.047573\n",
            "Train Epoch: 16 [1450/5216 (28%)]\tLoss: 0.162054\n",
            "Train Epoch: 16 [1500/5216 (29%)]\tLoss: 0.160761\n",
            "Train Epoch: 16 [1550/5216 (30%)]\tLoss: 0.077386\n",
            "Train Epoch: 16 [1600/5216 (30%)]\tLoss: 0.124659\n",
            "Train Epoch: 16 [1650/5216 (31%)]\tLoss: 0.039552\n",
            "Train Epoch: 16 [1700/5216 (32%)]\tLoss: 0.076455\n",
            "Train Epoch: 16 [1750/5216 (33%)]\tLoss: 0.116446\n",
            "Train Epoch: 16 [1800/5216 (34%)]\tLoss: 0.049230\n",
            "Train Epoch: 16 [1850/5216 (35%)]\tLoss: 0.072286\n",
            "Train Epoch: 16 [1900/5216 (36%)]\tLoss: 0.141919\n",
            "Train Epoch: 16 [1950/5216 (37%)]\tLoss: 0.100376\n",
            "Train Epoch: 16 [2000/5216 (38%)]\tLoss: 0.089948\n",
            "Train Epoch: 16 [2050/5216 (39%)]\tLoss: 0.072629\n",
            "Train Epoch: 16 [2100/5216 (40%)]\tLoss: 0.111918\n",
            "Train Epoch: 16 [2150/5216 (41%)]\tLoss: 0.052881\n",
            "Train Epoch: 16 [2200/5216 (42%)]\tLoss: 0.143957\n",
            "Train Epoch: 16 [2250/5216 (43%)]\tLoss: 0.084720\n",
            "Train Epoch: 16 [2300/5216 (44%)]\tLoss: 0.126297\n",
            "Train Epoch: 16 [2350/5216 (45%)]\tLoss: 0.050846\n",
            "Train Epoch: 16 [2400/5216 (46%)]\tLoss: 0.111847\n",
            "Train Epoch: 16 [2450/5216 (47%)]\tLoss: 0.076443\n",
            "Train Epoch: 16 [2500/5216 (48%)]\tLoss: 0.140794\n",
            "Train Epoch: 16 [2550/5216 (49%)]\tLoss: 0.052329\n",
            "Train Epoch: 16 [2600/5216 (50%)]\tLoss: 0.089035\n",
            "Train Epoch: 16 [2650/5216 (50%)]\tLoss: 0.097242\n",
            "Train Epoch: 16 [2700/5216 (51%)]\tLoss: 0.024072\n",
            "Train Epoch: 16 [2750/5216 (52%)]\tLoss: 0.095493\n",
            "Train Epoch: 16 [2800/5216 (53%)]\tLoss: 0.068321\n",
            "Train Epoch: 16 [2850/5216 (54%)]\tLoss: 0.108780\n",
            "Train Epoch: 16 [2900/5216 (55%)]\tLoss: 0.072450\n",
            "Train Epoch: 16 [2950/5216 (56%)]\tLoss: 0.075686\n",
            "Train Epoch: 16 [3000/5216 (57%)]\tLoss: 0.027518\n",
            "Train Epoch: 16 [3050/5216 (58%)]\tLoss: 0.055103\n",
            "Train Epoch: 16 [3100/5216 (59%)]\tLoss: 0.066344\n",
            "Train Epoch: 16 [3150/5216 (60%)]\tLoss: 0.057497\n",
            "Train Epoch: 16 [3200/5216 (61%)]\tLoss: 0.135743\n",
            "Train Epoch: 16 [3250/5216 (62%)]\tLoss: 0.044029\n",
            "Train Epoch: 16 [3300/5216 (63%)]\tLoss: 0.031508\n",
            "Train Epoch: 16 [3350/5216 (64%)]\tLoss: 0.068392\n",
            "Train Epoch: 16 [3400/5216 (65%)]\tLoss: 0.052922\n",
            "Train Epoch: 16 [3450/5216 (66%)]\tLoss: 0.034824\n",
            "Train Epoch: 16 [3500/5216 (67%)]\tLoss: 0.061759\n",
            "Train Epoch: 16 [3550/5216 (68%)]\tLoss: 0.017576\n",
            "Train Epoch: 16 [3600/5216 (69%)]\tLoss: 0.057342\n",
            "Train Epoch: 16 [3650/5216 (70%)]\tLoss: 0.056132\n",
            "Train Epoch: 16 [3700/5216 (70%)]\tLoss: 0.211507\n",
            "Train Epoch: 16 [3750/5216 (71%)]\tLoss: 0.083873\n",
            "Train Epoch: 16 [3800/5216 (72%)]\tLoss: 0.030775\n",
            "Train Epoch: 16 [3850/5216 (73%)]\tLoss: 0.005218\n",
            "Train Epoch: 16 [3900/5216 (74%)]\tLoss: 0.147221\n",
            "Train Epoch: 16 [3950/5216 (75%)]\tLoss: 0.133717\n",
            "Train Epoch: 16 [4000/5216 (76%)]\tLoss: 0.193285\n",
            "Train Epoch: 16 [4050/5216 (77%)]\tLoss: 0.076230\n",
            "Train Epoch: 16 [4100/5216 (78%)]\tLoss: 0.152098\n",
            "Train Epoch: 16 [4150/5216 (79%)]\tLoss: 0.176991\n",
            "Train Epoch: 16 [4200/5216 (80%)]\tLoss: 0.040223\n",
            "Train Epoch: 16 [4250/5216 (81%)]\tLoss: 0.050156\n",
            "Train Epoch: 16 [4300/5216 (82%)]\tLoss: 0.036730\n",
            "Train Epoch: 16 [4350/5216 (83%)]\tLoss: 0.052052\n",
            "Train Epoch: 16 [4400/5216 (84%)]\tLoss: 0.080516\n",
            "Train Epoch: 16 [4450/5216 (85%)]\tLoss: 0.021551\n",
            "Train Epoch: 16 [4500/5216 (86%)]\tLoss: 0.119209\n",
            "Train Epoch: 16 [4550/5216 (87%)]\tLoss: 0.033064\n",
            "Train Epoch: 16 [4600/5216 (88%)]\tLoss: 0.064984\n",
            "Train Epoch: 16 [4650/5216 (89%)]\tLoss: 0.151671\n",
            "Train Epoch: 16 [4700/5216 (90%)]\tLoss: 0.038966\n",
            "Train Epoch: 16 [4750/5216 (90%)]\tLoss: 0.062680\n",
            "Train Epoch: 16 [4800/5216 (91%)]\tLoss: 0.070499\n",
            "Train Epoch: 16 [4850/5216 (92%)]\tLoss: 0.139144\n",
            "Train Epoch: 16 [4900/5216 (93%)]\tLoss: 0.028116\n",
            "Train Epoch: 16 [4950/5216 (94%)]\tLoss: 0.090746\n",
            "Train Epoch: 16 [5000/5216 (95%)]\tLoss: 0.129600\n",
            "Train Epoch: 16 [5050/5216 (96%)]\tLoss: 0.103602\n",
            "Train Epoch: 16 [5100/5216 (97%)]\tLoss: 0.097487\n",
            "Train Epoch: 16 [5150/5216 (98%)]\tLoss: 0.211251\n",
            "Train Epoch: 16 [1664/5216 (99%)]\tLoss: 0.115049\n",
            "\n",
            "Test set: Average loss: 2.6841, Accuracy: 383/624 (61%)\n",
            "\n",
            "Train Epoch: 17 [0/5216 (0%)]\tLoss: 0.082219\n",
            "Train Epoch: 17 [50/5216 (1%)]\tLoss: 0.024912\n",
            "Train Epoch: 17 [100/5216 (2%)]\tLoss: 0.079113\n",
            "Train Epoch: 17 [150/5216 (3%)]\tLoss: 0.114435\n",
            "Train Epoch: 17 [200/5216 (4%)]\tLoss: 0.110423\n",
            "Train Epoch: 17 [250/5216 (5%)]\tLoss: 0.040824\n",
            "Train Epoch: 17 [300/5216 (6%)]\tLoss: 0.034369\n",
            "Train Epoch: 17 [350/5216 (7%)]\tLoss: 0.032887\n",
            "Train Epoch: 17 [400/5216 (8%)]\tLoss: 0.039393\n",
            "Train Epoch: 17 [450/5216 (9%)]\tLoss: 0.015021\n",
            "Train Epoch: 17 [500/5216 (10%)]\tLoss: 0.036733\n",
            "Train Epoch: 17 [550/5216 (10%)]\tLoss: 0.064552\n",
            "Train Epoch: 17 [600/5216 (11%)]\tLoss: 0.010049\n",
            "Train Epoch: 17 [650/5216 (12%)]\tLoss: 0.013823\n",
            "Train Epoch: 17 [700/5216 (13%)]\tLoss: 0.038274\n",
            "Train Epoch: 17 [750/5216 (14%)]\tLoss: 0.038225\n",
            "Train Epoch: 17 [800/5216 (15%)]\tLoss: 0.024321\n",
            "Train Epoch: 17 [850/5216 (16%)]\tLoss: 0.017811\n",
            "Train Epoch: 17 [900/5216 (17%)]\tLoss: 0.262314\n",
            "Train Epoch: 17 [950/5216 (18%)]\tLoss: 0.039758\n",
            "Train Epoch: 17 [1000/5216 (19%)]\tLoss: 0.063701\n",
            "Train Epoch: 17 [1050/5216 (20%)]\tLoss: 0.043321\n",
            "Train Epoch: 17 [1100/5216 (21%)]\tLoss: 0.032509\n",
            "Train Epoch: 17 [1150/5216 (22%)]\tLoss: 0.093452\n",
            "Train Epoch: 17 [1200/5216 (23%)]\tLoss: 0.027175\n",
            "Train Epoch: 17 [1250/5216 (24%)]\tLoss: 0.027781\n",
            "Train Epoch: 17 [1300/5216 (25%)]\tLoss: 0.066398\n",
            "Train Epoch: 17 [1350/5216 (26%)]\tLoss: 0.022565\n",
            "Train Epoch: 17 [1400/5216 (27%)]\tLoss: 0.035367\n",
            "Train Epoch: 17 [1450/5216 (28%)]\tLoss: 0.051999\n",
            "Train Epoch: 17 [1500/5216 (29%)]\tLoss: 0.236181\n",
            "Train Epoch: 17 [1550/5216 (30%)]\tLoss: 0.073461\n",
            "Train Epoch: 17 [1600/5216 (30%)]\tLoss: 0.007284\n",
            "Train Epoch: 17 [1650/5216 (31%)]\tLoss: 0.056840\n",
            "Train Epoch: 17 [1700/5216 (32%)]\tLoss: 0.142276\n",
            "Train Epoch: 17 [1750/5216 (33%)]\tLoss: 0.007394\n",
            "Train Epoch: 17 [1800/5216 (34%)]\tLoss: 0.039674\n",
            "Train Epoch: 17 [1850/5216 (35%)]\tLoss: 0.072151\n",
            "Train Epoch: 17 [1900/5216 (36%)]\tLoss: 0.061137\n",
            "Train Epoch: 17 [1950/5216 (37%)]\tLoss: 0.087830\n",
            "Train Epoch: 17 [2000/5216 (38%)]\tLoss: 0.126564\n",
            "Train Epoch: 17 [2050/5216 (39%)]\tLoss: 0.019508\n",
            "Train Epoch: 17 [2100/5216 (40%)]\tLoss: 0.115805\n",
            "Train Epoch: 17 [2150/5216 (41%)]\tLoss: 0.038565\n",
            "Train Epoch: 17 [2200/5216 (42%)]\tLoss: 0.105803\n",
            "Train Epoch: 17 [2250/5216 (43%)]\tLoss: 0.047006\n",
            "Train Epoch: 17 [2300/5216 (44%)]\tLoss: 0.015068\n",
            "Train Epoch: 17 [2350/5216 (45%)]\tLoss: 0.111381\n",
            "Train Epoch: 17 [2400/5216 (46%)]\tLoss: 0.039479\n",
            "Train Epoch: 17 [2450/5216 (47%)]\tLoss: 0.014873\n",
            "Train Epoch: 17 [2500/5216 (48%)]\tLoss: 0.039803\n",
            "Train Epoch: 17 [2550/5216 (49%)]\tLoss: 0.204538\n",
            "Train Epoch: 17 [2600/5216 (50%)]\tLoss: 0.175471\n",
            "Train Epoch: 17 [2650/5216 (50%)]\tLoss: 0.023163\n",
            "Train Epoch: 17 [2700/5216 (51%)]\tLoss: 0.074857\n",
            "Train Epoch: 17 [2750/5216 (52%)]\tLoss: 0.056463\n",
            "Train Epoch: 17 [2800/5216 (53%)]\tLoss: 0.060523\n",
            "Train Epoch: 17 [2850/5216 (54%)]\tLoss: 0.058818\n",
            "Train Epoch: 17 [2900/5216 (55%)]\tLoss: 0.038414\n",
            "Train Epoch: 17 [2950/5216 (56%)]\tLoss: 0.018515\n",
            "Train Epoch: 17 [3000/5216 (57%)]\tLoss: 0.020583\n",
            "Train Epoch: 17 [3050/5216 (58%)]\tLoss: 0.166056\n",
            "Train Epoch: 17 [3100/5216 (59%)]\tLoss: 0.037906\n",
            "Train Epoch: 17 [3150/5216 (60%)]\tLoss: 0.042302\n",
            "Train Epoch: 17 [3200/5216 (61%)]\tLoss: 0.096016\n",
            "Train Epoch: 17 [3250/5216 (62%)]\tLoss: 0.019028\n",
            "Train Epoch: 17 [3300/5216 (63%)]\tLoss: 0.010574\n",
            "Train Epoch: 17 [3350/5216 (64%)]\tLoss: 0.165953\n",
            "Train Epoch: 17 [3400/5216 (65%)]\tLoss: 0.035308\n",
            "Train Epoch: 17 [3450/5216 (66%)]\tLoss: 0.076099\n",
            "Train Epoch: 17 [3500/5216 (67%)]\tLoss: 0.169870\n",
            "Train Epoch: 17 [3550/5216 (68%)]\tLoss: 0.038883\n",
            "Train Epoch: 17 [3600/5216 (69%)]\tLoss: 0.092206\n",
            "Train Epoch: 17 [3650/5216 (70%)]\tLoss: 0.023945\n",
            "Train Epoch: 17 [3700/5216 (70%)]\tLoss: 0.122013\n",
            "Train Epoch: 17 [3750/5216 (71%)]\tLoss: 0.440687\n",
            "Train Epoch: 17 [3800/5216 (72%)]\tLoss: 0.004549\n",
            "Train Epoch: 17 [3850/5216 (73%)]\tLoss: 0.050548\n",
            "Train Epoch: 17 [3900/5216 (74%)]\tLoss: 0.018031\n",
            "Train Epoch: 17 [3950/5216 (75%)]\tLoss: 0.034750\n",
            "Train Epoch: 17 [4000/5216 (76%)]\tLoss: 0.042577\n",
            "Train Epoch: 17 [4050/5216 (77%)]\tLoss: 0.056045\n",
            "Train Epoch: 17 [4100/5216 (78%)]\tLoss: 0.735781\n",
            "Train Epoch: 17 [4150/5216 (79%)]\tLoss: 0.096335\n",
            "Train Epoch: 17 [4200/5216 (80%)]\tLoss: 0.135582\n",
            "Train Epoch: 17 [4250/5216 (81%)]\tLoss: 0.240613\n",
            "Train Epoch: 17 [4300/5216 (82%)]\tLoss: 0.130023\n",
            "Train Epoch: 17 [4350/5216 (83%)]\tLoss: 0.038710\n",
            "Train Epoch: 17 [4400/5216 (84%)]\tLoss: 0.050750\n",
            "Train Epoch: 17 [4450/5216 (85%)]\tLoss: 0.099348\n",
            "Train Epoch: 17 [4500/5216 (86%)]\tLoss: 0.128773\n",
            "Train Epoch: 17 [4550/5216 (87%)]\tLoss: 0.026265\n",
            "Train Epoch: 17 [4600/5216 (88%)]\tLoss: 0.055599\n",
            "Train Epoch: 17 [4650/5216 (89%)]\tLoss: 0.054724\n",
            "Train Epoch: 17 [4700/5216 (90%)]\tLoss: 0.113774\n",
            "Train Epoch: 17 [4750/5216 (90%)]\tLoss: 0.061796\n",
            "Train Epoch: 17 [4800/5216 (91%)]\tLoss: 0.095949\n",
            "Train Epoch: 17 [4850/5216 (92%)]\tLoss: 0.296805\n",
            "Train Epoch: 17 [4900/5216 (93%)]\tLoss: 0.173604\n",
            "Train Epoch: 17 [4950/5216 (94%)]\tLoss: 0.063002\n",
            "Train Epoch: 17 [5000/5216 (95%)]\tLoss: 0.127139\n",
            "Train Epoch: 17 [5050/5216 (96%)]\tLoss: 0.054385\n",
            "Train Epoch: 17 [5100/5216 (97%)]\tLoss: 0.087444\n",
            "Train Epoch: 17 [5150/5216 (98%)]\tLoss: 0.091052\n",
            "Train Epoch: 17 [1664/5216 (99%)]\tLoss: 0.100973\n",
            "\n",
            "Test set: Average loss: 2.2344, Accuracy: 388/624 (62%)\n",
            "\n",
            "Train Epoch: 18 [0/5216 (0%)]\tLoss: 0.047363\n",
            "Train Epoch: 18 [50/5216 (1%)]\tLoss: 0.054491\n",
            "Train Epoch: 18 [100/5216 (2%)]\tLoss: 0.041032\n",
            "Train Epoch: 18 [150/5216 (3%)]\tLoss: 0.033636\n",
            "Train Epoch: 18 [200/5216 (4%)]\tLoss: 0.021168\n",
            "Train Epoch: 18 [250/5216 (5%)]\tLoss: 0.064599\n",
            "Train Epoch: 18 [300/5216 (6%)]\tLoss: 0.020175\n",
            "Train Epoch: 18 [350/5216 (7%)]\tLoss: 0.106633\n",
            "Train Epoch: 18 [400/5216 (8%)]\tLoss: 0.020407\n",
            "Train Epoch: 18 [450/5216 (9%)]\tLoss: 0.091594\n",
            "Train Epoch: 18 [500/5216 (10%)]\tLoss: 0.027501\n",
            "Train Epoch: 18 [550/5216 (10%)]\tLoss: 0.066038\n",
            "Train Epoch: 18 [600/5216 (11%)]\tLoss: 0.064736\n",
            "Train Epoch: 18 [650/5216 (12%)]\tLoss: 0.018405\n",
            "Train Epoch: 18 [700/5216 (13%)]\tLoss: 0.057287\n",
            "Train Epoch: 18 [750/5216 (14%)]\tLoss: 0.066477\n",
            "Train Epoch: 18 [800/5216 (15%)]\tLoss: 0.016628\n",
            "Train Epoch: 18 [850/5216 (16%)]\tLoss: 0.014753\n",
            "Train Epoch: 18 [900/5216 (17%)]\tLoss: 0.035392\n",
            "Train Epoch: 18 [950/5216 (18%)]\tLoss: 0.008526\n",
            "Train Epoch: 18 [1000/5216 (19%)]\tLoss: 0.041572\n",
            "Train Epoch: 18 [1050/5216 (20%)]\tLoss: 0.020671\n",
            "Train Epoch: 18 [1100/5216 (21%)]\tLoss: 0.025277\n",
            "Train Epoch: 18 [1150/5216 (22%)]\tLoss: 0.027547\n",
            "Train Epoch: 18 [1200/5216 (23%)]\tLoss: 0.024243\n",
            "Train Epoch: 18 [1250/5216 (24%)]\tLoss: 0.017224\n",
            "Train Epoch: 18 [1300/5216 (25%)]\tLoss: 0.075762\n",
            "Train Epoch: 18 [1350/5216 (26%)]\tLoss: 0.092035\n",
            "Train Epoch: 18 [1400/5216 (27%)]\tLoss: 0.006018\n",
            "Train Epoch: 18 [1450/5216 (28%)]\tLoss: 0.004485\n",
            "Train Epoch: 18 [1500/5216 (29%)]\tLoss: 0.051320\n",
            "Train Epoch: 18 [1550/5216 (30%)]\tLoss: 0.013289\n",
            "Train Epoch: 18 [1600/5216 (30%)]\tLoss: 0.004715\n",
            "Train Epoch: 18 [1650/5216 (31%)]\tLoss: 0.047054\n",
            "Train Epoch: 18 [1700/5216 (32%)]\tLoss: 0.012603\n",
            "Train Epoch: 18 [1750/5216 (33%)]\tLoss: 0.027638\n",
            "Train Epoch: 18 [1800/5216 (34%)]\tLoss: 0.024064\n",
            "Train Epoch: 18 [1850/5216 (35%)]\tLoss: 0.003947\n",
            "Train Epoch: 18 [1900/5216 (36%)]\tLoss: 0.045627\n",
            "Train Epoch: 18 [1950/5216 (37%)]\tLoss: 0.031112\n",
            "Train Epoch: 18 [2000/5216 (38%)]\tLoss: 0.018828\n",
            "Train Epoch: 18 [2050/5216 (39%)]\tLoss: 0.022104\n",
            "Train Epoch: 18 [2100/5216 (40%)]\tLoss: 0.003737\n",
            "Train Epoch: 18 [2150/5216 (41%)]\tLoss: 0.010394\n",
            "Train Epoch: 18 [2200/5216 (42%)]\tLoss: 0.003066\n",
            "Train Epoch: 18 [2250/5216 (43%)]\tLoss: 0.080820\n",
            "Train Epoch: 18 [2300/5216 (44%)]\tLoss: 0.005657\n",
            "Train Epoch: 18 [2350/5216 (45%)]\tLoss: 0.004581\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-ecdc0b6a9355>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-e376bb9a2350>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, freq)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-9daa7499f4aa>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mdecode_image\u001b[0;34m(input, mode)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESULTS"
      ],
      "metadata": {
        "id": "9Wmid72nUZso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.plot(train_losses, marker='o', label='loss (train)')\n",
        "plt.plot(test_losses, marker='o', linestyle='dashed', label='loss (test)')\n",
        "\n",
        "plt.xlabel('#Epoch')\n",
        "plt.ylabel('Negative Log-Likelihood')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "OycAo2WDTuqv",
        "outputId": "3e5e0156-9bfc-43bc-83c7-5ad052500db0"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAKnCAYAAACVoMWWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACZQElEQVR4nOzdd3hUZf7+8fek95AQQhIIEDqhVwEVRVEQBVQU3e8quqvuysrad9X9rQV3FV3Xvq66dsWGDREUK4hIk15CTwgtQCAhvc7M74+TBCIpM8nMnJnkfl3XXHPmzJlzPgkKd5485/NY7Ha7HRERERERH+RndgEiIiIiIk2lMCsiIiIiPkthVkRERER8lsKsiIiIiPgshVkRERER8VkKsyIiIiLisxRmRURERMRnKcyKiIiIiM8KMLsAT7PZbBw6dIjIyEgsFovZ5YiIiIjIr9jtdgoKCkhKSsLPr+Gx11YXZg8dOkRycrLZZYiIiIhII/bv30/Hjh0bPKbVhdnIyEjA+OZERUWZXI2IiIiI/Fp+fj7Jyck1ua0hrS7MVk8tiIqKUpgVERER8WKOTAnVDWAiIiIi4rMUZkVERETEZynMioiIiIjPanVzZh1ht9uprKzEarWaXYq4kb+/PwEBAWrRJiIi4sMUZn+lvLycrKwsiouLzS5FPCAsLIzExESCgoLMLkVERESaQGH2FDabjYyMDPz9/UlKSiIoKEijdi2U3W6nvLyc7OxsMjIy6NGjR6NNmUVERMT7KMyeory8HJvNRnJyMmFhYWaXI24WGhpKYGAgmZmZlJeXExISYnZJIiIi4iQNRdVBI3Sth/6sRUREfJv+JRcRERERn6UwKyIiIiI+S2HWTaw2Oyv2HOfzDQdZsec4Vpvdrdc799xzuf322916jYYcP36c+Ph49u7d65bzv/nmm7Rp08apz1x99dU8+eSTbqlHREREvINuAHODRVuymPVFGll5pTX7EqNDeHBSKhP6JZpYmfs88sgjTJkyhS5dugCwZMkSxo4dS25urtMhtC5XXXUVEydOdOozf//73xkzZgw33ngj0dHRza5BREREvI9GZl1s0ZYsZsxZVyvIAhzOK2XGnHUs2pJlUmXuU1xczGuvvcYNN9zg9GfLy8sdOi40NJT4+Hinzt2vXz+6devGnDlznK5LREREfIPCbCPsdjvF5ZUOPQpKK3hw/lbqmlBQve+h+WkUlFY4dD67velTE3Jzc5k+fToxMTGEhYVx0UUXsWvXrpr3MzMzmTRpEjExMYSHh9O3b1++/PLLms/+9re/pV27doSGhtKjRw/eeOONeq/15ZdfEhwczMiRIwHYu3cvY8eOBSAmJgaLxcL1118PGNMhZs6cye23305cXBzjx48H4KmnnqJ///6Eh4eTnJzMn/70JwoLC2uu8etpBg899BCDBg3inXfeoUuXLkRHR3P11VdTUFBQq7ZJkybxwQcfNPn7KCIiIt5N0wwaUVJhJfWBr11yLjtwOL+U/g9949DxaQ+PJyyoaX9E119/Pbt27WL+/PlERUVxzz33MHHiRNLS0ggMDOSWW26hvLycpUuXEh4eTlpaGhEREQDcf//9pKWl8dVXXxEXF8fu3bspKSmp91o//fQTQ4cOrXmdnJzMJ598wtSpU9mxYwdRUVGEhobWvP/WW28xY8YMfv7555p9fn5+PPfcc6SkpJCens6f/vQn/vrXv/Lf//633uvu2bOHefPmsWDBAnJzc5k2bRqPPfYYjzzySM0xI0aM4JFHHqGsrIzg4OAmfS9FRETEeynMtkDVIfbnn39m9OjRALz77rskJyczb948rrzySvbt28fUqVPp378/AF27dq35/L59+xg8eDDDhg0DqJkHW5/MzEySkpJqXvv7+xMbGwtAfHz8aXNme/Towb/+9a9a+069ea1Lly7885//5Oabb24wzNpsNt58800iIyMBuPbaa/n+++9rhdmkpCTKy8s5fPgwnTt3bvDrEBEREd+jMNuI0EB/0h4e79CxqzNyuP6NXxo97s3fDWdESqxD126Kbdu2ERAQwBlnnFGzr23btvTq1Ytt27YBcOuttzJjxgy++eYbxo0bx9SpUxkwYAAAM2bMYOrUqaxbt44LL7yQSy+9tCYU16WkpMSp1bNOHcWt9t133zF79my2b99Ofn4+lZWVlJaWUlxcXO9qbF26dKkJsgCJiYkcPXq01jHVI8LFxcUO1yciIiK+Q3NmG2GxWAgLCnDocXaPdiRGh2Cp71wYXQ3O7tHOofNZLPWdqfluvPFG0tPTufbaa9m8eTPDhg3j+eefB+Ciiy4iMzOTO+64g0OHDnH++edz991313uuuLg4cnNzHb52eHh4rdd79+7lkksuYcCAAXzyySesXbuWF154AWj4BrHAwMBary0WCzabrda+nJwcANq1a+dwfSIiIuI7FGZdyN/PwoOTUgFOC7TVrx+clIq/n/tCKkCfPn2orKxk1apVNfuOHz/Ojh07SE1NrdmXnJzMzTffzKeffspdd93FK6+8UvNeu3btuO6665gzZw7PPPMM//vf/+q93uDBg0lLS6u1LygoCACr1dpovWvXrsVms/Hkk08ycuRIevbsyaFDhxz+ehuyZcsWOnbsSFxcnEvOJyIiIt5FYdbFJvRL5MVrhpAQXfvX7gnRIbx4zRCP9Jnt0aMHU6ZM4aabbmLZsmVs3LiRa665hg4dOjBlyhTAmKP69ddfk5GRwbp161i8eDF9+vQB4IEHHuDzzz9n9+7dbN26lQULFtS8V5fx48ezdevWWqOznTt3xmKxsGDBArKzs2t1Jvi17t27U1FRwfPPP096ejrvvPMOL730kku+Fz/99BMXXnihS84lIiIi3kdh1g0m9Etk2T3n8f5NI3n26kG8f9NIlt1znkcXTHjjjTcYOnQol1xyCaNGjcJut/Pll1/W/GrearVyyy230KdPHyZMmEDPnj1rbrYKCgrivvvuY8CAAYwZMwZ/f/8G21v179+fIUOGMHfu3Jp9HTp0YNasWdx77720b9+emTNn1vv5gQMH8tRTT/H444/Tr18/3n33XWbPnt3s70FpaSnz5s3jpptuava5REREPMpmhYyfYPPHxrOt8d90tlYWe3Oamfqg/Px8oqOjycvLIyoqqtZ7paWlZGRkkJKS4tQNTQILFy7kL3/5C1u2bMHPzzt+RnrxxRf57LPP+Oab+luh6c9cRES8Ttp8WHQP5J8y5S4qCSY8DqmTzavLgxrKa7/mHalDfN7FF1/MH/7wBw4ePGh2KTUCAwNrbmoTERHxCWnzYe702kEWID/L2J8235y6vJhac4nLnNor1hvceOONZpcgIiLiOJvVGJGtdy1RCyy6F3pfDH5Na9/ZEmlkVkRERMQbZC4/fUS2FjvkHzSOkxoKsyIiIiLeoPCIa49rJRRmRURERLxBRHvXHtdKKMyKiIiIeIPOo42uBQ2tJRrVwThOaijMioiIiHgDP3+j/VZDJjymm79+RWFWRERExFukToYLHj59f0AwTHu71fSZdYbCrIiIiIg3CQo3nhMHwfhHje3Kckg+w7SSvJnCrLt4eBm6c88919Q+r8ePHyc+Pp69e/d6/Nr33nsvf/7znz1+XREREbc4tM547j4ORt0CHYcDdtjysalleSuFWXdImw/P9IO3LoFPbjCen+nXolfteOSRR5gyZQpdunQBYMmSJVgsFk6cOOGya+zduxeLxcKGDRtq7b/77rt56623SE9Pd9m1RERETHOwKsx2GGI8D7zaeN74vjn1eDmFWVdrhcvQFRcX89prr3HDDTeYcv24uDjGjx/Piy++aMr1RUREXGriEzBuFnQcYbzuezn4BcKJ/VB41NzavJDCrKPKi+p/VJQaxzS6DB3G+6dOOajvnM2Um5vL9OnTiYmJISwsjIsuuohdu3bVvJ+ZmcmkSZOIiYkhPDycvn378uWXX9Z89re//S3t2rUjNDSUHj168MYbb9R7rS+//JLg4GBGjhwJGCOoY8eOBSAmJgaLxcL1118PgM1mY/bs2aSkpBAaGsrAgQP5+OOTvzZp6NopKSkADB48GIvFwrnnnlvzuUmTJvHBBx80+/smIiJiui5nwVm3Q0Q743VYLPzuK7h7J0TEm1qaNwowuwCf8WhS/e/1uBB++5GDy9AdMo5LOdvY9Ux/KD5++qEP5TWr3Ouvv55du3Yxf/58oqKiuOeee5g4cSJpaWkEBgZyyy23UF5eztKlSwkPDyctLY2IiAgA7r//ftLS0vjqq6+Ii4tj9+7dlJSU1Hutn376iaFDh9a8Tk5O5pNPPmHq1Kns2LGDqKgoQkNDAZg9ezZz5szhpZdeokePHixdupRrrrmGdu3acc455zR47dWrVzNixAi+++47+vbtS1BQUM01R4wYwYEDB9i7d2/NVAcREZEWI3m42RV4LYVZV/KSZeiqQ+zPP//M6NFGY+V3332X5ORk5s2bx5VXXsm+ffuYOnUq/fv3B6Br1641n9+3bx+DBw9m2LBhAI2Gw8zMTJKSToZ9f39/YmNjAYiPj6dNmzYAlJWV8eijj/Ldd98xatSomusuW7aMl19+mXPOOafBa7drZ/yE2rZtWxISEmrVUH39zMxMhVkREfFdGz8AvwDoOhbC257+vt0O5YUQHOn52ryUwqyj/tbAiKulqnlxU5ahu31z02uqx7Zt2wgICOCMM0628Gjbti29evVi27ZtANx6663MmDGDb775hnHjxjF16lQGDBgAwIwZM5g6dSrr1q3jwgsv5NJLL60JxXUpKSkhJCSk0bp2795NcXExF1xwQa395eXlDB48uEnXrlY98ltcXNzosSIiIl5ryWzI3QvXzoNuY2u/t2ORMV2x0yi47CUzqvNKmjPrqKDw+h+BVUGuKcvQ1XdON7vxxhtJT0/n2muvZfPmzQwbNoznn38egIsuuojMzEzuuOMODh06xPnnn8/dd99d77ni4uLIzc1t9JqFhYUALFy4kA0bNtQ80tLSaubNOnvtajk5OcDJ0VsRERGfU3TcCLIASYNPfz80xng/bb5L7q9pKRRmXanWMnS/DrRVrz2wDF2fPn2orKxk1apVNfuOHz/Ojh07SE1NrdmXnJzMzTffzKeffspdd93FK6+8UvNeu3btuO6665gzZw7PPPMM//vf/+q93uDBg0lLS6u1r3o+q9V68ma31NRUgoOD2bdvH927d6/1SE5ObvTadZ2z2pYtWwgMDKRv374OfY9ERES8zqH1xnPb7hDa5vT3k0dATApUFMH2hR4tzZspzLpa6mRjubmoxNr7o5I8tgxdjx49mDJlCjfddBPLli1j48aNXHPNNXTo0IEpU6YAcPvtt/P111+TkZHBunXrWLx4MX369AHggQce4PPPP2f37t1s3bqVBQsW1LxXl/Hjx7N169Zao7OdO3fGYrGwYMECsrOzKSwsJDIykrvvvps77riDt956iz179rBu3Tqef/553nrrrUavHR8fT2hoKIsWLeLIkSPk5Z28Se6nn37i7LPPrpluICIi4nMOrjWek4bU/b7FAgOuMrY3qoNPNYVZd0idDLdvgesWwNTXjOfbN3t0PeU33niDoUOHcskllzBq1CjsdjtffvklgYGBgDG6ecstt9CnTx8mTJhAz549+e9//wsYI6D33XcfAwYMYMyYMfj7+zfY9qp///4MGTKEuXPn1uzr0KEDs2bN4t5776V9+/bMnDkTgH/84x/cf//9zJ49u+baCxcurGm71dC1AwICeO6553j55ZdJSkqqCeYAH3zwATfddJNrv4kiIiKeVL3yV4eh9R8zYJrxnL4YCg67vyYfYLHb7XU1RW2x8vPziY6OJi8vj6ioqFrvlZaWkpGRQUpKikM3NMlJCxcu5C9/+QtbtmzBz8+zPyN99dVX3HXXXWzatImAAOfuadSfuYiIeAW7Hf7dA4qy4YZvjSkF9Xn1AjiwGi78J4xumcu5N5TXfk0js+ISF198MX/4wx84ePCgx69dVFTEG2+84XSQFRER8Rr5B40g6xcACf0bPnZg9VSDD91flw/Qv/7iMrfffrsp173iiitMua6IiIjLRHWAO7bCsZ0Q2Mj9H30vh6yNxvxZu92YS9uKKcyKiIiImM1igeiOxqMxYbEw+Xn31+QjNM1ARERERHyWwmwdWtk9ca2a/qxFRMR0Nht8dD0sfcK5xRAObYAv/wJ7f3ZXZT5BYfYU1W2rtCRq61H9Z139Zy8iIuJxx3fB1s9g6ZPgH+z459a9Dav/Zzy3Ypozewp/f3/atGnD0aNHAQgLC8PSyidVt1R2u53i4mKOHj1KmzZt8Pd376psIiIi9TpY1V82aRD4OxHNBl4Na16DbV9A+VMQFO6W8rydwuyvJCQkANQEWmnZ2rRpU/NnLiIiYorqlb8aWiyhLh2HQ2xXyEmHbQtOtuxqZRRmf8VisZCYmEh8fDwVFRVmlyNuFBgYqBFZERExX/XKX0mDnftc9fK2S2bDpg8UZqU2f39/BR0RERFxr8oyOLzZ2HZ2ZBaM5W2XzIb0JZCfBVGJLi3PF+gGMBERERGzHNkC1nIIjYWYLs5/PrYrJI8Euw02f+Ty8nyBwqyIiIiIWfIOQEAodBjS9JW8Bl4FEQkQ4EQnhBbEYm9ljTbz8/OJjo4mLy+PqKgos8sRERGR1s5aCSW5ENGuaZ+vLAc/f+PRQjiT1zRnVkRERMRM/gFND7IAAUGuq8UHaZqBiIiISEtgrYQ9P4DNanYlHqUwKyIiImKGzOXwwkj49sHmn8tuh5fPhncuMzobtCIKsyIiIiJmOPALZG8zFj1oLosFOo82tjd92Pzz+RBTw+zs2bMZPnw4kZGRxMfHc+mll7Jjx44GP/Pmm29isVhqPUJCQjxUsYiIiIiLVC9j22GIa8434GrjedsXUFbomnP6AFPD7I8//sgtt9zCypUr+fbbb6moqODCCy+kqKiowc9FRUWRlZVV88jMzPRQxSIiIiIuUh1mk1wUZjsOg9huUFFsBNpWwtRuBosWLar1+s033yQ+Pp61a9cyZsyYej9nsVhISEhwd3kiIiIi7lGYDXn7AAskDXLNOWuWt33UWN520G9cc14v51VzZvPy8gCIjY1t8LjCwkI6d+5McnIyU6ZMYevWrfUeW1ZWRn5+fq2HiIiIiKkOVY3KxvWAkGjXnXfANOM5/UfIP+S683oxrwmzNpuN22+/nTPPPJN+/frVe1yvXr14/fXX+fzzz5kzZw42m43Ro0dz4MCBOo+fPXs20dHRNY/k5GR3fQkiIiIijqmZLzvUteeNTYFOowA77FzU6OEtgdesADZjxgy++uorli1bRseOHR3+XEVFBX369OE3v/kN//jHP057v6ysjLKysprX+fn5JCcnawUwERERMc+KF2DtWzDiJuPhSvtWGQspJA5q+hK5JvO5FcBmzpzJggULWLp0qVNBFiAwMJDBgweze/fuOt8PDg4mOLh1rlUsIiIiXmrULcbDHWOKnc5w/Tm9mKnTDOx2OzNnzuSzzz7jhx9+ICUlxelzWK1WNm/eTGJiohsqFBEREXEjd4+c2mzuPb8XMDXM3nLLLcyZM4f33nuPyMhIDh8+zOHDhykpKak5Zvr06dx33301rx9++GG++eYb0tPTWbduHddccw2ZmZnceOONZnwJIiIiIs4pK3D/krNFx2Hen+CF4cYyty2YqWH2xRdfJC8vj3PPPZfExMSax4cfnly5Yt++fWRlZdW8zs3N5aabbqJPnz5MnDiR/Px8li9fTmpqqhlfgoiIiIhzfnwcZifDz8+57xrBkbDjKzi+GzKWuO86XsDUObOO3Hu2ZMmSWq+ffvppnn76aTdVJCIiIuJmB9dDRRGENdyKtFkCgqDfVPjlFdj4IXQf575rmcxrWnOJiIiItHg2Kxxab2y7auWv+gysWt52+wJjakMLpTArIiIi4inHdhqjsoHh0K6Xe6/VYWirWN5WYVZERETEUw6uNZ6TBoGfv3uvZbHAwKolbTd+4N5rmUhhVkRERMRTqlf+ShrsmetVL2+bsRTyDnrmmh7mFYsmiIiIiLQK1SOzrl7Gtj4xnY3R2ZguENAyF5FSmBURERHxlNQpEJkAHYd57pqXveS5a5lAYVZERETEU86+0+wKWhzNmRURERFp6SpKYes8WP+u2ZW4nMKsiIiIiCcc2gD5h8y5dvpi+Og6+O6hFre8rcKsiIiIiCd8ehM81Qd2f+f5a3cfB2FtoegopC/x/PXdSGFWRERExN1K8+DYLmM7cZDnr+8faCxvC7CpZfWcVZgVERERcbdDGwA7RHeC8DhzahhQtbzttpa1vK3CrIiIiIi7HapaLKHDEPNq6DAE2vaAyhJIm29eHS6mMCsiIiLibge9IMxaLDDwKmO7BU01UJgVERERcbeaMOuhlb/q079qedvyYqgsM7cWF9GiCSIiIiLuVHAE8g8AFkgcaG4tMZ3h9i3QJtncOlxIYVZERETEnYLC4PJX4MQ+CI40u5oWFWRBYVZERETEvYIjYcA0s6s4XckJsFZARDuzK2kWzZkVERERaW1W/Bf+3ROWPWV2Jc2mMCsiIiLiLnY7rHwR9v7sXcvIxnYFaxls/ti76moChVkRERERd8ndC4vuhXcuBbvV7GpO6n4+hMVVLW+72OxqmkVhVkRERMRdDq41ntv3g4Bgc2s51anL225839xamklhVkRERMRdDq03ns3uL1uXgVXL225fCKX55tbSDAqzIiIiIu5SPTJr5spf9UkaDHE9obIUtvnu8rYKsyIiIiLuYK2ErI3GdpIXhlmLBQZUL2/7obm1NIP6zIqIiIi4Q/Z2qCiGoEiI62F2NXUbeDUEhED/K8yupMkUZkVERETc4dA64zlpEPj5m1pKvaI7wuiZZlfRLAqzIiIiIu7Qfxq06+NdLblaIM2ZFREREXGHwBBIHg6dRppdSeM2fwxvTTrZfcGHKMyKiIiItHbbF0LGUtjoezeCKcyKiIiIuNrhLbDgDtj6mdmVOKa65+yWj8FaYW4tTlKYFREREXG1vctgzeuw8QOzK3FMt/MgvB0UZcOeH8yuxikKsyIiIiKuVr1Ygjf2l62LfyD0q2rP5SsBvIrCrIiIiIirVbfl8sZlbOszsGoBhR1fQmmeubU4QWFWRERExJVKTsDx3cZ20mBTS3FK4iCI62Usb5vmO8vbKsyKiIiIuFJ1e6uYLhDe1tRSnGKxwKDfQJezITLR7GocpkUTRERERFypZuUvH5kve6ozb4ez7jC7CqdoZFZERETElXIzjecOPhhmLRazK3CawqyIiIiIK01+Dv6yBwb91uxKmq7gCKx9C+x2sytplKYZiIiIiLhaeJzZFTRdRSk8PwTKCyGhv9ePMGtkVkREREROCgyBHhca25u8f3lbhVkRERERV1n2NLx9KWz7wuxKmqd6edvN3r+8rcKsiIiIiKuk/wjpi6HwqNmVNE/18rbFx2D392ZX0yCFWRERERFXsNtP9pj1pZW/6nLq8rabvHt5W4VZEREREVfISYfSE+AfDO37ml1N81Uvb7vdu5e3VTcDEREREVc4WLVYQkJ/Y2TT11Uvb5uTbrTpikqCiPbQeTT4+ZtdXQ2FWRERERFXqF75y9enGFSzWGDwtbDyBfj2/pP7o5JgwuOQOtm82k6haQYiIiIirnBwrfHs5X1ZHZY23wixBVm19+dnwdzpxvteQGFWRERExBVCYyE4umWMzNqssOgeoK4VwKr2LbrXOM5kmmYgIiIi4gr/9wHYbMav531d5nLIP9TAAXbIP2gcl3K2x8qqi8KsiIiIiKv4tZBfehcece1xbtRCvuMiIiIiJqooNbsC14po79rj3EhhVkRERKS5Xh0Hzw48eROYr+s82uhaQH1TJiwQ1cE4zmQKsyIiIiLNUV4MR9Mgdy9EJJhdjWv4+Rvtt4DTA23V6wmPeUW/WYVZERERkeY4vAnsViPIRiWZXY3rpE6GaW9DVGLt/VFJxn4v6TOrG8BEREREmuPU/rItoZPBqVInQ++Lja4FhUe0ApiIiIhIi1O9jG1LWSzh1/z8TW+/1RBNMxARERFpjuplbJNaaJj1cgqzIiIiIk1VnAM56cZ20mBza2mlNM1AREREpKkqy2Do74z5pGGxZlfTKinMioiIiDRVVCJMesbsKlo1TTMQEREREZ+lMCsiIiLSFHa70cmgsszsSlo1hVkRERGRpsg/BK+Mhcc6K9CaSGFWREREpCmqW3K17Q4BwebW0oopzIqIiIg0Rc1iCWrJZSaFWREREZGmqF7GVoslmEphVkRERMRZNhsc2mBsdxhqaimtncKsiIiIiLNy9kBZHgSEQHwfs6tp1RRmRURERJxVPV82cSD4B5pbSyunFcBEREREnNVhKFzwMIS3M7uSVk9hVkRERMRZcd0h7jazqxAUZkVERMQsNitkLofCIxDRHjqPBj9/s6sSH6MwKyIiIp6XNh8W3WOsolUtKgkmPA6pk82ryxEn9sG+VdBxGMSmmF1Nq6cbwERERMSz0ubD3Om1gyxAfpaxP22+OXU5ate38OmNsOAOsysRFGZFRETEk2xWY0QWex1vVu1bdK9xnLeqXsZW/WW9gsKsiIiIeE7m8tNHZGuxQ/5B4zhvVbOMrVb+8gYKsyIiIuI5hUdce5ynlRdB9nZjWyOzXkFhVkRERDwnor1rj/O0rI1gt0FkEkQmmF2NoDArIiIintR5tNG1AEv9x0R1MI7zRgfXGs+aYuA1FGZFRETEc/z8jfZbDRl9q/f2m9V8Wa+jMCsiIiKelToZLqoj0PoHG89rXoOyAs/W5Kjxj8C0d6DPFLMrkSpaNEFEREQ8r+iY8dxhGIycYcyRbdsdXjkPju2E+bfClW+YW2NdopK8f1GHVkYjsyIiIuJZdjts+tDYHjkD+l8BKWdDVCJMewuik2HETebWKD5DI7MiIiLiWftXwYlMCIqAXhNrv5c8Av68DgKCzKmtIVs+heO7oddFkNDf7GqkisKsiIiIeFb1qGyfSRAUdvr7pwbZo9sgOAqiO3imtoZs+hB2LjLqUZj1GppmICIiIp5TWW6McAIMuKrhY3d9C6+cDx9dZ3zOTHa7Ohl4KYVZERER8Zy8/RDeDiISIGVMw8e27Qb+AXDgF/j6b56prz55B6DoKPgFaFTWyyjMioiIiOe07QYzf4E/LGm8l2xsV7j8FWP7l1dg4wduL69eh6pGZeNTITDUvDrkNAqzIiIi4lkWi9G5wBE9x8M59xrbX9wGWZvcV1dDtPKX11KYFRERcTebFTJ+gs0fG882q9kVmSN3L1SUOP+5c+6B7hdAZSnMvRZKcl1eWqNq5ssO9fy1pUEKsyIiIu6UNh+e6QdvXQKf3GA8P9PP2N/azLsFnugBO7927nN+fnD5/6BNJyMQr3jBLeXVy243FnIASNLIrLdRay4RERF3SZsPc6cD9tr787OM/dPebj2rSZ3YD5nLjO32fZ3/fFgsXDXH6IRQPe3AUywWuCMNsrdDu96evbY0SmFWRETEHWxWWHQPpwVZqNpngUX3Qu+LG78RqiXYPNd47nI2RHds2jkSBxoPM/gHQEI/c64tDdI0AxEREXfIXA75hxo4wA75B43jWjq7HTZWLZTQWG9ZR1WWw7cPGNMOpFVTmBUREXGHwiOuPc6XHd4Ex3aAf7DrplV8fR/8/KwxXaMpN5U54+Mb4LMZcHyPe68jTaIwKyIi4g4R7V17nC/bVDXFoNdFEBLtmnOeeTuEtYWsjbDwbmP01x0qy2DbfNj4HlgUm7yRqX8qs2fPZvjw4URGRhIfH8+ll17Kjh07Gv3cRx99RO/evQkJCaF///58+eWXHqhWRETECZ1HQ1RSAwdYIKqDcVxLZrMZLcnAdVMMANokwxWvGwFzwxxY+6brzn2qI1vAWg6hsRDTxT3XkGYxNcz++OOP3HLLLaxcuZJvv/2WiooKLrzwQoqKiur9zPLly/nNb37DDTfcwPr167n00ku59NJL2bJliwcrFxERaYSfP0x4vOFjJjzW8m/+8vOD330J5/0duo9z7bm7ngvnP2Bsf/VXOLDWteeHU/rLDjG6GojXsdjt7hqXd152djbx8fH8+OOPjBlT93rNV111FUVFRSxYsKBm38iRIxk0aBAvvfRSo9fIz88nOjqavLw8oqKiXFa7iIjIabJ3wAsj6n7vgn/CmX/2bD0tkd0OH14D2xdAVEf4448QHue688/7E2x4F8b8Fc77f647rzTImbzmVZM/8vLyAIiNja33mBUrVjBuXO2f7MaPH8+KFSvqPL6srIz8/PxaDxEREY/YUTUNrudFcN0CmPoadKkarMneZl5dLYnFApe+CG27Q1m+8QOEK2kZW6/nNX1mbTYbt99+O2eeeSb9+tXfx+3w4cO0b197snz79u05fPhwncfPnj2bWbNmubRWERERh5x5u9FX1T8IEgcY+2K6wKvnw+aPYNxDEBFvYoFutvUzY77s8Bug23nuu05IFFz9Hlj8Ia67685bVnAyHGvlL6/lNSOzt9xyC1u2bOGDDz5w6Xnvu+8+8vLyah779+936flFRETqZbFAx2EngywYrzsON24qWvO6ebV5wvp3jV//71vp/mu161U7yForm3/OgsMQ1xOiO0FkK+g64aO8IszOnDmTBQsWsHjxYjp2bHhVkISEBI4cqd2T78iRIyQkJNR5fHBwMFFRUbUeIiIibmW3G6N69Rk5w3j+5VWj9VNLVHgU9vxgbLuyi4Ej0pfAf4bBsd3NO09cD5i5Gv68xiVliXuYGmbtdjszZ87ks88+44cffiAlJaXRz4waNYrvv/++1r5vv/2WUaNGuatMERER5xz4Bf7dCxbdV/f7fSYbbbmKsmHLJ56tzVO2fAJ2K3QYBm27ee66djss/TfkZhg3hpUVNv+cAcHNP4e4jalh9pZbbmHOnDm89957REZGcvjwYQ4fPkxJycmVPKZPn8599538y+C2225j0aJFPPnkk2zfvp2HHnqINWvWMHPmTDO+BBERkdP98ipUFEFpPTcd+wfCqJkw8P8gabBna/OUTS5evtZRFotxo11EgnGT3fw/N31BBZvVtbWJW5gaZl988UXy8vI499xzSUxMrHl8+OGHNcfs27ePrKysmtejR4/mvffe43//+x8DBw7k448/Zt68eQ3eNCYiIh5is0LGT8ZNPxk/tc4wUHTMuPEJjBuf6jPqT3DZixDfxzN1eVL2Tji03rghq9/lnr9+ZHu48k3wC4Ctn8LKF50/R9ExmN0RXr/INfNvxW1M7WbgSIvbJUuWnLbvyiuv5Morr3RDRSIi0mRp82HRPZB/6OS+qCRj4YDUyebV5Wnr3zFu7koa0nrbOVWPynYf59qer87oPAoufMT4b/Kbv0PSIOdWWzu4DiqKofgY+HtN8yepg1fcACYiIj4ubT7MnV47yALkZxn70+abU5en2awnOxQMv9GxzxzeDPNugZwM99XlaXE9jOkTAz08xeDXzvgj9L/SmLs79zqjO4GjavrLDnVPbeIy+lFDRESax2Y1Rr+o67dtdsACi+6F3he3/KVbd30LJ/ZBSBvHf73+3UOw+zujV+qE2e6sznMGXm08zF5k1GKBSc/Cka2QOBCCnehodKhqGVv1l/V6GpkVEZHmyVx++ohsLXbIP2gc19JVj8oOvgYCQx37THWbrnXv1H/DmK+yWMyuAILC4XdfGauEBYU59hm7XSt/+RCFWRERaZ7CI40f48xxvuySp2DMXxq+8evXup0Pcb2gvAA2vOu+2jyhsswI5SUnzK6kttA2J4O1zQZH0ho+/sQ+KD4OfoHQXjeYezuFWRERaZ4IB1dGcvQ4XxbdEc77O8R2dfwzFguMvNnYXvWSb3eA2PUNzJ8J/zvX/CkGdSkrhA9+YywnfGRr/cdVTzFo3xcCQzxTmzSZwqyIiDRP59FG14J6WYwFApy5k7y1GXC1Mc82dy/sXGR2NU1X3cWgzyTvmGLwa4GhxuhxRbGxoEJpXt3HhbSBnhcZ3RjE6ynMiohI8/j5G+236lQVaCY81rJv/tr4Icy5AtJ/bNrng8Jg2O+M7ab0RPUGJbmw82tj29MLJTjKz99YUCE6GXLS4bMZxrSDX+s2Fv7vAzj/fs/XKE5TmBURkeZLnQzT3jl9hDYqCaa93fL7zK7+H+z+1ljGtqmG32SMYKeMqTtgebu0z43+uvF9IcGL55mGtzX+m/QPgh0L4eenza5ImkmtuURExDVSJxvttzKXGzd7RbQ3pha05BFZMFa6OrjGuFloyHVNP090B7h9C/j56DjTxurla6eZW4cjOgyBif+GL26FH/5p9MTtdp7xXskJKC8yfhDzxqkSchof/T9GRES8SsERY8lPP39IORv6X2E826yw8QPfvqmpMb+8Zjz3vRQi2jXvXL4aZHMzYd9ywGIsUuALhl4Hg68Fu81YtKKyzNi/fQE8nWrMqRWfoJFZERFpvk9vNFayuvxV6FF104zdDq+Ng6yNYPHzjRE7Z5XkwuaPjW1HV/xqjM0KO76C8kJj4QFfsH+V8Wfc5SxjhNlXTPy38Wd49l0QEGx877ctMN7zDzRet/TfLLQACrMiItI8pXnG1AJbJbQ9pSWVxQKpU4wwu2Q29L3MCAgtyYb3oLLE6EWafIZrzrl9Icy9FsLbGd+zgGDXnNedBkyDLmdDSY7ZlTgnMASururtmzbfWMmuegGQrZ8ZIX3C4y1/zreP89HfZ4iIiNfY/Z0RZON6nd5fdcQfISzOuHN84/vm1OcuNtvJKQbDb3Dd/MpeF0FkEhRlw5ZPXHNOT4hKNPqy+qK0+TB3+ukr2eVnGfvT5ptTlzhEYVZERJpnx1fGc68Jp78XHAFn32ls//ivk/MSWwK7Dc68DTqfCf1dOIXCPxBG3GRsr/yvdy4+cKryYrMraB6b1RiRpa7vc9W+Rfe27HnfPk5hVkREms5aCbu+NbZ7XlT3McN+D5GJkLcf1r3tudrczT/AuInod18aod2Vhl4PAaHGPOTMn117bleyVsLzQ+DtKaePavqKzOWN1G6H/IPGceKVFGZFRKTp9q+E0hMQGgvJI+o+JjAUxtxtbC99wvdH8jwhLPbkzV/evIhCxhIoyIKsTcYcX19UeMS1x4nHKcyKiEjTVU8x6HFhw3d9D54ObTpBQn8j/Pq6X16F1a9Aab77rnHGzcbz9oWQk+G+6zTHprnGc7+pvntzX0R71x4nHqduBiIi0nQDroKAEKMlU0MCguAPPxojjr6ushyWPGbcoBUeZ3QccIf43tDtfKNDQEkukOKe6zRVWSFs+8LY9tblax3RebSxQEJ+FnXPm7UY73ce7enKxEEKsyIi0nSJA4yHI1pCkAXYNt8IshEJ0PsS915r2lsQFOGdK1FtXwgVxUYHi47DzK6m6fz8jfZbc6cDFmoH2qrv+4TH1G/Wi2magYiIeFbBEfj6/1WNNvqg6nZcQ693/6/WgyO9M8gCbKpevvYq763RUamTYdrbRnuxU0UlGfvVZ9araWRWRESaZtkzENfTWNM+MMTxz33wGzi41rgx7Ly/u608tziy1Vi21eJvdDLwlJITsOVjGPo77xghLDgC6YuNbV9ZvrYxqZOh98VG14LCI8Yc2c6jveP7LQ1SmBUREecV58D3D4PdCrdtgpjOjn/2rDuMde9Xvmjc5BQe5746Xa16VLb3xcaonSfYrPDSWUZrs8hE49pmC46ASc/B4U3QtpvZ1biOnz+knG12FeIkTTMQERHn7f7OCLLxfZ0LsmDMM00cCOWF8PMzbinPLUrzT/5qffiNnruun7/RLQC8p01XUDgMuRYmPmF2JSIKsyIi0gQNrfrVGIsFxlZNL1j9ChQcdl1d7lSaB13PhXZ9IGWMZ6894iZjasPen4yeriJSQ2FWREScU1lujMxC/at+NabHBdBxBFSWwk9Puq42d2qTDFe/C3/80fM3PEV3hNQpxvaqlzx77V9b8was+C8UZptbh0gVhVkREXHOvuVQlm+s+NRhaNPOYbGcvPlrzRtwYp/r6nO3gGBzrjvyT8bz5o+g8Kg5Ndhs8NNT8PV9xiixiBdQmBUREefsWGQ89xgPfs34Z6TrOdB9nDH/NDDcNbW5y8YPzV+FK3k4dBgG1nJY87o5NexbAXn7ICgSejVxVF7ExdTNQEREnHN8l/HclPmyv/bbj72/R2nBYfj8T0ZXgVvXQ6yJK3GNnAGf/gGKj5tz/eob4FKnGK3VRLyAwqyIiDjnmk/g+B6jTVRzeXuQBVj3NtgqIfkMc4MsGCGy00hjDq2nVZTC1nnG9kAfXr5WWhxNMxAREee17QZBYa4734G1MGcqHN3uunO6grXSmNMLnm3HVR//QHOCLMCub6AsD6I6QOezzKlBpA4KsyIi4jhrhXvOu+wpo0PCktnuOX9T7fwKCg5BWNzJbgLe4tguyN7puetVTzHof0Xz5kqLuJj+axQREccUHYN/dTVW76osd+25x/4NsEDaPO/qo/rLq8bzkOnmdTGoy6r/wX+GwfezPHM9u934+v0CYYCmGIh3UZgVERHH7PrGaMmVuxcCglx77vZ9T65ytfhR1567qY7tgvQlgAWG/c7samqrXrRh+0LPdFmwWOCK1+Evu40/KxEvojArIiKOqV71q6kLJTTm3PvA4mf8av/AGvdcwxnZ2yE4GnpOgDadzK6mtvje0O18wG6souYpoW08dy0RBynMiohI4yrLYM8PxrYrWnLVJa47DPw/Y/uHf7rnGs7oMwnu2gYTnzC7krpVL6Kw7m0ozXffdQqzje4VIl5KYVZERBq3dxmUF0JEAiQOdt91zvmrMS8zfTFkLnffdRwVFG4sY+uNup0HcT2hvAA2vOe+66x7E54fAl/+1X3XEGkGhVkREWnczqpVv3o2c9WvxsR0hjF3w8VPGatdmcFuN1qF2e3mXN9Rfn5wxs3G9qqXjEUdXM1uh01zje3Ega4/v4gLKMyKiEjD7PaTS9h6YgnTc++F4Te4/iYzRx1YA6+eB6+cBzabOTU4auDVENIGSk9ATrrrz5+1AY7thIBQY9qFiBfSCmAiItIwa4VxN/+eHyDlHM9f2y/AsyuFVbfjiu/j/f1Ug8Lhmk+NWl25iEW1jVW9ZXtPhJAo159fxAW8/P9SERExXUAQnH0nXL/APYGpPls+hf8MPznFwROKjsPWT43t4Td47rrN0XGoe/5crJWw5WNjW71lxYspzIqIiHc6vBlyM+CHRzz36/7174C1HJIGQ4ehnrmmq9hscCTNdedLXwJF2cbqZ93Oc915RVxMYVZEROpXdMy4Aag4x/PXHv1nCI6CI5th2+fuv57NCmteN7aH3+j+67lScQ789wz437lGKy1X2PqZ8dxvKvgHuuacIm6gMCsiIvXb8SV8ehO8e4Xnrx0WC6NuMbYXP+qeu/VPtfs7OJFp3FDV93L3XsvVwmKN4G8tg7VvuOacF/8brngDhv3eNecTcROFWRERqV91F4Me4825/sgZEBpj3FG/+SP3Xmv7AuN58DWenRvsKiNnGM+/vGosctFcgaHQ73JjtTERL+ZQN4PnnnvO4RPeeuutTS5GRES8SEWpsXgBuG/Vr8aERMOZt8F3D8GS2e79lfclz0Lfy6Btd/ec391Sp8A390PBIWOKwMCrza5IxCMsdnvjXaFTUlJqvc7Ozqa4uJg2bdoAcOLECcLCwoiPjyc93Q197lwoPz+f6Oho8vLyiIpSmxERkXrt/AbeuxKiOsAdWz3bHutU5UXw7EDjZqSr34PeF5tThy/46Un4/mFIGAB/XNq0P7OCw/D2FOh3BZx9l/e3J5MWyZm85tB/oRkZGTWPRx55hEGDBrFt2zZycnLIyclh27ZtDBkyhH/84x8u+QJERMQL7PzKeO453rwgC0Yv1YufhOu+cE+QrSw3RqFbgqG/g4AQOLwJ9q1o2jk2fwzZ22HXNwqy4hOc/q/0/vvv5/nnn6dXr141+3r16sXTTz/N3//+d5cWJyIiJrHbYefXxnaviebWAsav0FPGuOfcWz+Fp1Nh+X/cc35PCos9Ob0gbX7TzrGpaqGEgeotK77B6RXAsrKyqKysPG2/1WrlyJEjLilKRERMdmwn5B+EwHDocrbZ1dRWdAwCw1x3k9Yvr0LxcahsIaOzZ95mdGNoSvg/us0Y1fUL8L2ODtJqOT0ye/755/PHP/6RdevW1exbu3YtM2bMYNy4cS4tTkRETNKuF9y5Daa9DYEhZldz0qqX4ZkB8MsrrjnfoQ1w4BfwC4Qh011zTrPFdoWu5zRtasimucZzjwuNUV4RH+B0mH399ddJSEhg2LBhBAcHExwczIgRI2jfvj2vvvqqO2oUEREzRCVBDy8bpAiOhIoiWPYMlOY3/3xrXjOeU6dARHzzz+dtygqhvNixY222k+3PtHyt+BCnpxm0a9eOL7/8kp07d7Jt2zYsFgu9e/emZ8+e7qhPRETkpP7T4Ken4PguWPUSnPPXpp+r5ARsqgpvvrbilyNW/BeWPAZj/wYjb278+H0rIG+/sfhCT5NasYk0QZNvU+zZsyeTJ09m0qRJCrIiIi3J+nfhncuafgORO/kHwNj7jO3lzzdvmd2N70NlCcT3hU4jXVOfNwkIgrI8I/Q7snpaaAwM/A0M+q13TS0RaUSTwuzbb79N//79CQ0NJTQ0lAEDBvDOO++4ujYRETHDtvmw5wc4tsPsSuqWepkRQMvyYUUTOxDY7fBL1RSD4TeY23rMXQb+xlh0IjfjZGeKhrRPhctegosec39tIi7kdJh96qmnmDFjBhMnTmTu3LnMnTuXCRMmcPPNN/P000+7o0YREfGU8mJIX2Jse0NLrrr4+cF5/8/YXvkSFGY7fw6LBa5+F0beAgOmubY+bxEUDkOvN7ZX/tfUUkTcyaEVwE6VkpLCrFmzmD699l2fb731Fg899BAZGRkuLdDVtAKYiEgDdnwF718N0Z3g9k3eO2Jpt8MrY+HwFrjidUidbHZF3unEfmP1NLsVbl4GCf3rPm7dO5A4wFg5zFv/zKVVcfkKYKfKyspi9OjRp+0fPXo0WVlZzp5ORES8yY4vjedeE7w71FgsMPl5+PNaBdmGtEk++f1Z+VLdxxTnwII74OUxcHy352oTcRGnw2z37t2ZO3fuafs//PBDevTo4ZKiRETEBDbbybmVvnA3e0J/iOns/Od+fAI+/j1kbXR9Td5o5J+M5y2f1N3ObOtnYKswvp9x+ndcfI/TrblmzZrFVVddxdKlSznzzDMB+Pnnn/n+++/rDLkiIuIjstZD4REIioAuZ5ldjXOyNkJYHER3aPi4ynJjwYXCI9D7Ekgc6Jn6zNRxOFzwD+gzCULq+HVt9UIJ6i0rPsrpkdmpU6eyatUq4uLimDdvHvPmzSMuLo7Vq1dz2WWXuaNGERHxBJsNuo6FXhdBQLDZ1Thu6RPGr8iXzG782O0LjCAb0d4Id62BxQJn3gqxKae/l5MB+1eCxQ/6XeH52kRcwOmRWYChQ4cyZ84cV9ciIiJmSh4O0+cZN1f5kpRzgH/ChvfgrDugbbf6j61uxzX0evAP9ER13qey3OhBC7D5Y+M55RyISjSvJpFmaFKYtVqtzJs3j23btgHQt29fJk+ejL+/v0uLExERE3jzjV91SR4BPcbDrq+NFa+mvlL3cUfSIHMZWPxhyHWerdEb5GbCN/8P8g7CTT8Y+zZ9aDxrioH4MKfD7O7du7n44os5cOAAvXr1AmD27NkkJyezcOFCunVr4CdiERHxTke3Gw32fXV0buzfjDC7+SM4+06I73P6MWuqRmV7T2x8bm1LFBwJu76FylJY9TL4+UPRUfAPgT6XmF2dSJM5PWf21ltvpWvXruzfv59169axbt069u3bR0pKCrfeeqs7ahQR8S42K2T8ZPyKNuMnx5YK9Xbf/D94qrfRb9QXJQ2qmgNrh8WPnv5+WQFs/MDYHn6jJyvzHmGx0GmUsb3oHvjybijNg5BI2LPY3NpEmsHpkdkff/yRlStXEhsbW7Ovbdu2PPbYYzXdDUREWqy0+UYQyD90cl9UEkx43Hf7nZYVQsZSYzt5hLm1NMe5f4NtC4zleA9tMAJuDQucey+k/1g1x7YVSpsP6XWE1qJjMHc6THvbd/8bllbN6ZHZ4OBgCgoKTttfWFhIUFCQS4oSEfFKafONf/RPDbIA+VnG/rT55tTVXOmLwVoOMSkQ19PsapqufSr0vwLC46HgcO33giNg9J/hmo99b06wK9isxg9hdaq64W/RvS3jtwzS6jgdZi+55BL+8Ic/sGrVKux2O3a7nZUrV3LzzTczebJ+ohORFqomDNR1p7+Ph4Edi4znXhf5ftAbPxtu22isYCYnZS4//YewWuyQf9A4TsTHOB1mn3vuObp168aoUaMICQkhJCSEM888k+7du/Pss8+6o0YREfO11DBgs8LOqjDrC6t+NSaiHQSFGdvVc5vnXgffPQxlRebWZqbCI649TsSLOD1ntk2bNnz++efs2rWL7du3A9CnTx+6d+/u8uJERLxGSw0DB9dC8TEIjobOo82uxnW2fg4LboOS3JP71r8NFz/VOueFRrR37XEiXqRJfWYBevToQY8eWsNZRFqJlhoGdnxlPHc/v+UsIpA2Hz6afvr+1nyjU+fRxo2K+VnUPVXGYrzfkn6gkVbD6TBrtVp58803+f777zl69Cg2m63W+z/88IPLihMR8RotNQyM/rPRkzU62exKXKPRG50sxtzm3hcbfVZbCz9/o+PG3OmAhdr/DVfNk57wWOv6nkiL4XSYve2223jzzTe5+OKL6devHxZfv1lARMQRNWHg2jrerAoH4x/1vTAQFgsDppldhes4M7c55WyPleUVUicbo9J1tpZ7rPWNVkuL4XSY/eCDD5g7dy4TJ050Rz0iIt4rdTLEdoOcPbX3+/mDrRJO7DOnLjmppc5tdpXUycaodOZy43sQ0d74bYKv/RAmcgqnw2xQUJBu9hKR1sluhxF/MFaSOvtOozdrRHsj3H5xG/zwT+g5Htr1MrtSx3z9/yCsLQy+BiLiza7GNVrq3GZX8vNvfaPS0qI53Zrrrrvu4tlnn8Vur2vOmIhIC2axwMib4Y9LjBGu/lcYoWDIddB9HFjLYN4MsFaaXWnjSvNh1cvw/SxjSdOWonpuM/VNgbNAVAffm9ssIvVyaGT28ssvr/X6hx9+4KuvvqJv374EBta++/XTTz91XXUiIr7AYoFJz8F/RxmtrpY/C2ffZXZVDdvzPdgqoG13iGtBnWl0o5NIq+NQmI2Ojq71+rLLLnNLMSIiXuvQBji8CVKnQEj06e9Hd4CLHjNGZhfPNhYgaN/X42U6bEcLWijh13Sjk0irYrG3svkC+fn5REdHk5eXR1RUlNnliIivmHcLbJgDQ6+HSfWsdmi3w/u/gZ1fQfcL4JqPPVqiw2xWeKI7lOTA9Quhy1lmV+QeNqtudBLxUc7ktSYvmiAi0mqUF0Pa58b2gKvqP85igUnPwOJ2MG6WR0prkv2rjSAb0gaSR5pdjfvoRieRVsGhMDtkyBC+//57YmJiGDx4cIO9ZdetW+ey4kREvMKOL6G8ANp0bjz8RSbA5Oc9U1dT7axa9avHheCvMQ0R8W0O/S02ZcoUgoODAbj00kvdWY+IiPfZ+L7xPPBq8HOiCYzdbiwX230cBAS5p7amsFZCYBj0aoHzZUWk1dGcWRGRhhQchqf6gN0Gf14Hbbs5/tn5t8K6t+Cce2Ds39xXY1NUlBrTIgKCza5EROQ0zuQ1p/vMioi0Kps/MoJs8hnOBVmArucazz89aXRD8CaBIQqyItIiODTNICYmpsF5sqfKyclpVkEiIl7lxD7AYkwxcFa/y40bx9Lmwbw/wR8Wmx8g87MgKtHcGkREXMihMPvMM8+4uQwRES818Qk46w4Ijmza5y9+EvYug6Nb4cd/wfn3u7Y+Z5TkwtN9jRHmG7+ru1+uiIiPcSjMXnfdde6uQ0TEe0UlNf2z4XFwyVPGilTLnobeE6HDUNfV5ozd34PdClgUZEWkxWjSnNk9e/bw97//nd/85jccPXoUgK+++oqtW7e6tDgREdNYK2uvHtUcqVOg3xVGkJx3i9HM3ww7qlpy9brInOuLiLiB02H2xx9/pH///qxatYpPP/2UwsJCADZu3MiDDz7o8gJFREyRvgSeSoWPb3DN+SY+AR2Hw0WPm7MKlbUCdn9rbCvMikgL4nSYvffee/nnP//Jt99+S1DQyb6J5513HitXrnRpcSIiptn4PmCHsLauOV9YLNzwLXQ9xzXnc9a+lVCaZ3w9HYebU4OIiBs4HWY3b97MZZdddtr++Ph4jh075pKiRERMVZoP2xcY203pYlCfU7vC5GZCRYnrzt2YnYuM5x4XmjMyLCLiJk6H2TZt2pCVlXXa/vXr19OhQweXFCUiYqq0z6GyFOJ6QdJg159/01z47yj44Z+uP3d9qufL9tSqXyLSsjgdZq+++mruueceDh8+jMViwWaz8fPPP3P33Xczffp0d9QoIuJZGz8wngdeXXs01VVC2kBFEax4ATJXuP78v2a3w4X/gCHTodt57r+eiIgHOR1mH330UXr37k1ycjKFhYWkpqYyZswYRo8ezd///nd31Cgi4jm5mZC5DLDAgGnuuUbPC2HwNYAdPv8TlBe55zrVLBbofTFMfh5CtIy3iLQsTofZoKAgXnnlFdLT01mwYAFz5sxh+/btvPPOO5SXl7ujRhERz9k013hOGQPRHd13nfGPQlQHyEmH7x9233VERFo4p8PsrbfeCkBycjITJ05k2rRp9OjRg6KiIiZOnOjyAkVEPOqMPxgjmGfe6t7rhETD5OeM7VUvGauEuUNxDnz/Dziw1j3nFxExmdNhduHChaf1ky0qKmLChAlUVla6rDAREVOERBtzS7uPc/+1uo+DIVUrLM77E5QVuv4au76Fn/4N8//s+nOLiHgBp8PsN998wyuvvMIzzzwDQEFBARdccAEWi4VFixY5da6lS5cyadIkkpKSsFgszJs3r8HjlyxZgsViOe1x+PBhZ78MERHvcOE/IaYL9JsK/oGuP//O6lW/1MVARFqmAGc/0K1bNxYtWsTYsWPx8/Pj/fffJzg4mIULFxIeHu7UuYqKihg4cCC///3vufzyyx3+3I4dO4iKOnkTQ3x8vFPXFRE5TWUZvHsF9JoIQ38HgSGeuW5IFPxpJQSGuv7cleWw+3tju5emgYlIy+R0mAUYMGAACxYs4IILLuCMM85gwYIFhIY6/xfxRRddxEUXOb+sYnx8PG3atHH6cyIi9dr1DWQshWO7YMQfPHvtU4OstRKs5RAU1vzz7lsOZfkQHg9JQ5p/PhERL+RQmB08eDCWOnotBgcHc+jQIc4888yafevWrXNddfUYNGgQZWVl9OvXj4ceeqjW9UVEmqS6t+yAaeatkHV0O8y7GRIHwqRnm3++moUSLgQ/p2eViYj4BIfC7KWXXurmMhyTmJjISy+9xLBhwygrK+PVV1/l3HPPZdWqVQwZUveoQ1lZGWVlZTWv8/PzPVWuiPiK4hzY+bWxPcCFy9c6XccxOLTeePSZDN3Pb/q57PZTwqzzvwETEfEVDoXZX3cvMEuvXr3o1atXzevRo0ezZ88enn76ad555506PzN79mxmzZrlqRJFxBdt+QRsFcaIaPtU8+rochaccbPRqmv+n+FPK4zuCk1ReMSYYuAfDN3GurZOEREv4vO/dxoxYgS7d++u9/377ruPvLy8msf+/fs9WJ2I+ISaKQYmjspWO/8BiO0K+Qfh6781/TyRCXD3bvjDYghy7uZcERFf4tDIbGxsLDt37iQuLo6YmJg6589Wy8nJcVlxjtiwYQOJiYn1vh8cHExwcLAHKxIRn3JsFxxcAxZ/6H+F2dUYwXPKf+GNi2D9HGO6Qc/xTTuXfwC07+va+kREvIxDYfbpp58mMjISoKa/rCsUFhbWGlXNyMhgw4YNxMbG0qlTJ+677z4OHjzI22+/XXPtlJQU+vbtS2lpKa+++io//PAD33zzjctqEpFWxlYJvS8xtiO8pM1f51Ew6hZY8R+YfyvcshJCYxz/vLXSuImtgYEHEZGWwqEwe91119W5fari4mI2bNjg1MXXrFnD2LEn53LdeeedNdd48803ycrKYt++fTXvl5eXc9ddd3Hw4EHCwsIYMGAA3333Xa1ziIg4Jb4PXP2uccOUNznv77BzEYS0gdJ858Ls5rnwwyPG0rxn3ua2EkVEvIHFbnfN3+AbN25kyJAhWK1WV5zObfLz84mOjiYvL6/WwgsiIl4n7wBEJBjTBZzx4TWw7QsY81c47/+5pzYRETdyJq/5/A1gIiJNtm0BHN9jdhX1i+5YO8g6MvZQWQZ7FhvbvdSSS0RaPoVZEWmdyovhsz/C80Mga6PZ1TSsohS+fcCotzF7f4LyQmNEN3GQ20sTETFbk5azFRHxedsXGqGvTWdIGGB2NQ3L3g7L/wN2K/S+GFKn1H/sjkXGc8/xWvVLRFoFh8Ps/PnzG3w/IyOj2cWIiHjMxveN54G/8f67/pMGwVm3w09PwoI7ofOZEB53+nGnrvqlKQYi0ko4HGYdWdK2of6zIiJeIz8L0qvmlQ68ytxaHHXOPcao69GtsPAumPbW6ccc2QL5ByAgFFLO8XyNIiImcPh3UDabrdGHt3cyEBEBYPNHYLdB8khjtS1fEBAMl/4X/AIgbR5s+fT0Y4LCjeVwB/0fBIV5vEQRETNoQpWItD6bPjSeB3rB8rXOSBoEZ99tbC+8CwqP1n4/titc9Dhc8pTHSxMRMUuzwmxUVBTp6emuqkVExP3ys+DEPvAPhr6Xml2N886+CxL6g7XCmFYgItLKNaubgYvWWxAR8ZyoRLh7p9GOy5lVtbxFQBBMfR0CQ6BNJ7BZIXO58QgKh+E3Gu+JiLQSas0lIq1PYCh0Gml2FU3XrqfxnDYfFt0D+YdOvvfjYzDlv5A62ZzaREQ8rFnTDK655hotCSsivqOswLFVtHxB2nyYO712kAXja5w73XhfRKQVaFaYffHFF4mLq6PXoYiIN/riNvjPMNj9vdmVNI/NaozI0kAwX3SvcZyISAvn9DSD5557rs79FouFkJAQunfvzpgxY/D39292cSIiLlOaZ6z6VVnqm3NlT5W5/PQR2VrskH/QOC7lbI+VJSJiBqfD7NNPP012djbFxcXExBj/IOTm5hIWFkZERARHjx6la9euLF68mOTkZJcXLCLSJGmfG0E2rhckDTa7muYpPOLa40REfJjT0wweffRRhg8fzq5duzh+/DjHjx9n586dnHHGGTz77LPs27ePhIQE7rjjDnfUKyLSNBs/MJ4HXu39y9c2JqK9a48TEfFhFruT/bW6devGJ598wqBBg2rtX79+PVOnTiU9PZ3ly5czdepUsrKyXFmrS+Tn5xMdHU1eXp5uXhNpLXL3wrMDAQvcsQWiO5pdUfPYrPBMP6Nnbp3zZi0QlQS3bwY/TfkSEd/jTF5zemQ2KyuLysrK0/ZXVlZy+PBhAJKSkigoKHD21CIi7rFprvGcMsb3gywYAXXC41Uvfj3KXPV6wmMKsiLSKjgdZseOHcsf//hH1q9fX7Nv/fr1zJgxg/POOw+AzZs3k5KS4roqRUSaym6Hje8b2wN/Y24trpQ6Gaa9bSwCcaqoJGO/+syKSCvh9A1gr732Gtdeey1Dhw4lMDAQMEZlzz//fF577TUAIiIiePLJJ11bqYhIU138FGz+CPpMMrsS10qdDL0vNroWFB4x5sh2Hq0RWRFpVZyeM1tt+/bt7Ny5E4BevXrRq1cvlxbmLpozKyIiIuLdnMlrTV7Otnfv3jUB1uLrdwaLiIiIiE9q0gpgb7/9Nv379yc0NJTQ0FAGDBjAO++84+raRESaZ/d38PX/gyNpZlciIiJu4vTI7FNPPcX999/PzJkzOfPMMwFYtmwZN998M8eOHVN/WRHxHmvegO0LwC8ALphldjUiIuIGTofZ559/nhdffJHp06fX7Js8eTJ9+/bloYceUpgVEe9QdBx2fm1sD7za3FpERMRtmtRndvTo0aftHz16tFcukiAirdTWT8FWAYkDIb6P2dWIiIibOB1mu3fvzty5c0/b/+GHH9KjRw+XFCUi0mwtsbesiIicxulpBrNmzeKqq65i6dKlNXNmf/75Z77//vs6Q66IiMdl74SDa8HiD/2uMLsaERFxI6dHZqdOncqqVauIi4tj3rx5zJs3j7i4OFavXs1ll13mjhpFRJyz6QPjuccFENHO3FpERMStmtRndujQocyZM6fWvqNHj/Loo4/yt7/9zSWFiYg0WVA4hMXpxi8RkVagySuA/drGjRsZMmQIVqvVFadzG60AJtJKWCuMZ/9Ac+sQERGneWQFMBERr6YQKyLSKjRpBTAREa9UXgTpP4LNZnYlIiLiIQqzItJybF8Ib0+GtyaZXYmIiHiIw9MM7rzzzgbfz87ObnYxIiLNUt1btstZ5tYhIiIe43CYXb9+faPHjBkzplnFiIg0WX4WpC8xtgdeZWopIiLiOQ6H2cWLF7uzDhGR5tn8EdhtkDwSYruaXY2IiHiI5syKiO+z209Zvla9ZUVEWhOFWRHxfYc3w9E08A+GvpeaXY2IiHiQwqyI+L4dXxnPvSZAaIy5tYiIiEdp0QQR8X3n/BW6nQcBwWZXIiIiHqYwKyK+z2KB5OFmVyEiIiZo0jSDn376iWuuuYZRo0Zx8OBBAN555x2WLVvm0uJERBpls5pdgYiImMjpMPvJJ58wfvx4QkNDWb9+PWVlZQDk5eXx6KOPurxAEZF6lebBU6nwxe1QUWJ2NSIiYgKnw+w///lPXnrpJV555RUCAwNr9p955pmsW7fOpcWJiDQo7XMoPAz7VkBAiNnViIiICZwOszt27Khzpa/o6GhOnDjhippERByz8QPjeeDVxrxZERFpdZwOswkJCezevfu0/cuWLaNrV626IyIekrsXMn8GLNB/mtnViIiISZwOszfddBO33XYbq1atwmKxcOjQId59913uvvtuZsyY4Y4aRUROt2mu8dz1HIjuYG4tIiJiGqdbc917773YbDbOP/98iouLGTNmDMHBwdx99938+c9/dkeNIiK11Vq+9jfm1iIiIqay2O12e1M+WF5ezu7duyksLCQ1NZWIiAhX1+YW+fn5REdHk5eXR1RUlNnliEhT7F8Nr10AgeFw904I9o2/f0RExDHO5DWnR2bnzJnD5ZdfTlhYGKmpqU0uUkSkyaKS4Oy7wVapICsi0so5PTLbrl07SkpKmDx5Mtdccw3jx4/H39/fXfW5nEZmRURERLybM3nN6RvAsrKy+OCDD7BYLEybNo3ExERuueUWli9f3uSCRURERESaoslzZgGKi4v57LPPeO+99/juu+/o2LEje/bscWV9LqeRWfFqNitkLofCIxDRHjqPBj/f+c2HW1V/b9a+Ce36wOiZEKiFEkREWiK3zpk9VVhYGOPHjyc3N5fMzEy2bdvWnNOJtG5p82HRPZB/6OS+qCSY8DikTjavLm9Q1/dm9ctw8ZP63oiItHJOTzMAY0T23XffZeLEiXTo0IFnnnmGyy67jK1bt7q6PpHWIW0+zJ1eO6wB5GcZ+9Pmm1OXN6jve1OUre+NiIg4PzJ79dVXs2DBAsLCwpg2bRr3338/o0aNckdtIq2DzWqMOlLXjB87YIFF90Lvi1vflAN9b0REpBFOh1l/f3/mzp3rc10MRLxSaR6se+f0Ucda7JB/0JgvmnK2x0rzCpnL9b0REZEGOR1m3333XXfUIdLylRdDRQmEtzVeH1wLr5zn+Oe/fwhG/gl6XAjBkW4p0SvYrGAth8BQ40Y4Rzh6nIiItDgOhdnnnnuOP/zhD4SEhPDcc881eOytt97qksJEfFplGRzZCofWwaH1cHA9ZG+D4TfBxH8Zx7TrDRY/CIuDoqONn/PAGvj49+AfBF3HQp9LoNdECI9z79fiCZVlkP4jbP8CdnwFo26Bs+4wOjo4wtHjRESkxXGoNVdKSgpr1qyhbdu2pKSk1H8yi4X09HSXFuhqas0lp3FlO6yyQnjrEiPIWstPf7/HePjt3JOvS04Yo6zP9DNu9qpzbqgFwtvBoN/A9oVwfPcpb/lBp9Ew9VWISmxazWYpK4Bd38C2BbDrWygvOPle13Nh+ufGn01j35uoJLh9s+bMioi0IC5vzZWRkVHntojPc7Ydls0GOenGaGv1qGtEe5j2lvF+cAQUHDaCbGgMJA2BDkMgabCx/evAGdrGeJ7wuHFnPhZqhzaL8VTdgmrcLMjeAdu+MEYxszbC0TQj7Fbb/T1EdYB2vcBiad73x12slfBMfyjJPbkvIsG4kavPJdClav6rn3/j35sJjynIioi0Yk4vmvDwww9z9913ExYWVmt/SUkJTzzxBA888IBLC3S1Vj0yq4b8tVW3fDptxK8qJE17+2Sg/fFfsPcnOLQRyvJqHx4WB3/ZfTI4Zq4wQmubzs6FyTqDdQcjrNXXSzU30xip7X6+8dpuh6f7Qf4BaNvDCIa9JxmB2qxgm7vXGFE+vBkue+nk/g+vgSNpp9Q4FPzq6RbYlO+NiIj4LGfymtNh1t/fn6ysLOLj42vtP378OPHx8VitVucr9qBWG2bVkL+2ml9fN3CnfFSHk7++fmMiZP5s7A8IgYT+tUdd43q6Jiw29weOklz49I+Qvrj2NIeoDsaoZ/8rIXlE8+tsiN1uTLPYvsCYQnBk88n3Zq6FuO7GdnkRBIY5/n3TD2MiIq2GW1cAs9vtWOr4x2fjxo3ExsY6ezrxhPpGIKsb8p86AtlaNNryidotn0b8AQZMMwJsfB/wD3RPXX7+zWsxFRpjzMktzTfmo26vmo+afxBW/8+YY1sdZk/tGlAfZwPklk/h+1nGaGw1ix90PhN6XwJhp/wdERTu3NfW3O+NiIi0SA6H2ZiYGCwWCxaLhZ49e9YKtFarlcLCQm6++Wa3FCnNoKbzdXO25VPfS91WiluEREH/K4xHRSmkLzHm2Pa9/OQxmT/De1dDj3HGr/l7Xggh0Sffb2w0v7Ic9i6F2G4QW3VjaECwEWT9g6HbecYUgp4XnWxHJiIi4mIOh9lnnnkGu93O73//e2bNmkV09Ml/9IKCgujSpYtWAvNGajpft9bU8ikwBHpNMB6nSl8CFUWQ9rnx8AuErucYI6h+ATD/z9Q9mn8tJI+Co1uhLB/OvhvOv994v9t5cOVb0H2ccTOciIiImzkcZq+77jrAaNM1evRoAgPd9GtWcS01na9b59HGKGO9Qb+q5VPn0R4ty6POux/6TDI6I2xbAMd2wO7vjEe9qsLt/hXGc0T72tMUAkN9bxRbRER8mtNzZs8555ya7dLSUsrLa/fSbFU3VfmC1jQC6YxaLZ/q6WbQ0ls+WSxVLcMGw/kPQPZOYyrChvdq97Ktz8R/w7Ab6u9AICIi4gFO/ytUXFzMzJkziY+PJzw8nJiYmFoP8TIdhxvzF+tlMe50b8kjkPVJnWzc/BaVVHt/VFLrvCmuXU84+y449z7Hjg+NUZAVERHTOT0y+5e//IXFixfz4osvcu211/LCCy9w8OBBXn75ZR577DF31ChNZbfDF7eBtayhg1r+CGRd0uZDzwlGYO19sVo+nUqj+SIi4kOcDrNffPEFb7/9Nueeey6/+93vOPvss+nevTudO3fm3Xff5be//a076pSm+O4h2PQBWPyNde43vnf6HNE2naHXRaaUZ5qd3xg3MbXvBzf9YNyB35pufmtMzXziRpaQbY2j+SIi4nWc/h1hTk4OXbt2BYz5sTk5OQCcddZZLF261LXVSdOtehl+fsbYnvy8cbf57VvgugUw9TWY9g6EtIETmbD032ZW6lml+bDgDmO767lGkJXaqucTAzXzh2u0kvnEIiLiM5wOs127diUjIwOA3r17M3fuXMAYsW3Tpo1Li5NmiEkxVlc6734YXDVaXt10vv8Vxq/XL37S2P/Tv+HQBtNK9ajvZxlLvcZ0gbF/M7sa71Uznzix9v7WOp9YRES8ltPL2T799NP4+/tz66238t133zFp0iTsdjsVFRU89dRT3Hbbbe6q1SVa1XK2uXuNaQT1LRdqt8NH1xk9Rtv1gT/+2LJHKjOXwxtVUyqmzzd6qkrDtISsiIiYwJm85nSY/bXMzEzWrl1L9+7dGTBgQHNO5REtOswe2Wr0+Yzt6vhnio7Bi6NhwFVw3t9bbpitKIEXz4ScPTDkOpj8nNkViYiISD2cyWtO3wD2a507d6Zz587NPY0014l98M7lYKuE6+ZD+76OfS48DmauMZY/bcmWPmEE2chEuOBhs6sRERERF3E6zD73XN0jWhaLhZCQELp3786YMWPw99evIj2mOAfmTIXCw8Z0gV/3TW3MqUHWWgl2a8sboR18DexfDSP/BKFtzK5GREREXMTpaQYpKSlkZ2dTXFxcs0hCbm4uYWFhREREcPToUbp27crixYtJTk52S9HN0eKmGZQXw9tT4MBqY/GDG76F6A5NO9exXfDZzZB8Bkx41LV1egO7vf75wyIiIuI1nMlrTnczePTRRxk+fDi7du3i+PHjHD9+nJ07d3LGGWfw7LPPsm/fPhISErjjjjua/AWIg6yV8MkNRpANiYZrPml6kAXIyYCDa2Dlf42bflqCgsMntxVkRUREWhynR2a7devGJ598wqBBg2rtX79+PVOnTiU9PZ3ly5czdepUsrKyXFmrS7SYkVm7HRbcDmvfhIAQuHYedB7V/PN+fgusn2O0rrr5ZwiOaP45zZK9E14+25hiMH42BASZXZGIiIg4wK0js1lZWVRWVp62v7KyksOHjVGwpKQkCgoKnD21OKO8CA5vBosfTH3VNUEWYPyjENXRaOv13YOuOacZbDaYPxMqSyE3E/wDza5IRERE3MDpMDt27Fj++Mc/sn79+pp969evZ8aMGZx33nkAbN68mZSUFNdVKacLjoDrvoCr34c+k1x33pBomPIfY/uXVyF9ievO7Um/vAr7V0FQBFzytKYYiIiItFBOh9nXXnuN2NhYhg4dSnBwMMHBwQwbNozY2Fhee+01ACIiInjyySddXqwAeQdPbgeFQ68Jrr9Gt7Ew7AZj+/OZxhKwvuTEPvjuIWN73EPQxvtuRBQRERHXcLo1V0JCAt9++y3bt29n586dAPTq1YtevXrVHDN27FjXVSgn7f8F3poEZ90B5/zVvaONFzwMe76HkDZQkus7fWjtdvjidqgogk6jToZyERERaZGavGhC165dsVgsdOvWjYCAZq+9II3J3gnvXQmVJUbHAZsV/N34fQ+OgOmfG+2+fGm+6cYPjBDuHwyTnwc/p3/5ICIiIj7E6X/pi4uLueGGGwgLC6Nv377s27cPgD//+c889thjLi9QgPwsY1GEklzoMBSufNO9QbZaTJfaQbZ5Kx97RkAwhMbAufdCXA+zqxERERE3czrM3nfffWzcuJElS5YQEhJSs3/cuHF8+OGHLi1OgNI8ePdKyNsHsd3g/+Yac2U9qbIMvn/YaNvl7fpdDrf8AqP/bHYlIiIi4gFOD+/NmzePDz/8kJEjR2I5Zc5m37592bNnj0uLa/Uqy+CD38KRzRAeD9d+CuFxnq/jaBosexrsNug1Efpc4vkaGnPq6l4R7cytRURERDzG6ZHZ7Oxs4uPjT9tfVFRUK9yKC+z6Fvb+BEGRcM3Hxq/9zZA0GEbfamwvuB2KjptTR31KcuF/58K2BWZXIiIiIh7mdJgdNmwYCxcurHldHWBfffVVRo1yUeN+MfS5BCb/B656BxIHmlvL2L9Buz5QlA0L7zS3ll/75n7I2mAs8lBZbnY1IiIi4kFOTzN49NFHueiii0hLS6OyspJnn32WtLQ0li9fzo8//uiOGlsfmxX8/I3tIdeaW0u1gGC47EV45XxImwdbPoF+U82uyljUYf07xvbk/2jJWhERkVbG6ZHZs846iw0bNlBZWUn//v355ptviI+PZ8WKFQwdOtQdNbYuGz+E1y6AwmyzKzld0mAYc7exvfAuKDhibj3lRTC/avrD8Jtct6SviIiI+Iwm9Xfq1q0br7zyiqtrkd3fw+d/AlslrHvrZHD0JmffDTu+hOPpcHgzRLY3r5YfHoETmRDVEcY9aF4dIiIiYhqtduAtDm2AudONINtvKpzlZfNSqwUEwdTXjf6zsSnm1bH/F1j5X2N70jMQHGleLSIiImIah8Osn59fo90KLBYLlZWVzS6q1clJh3evgPJCSBkDl77o3StXtetpdgWw+zvADgOuhh4XmF2NiIiImMThMPvZZ5/V+96KFSt47rnnsNlsLimqpbDa7KzOyOFoQSnxkSGMSInF3+9XPxAUZhurexVlQ/v+cNW7xs1WviJjKWz+GCY9e7LPqyeMvQ86DocOQzx3TREREfE6DofZKVOmnLZvx44d3HvvvXzxxRf89re/5eGHH3Zpcb5s0ZYsZn2RRlZeac2+xOgQHpyUyoR+iScPnP9nY2Q2upPRSzYkyoRqm6joGLw7DSpLjFA59HrPXr/HOM9eT0RERLxOk36XfejQIW666Sb69+9PZWUlGzZs4K233qJz585OnWfp0qVMmjSJpKQkLBYL8+bNa/QzS5YsYciQIQQHB9O9e3fefPPNpnwJbrVoSxYz5qyrFWQBDueVMmPOOhZtyTq5c8Js6DjCWN0rMsHDlTZTeByc93dj++v/B7mZ7r2ezQrfPgD5h9x7HREREfEZToXZvLw87rnnHrp3787WrVv5/vvv+eKLL+jXr1+TLl5UVMTAgQN54YUXHDo+IyODiy++mLFjx7JhwwZuv/12brzxRr7++usmXd8drDY7s75Iw17He9X7Zn2RhtVW9So2BW74BuJ6eKpE1xo5AzqNMub7fn4LuHOqycoX4edn4bXxYK1w33VERETEZzg8zeBf//oXjz/+OAkJCbz//vt1Tjtw1kUXXcRFF13k8PEvvfQSKSkpPPnkkwD06dOHZcuW8fTTTzN+/Phm1+MKqzNyThuRPZUduLLwPXb+dJw+50wzdvryMsB+/nDpf+HFM42ld395Fc74g+uvk5MOP/zT2D7nL0Y3BREREWn1HA6z9957L6GhoXTv3p233nqLt956q87jPv30U5cV92srVqxg3Lja8yTHjx/P7bffXu9nysrKKCsrq3mdn5/vrvIAOFpQf5AFuNb/G+4M/Bjbks+g31Bo282t9XhEbFe44GH48m5jSdnu57v267LbjcURKkuMbg+DvWRVNBERETGdw2F2+vTpjbbmcrfDhw/Tvn3tJv3t27cnPz+fkpISQkNDT/vM7NmzmTVrlqdKJD4ypGbbDxsj/LYTzwmO0oYY8pkVYPwQcGDArXRqCUG22rAbYNt8o7vBprlGtwFXWfe2MeobEAqTnvPtkWwRERFxKYfDrDfeaOWI++67jzvvPLkAQX5+PsnJyW673oiUWBKjQxhYsJQHAt8myZJT857dbuSwD+3jmDrpfrfVYAo/P5jyAuxbBf2vcN158w/BN1U3mZ1/v7kLNYiIiIjX8akVwBISEjhy5EitfUeOHCEqKqrOUVmA4OBggoM917fV38/Cf4ccYODyZ057z2IxAu2Sir7sXLSDv1/cx/TRbpdq08l4uNLSJ6AsHzoMhTNudu25RURExOf5VJgdNWoUX375Za193377LaNGjTKpojrYrAze+hh2C9QZUy1wf+Aczlo2nKAAP/46vlfLCrTVinOM5WbPuRf8m/Gf2YX/hMAwGPRb42YzERERkVOYGmYLCwvZvXt3zeuMjAw2bNhAbGwsnTp14r777uPgwYO8/fbbANx888385z//4a9//Su///3v+eGHH5g7dy4LFy4060s4XeZyyD9Ud5DFCLhJluOM8NvOi0v8CPL3444LvGB5WFeyWeH18XBsJwSEwJi7m36uoHAY/4jrahMREZEWpUmLJrjKmjVrGDx4MIMHDwbgzjvvZPDgwTzwwAMAZGVlsW/fvprjU1JSWLhwId9++y0DBw7kySef5NVXX/WatlwAFB5p/Bjgj0PCAXj2+13854dd7qzI8/z84ey7jO0lj8HhLc6fY/f37u1ZKyIiIi2CxW6319Xfv8XKz88nOjqavLw8oqLcsHRsxk/w1iWNH3fdAl7el8Tsr7YDcN9FvfnjOS2ou4HdDh/8FnYshIT+cOMPEBDk2Gd3fQvvXgGdz4Tp85s3TUFERER8jjN5zdSR2Rap82iISqKeGbPG/qgO0Hk0fzynG3dfaEwxmP3Vdl5fluGxMt3OYoFJz0BoLBzeDD/927HPlRXAF7cb20mDFWRFRESkQQqzrubnDxMer3rx60Bb9XrCYzU3M808rwe3nm8sZfvwgjTeWbHXI2V6REQ8XGys1sbSf8Oh9Y1/5ruHIP8AxHSBsX9zZ3UiIiLSAijMukPqZJj2NkQl1t4flWTsT51ca/cd43pwc9UUg/s/38oHq/fRYvS7HPpeBnYrfP+Pho/NXG4shwvG4ghB4e6vT0RERHyafofrLqmToffFRkArPAIR7Y0pCHW0l7JYLNwzoRcVVhuvLcvgvs82E+DvxxVDO5pQuBtMfBLC4xseaa0ogc9nGttDroOu53imNhEREfFpCrPu5OcPKWc7dKjFYuHvF/ehwmrj7RWZ/PXjjQT6W5gyqIObi/SA8LYw8V8NH/Pj45CzByIT4YKHPVOXiIiI+DyFWS9isVh4aFJfKqx23l+9jzvnbiTQ34+J/RMb/7CvsNthyyfQYzxkbTg5at1nstGO69z7ILSN2VWKiIiIj1CY9TJ+fhYeubQfFVYbH689wK3vryfAz8KFfRPMLs01vrgV1r1tzIctLzq5PyoJxs+G3hPNq01ERER8jm4A80J+fhYenzqASwclUWmzc8t761i8/ajZZblGeHvj+dQgC5CfBR9dD2nzPV6SiIiI+C6FWS/l72fh31cO5OIBiVRY7fxxzlqW7sw2u6zmsVlh47v1vFm1dseie43jRERERBygMOvFAvz9eOaqQYzv257yShs3vb2G5buPmV1W02Uuh/xDDRxgh/yDxnEiIiIiDlCY9XKB/n48/5shnNc7nrJKGze8tYbVGTlml9U0hUdce5yIiIi0egqzPiAowI///nYIY3q2o6TCyu/eWM3azFyzy3JeRHvXHiciIiKtnsKsjwgJ9Od/1w5ldLe2FJVbuf711Ww6cMLsspzTebTRteC0ZX6rWSCqg3GciIiIiAMUZn1ISKA/r143jBEpsRSUVXLNq6vYcjDP7LIc5+cPEx6vevHrQFv1esJjda6SJiIiIlIXhVkfExYUwOvXD2do5xjySyu59rVVbD+cb3ZZjkudDNPehqhfLQQRlWTsT51sTl0iIiLikyx2u91udhGelJ+fT3R0NHl5eURFRZldTpPll1Zw7Wur2bj/BG3Dg/jgDyPp0T7S7LIcZ7MaXQuqVwDrPFojsiIiIgI4l9c0MuujokICeft3I+ibFMXxonL+79VVpGcXml2W4/z8IeVs6H+F8awgKyIiIk2gMOvDosMCmXPDGfROiCS7oIz/e2UVmceLGv+giIiISAuhMOvjYsKDmHPjGfSIj+Bwfin/98oqDuQWm12WiIiIiEcozLYAcRHBvHvTGXSNC+fgiRJ+88pKsvJKzC5LRERExO0UZluI+MgQ3rtpJJ3bhrE/p4T/e2UVR/JLzS5LRERExK0UZluQhGgj0HaMCSXjWBH/98pKsgvKzC5LRERExG0UZluYDm1Cef+mkSRFh7Anu4jfvrqS44UKtCIiItIyKcy2QMmxYbx300jiI4PZeaSQa15bzYnicqw2Oyv2HOfzDQdZsec4VlurajEsIiIiLZAWTWjBdh8t5Or/reRYYRmdYsMoq7RyJP/kKG1idAgPTkplQr/EBs4iIiIi4llaNEEA6B4fwXs3nUFEcAD7coprBVmAw3mlzJizjkVbskyqUERERKR5FGZbuG7tIggJrPuPuXpIftYXaZpyICIiIj5JYbaFW52Rw7HC8nrftwNZeaWszsjxXFEiIiIiLqIw28IdLXCs16yjx4mIiIh4E4XZFi4+MsSlx4mIiIh4E4XZFm5ESiyJ0SFY6nnfgtHVYERKrCfLEhEREXEJhdkWzt/PwoOTUgHqDLR24MFJqfj71Rd3RURERLyXwmwrMKFfIi9eM4SE6NOnEgT6W+iV0LL77YqIiEjLpUUTWhGrzc7qjByOFpTSLiKY/yzexfI9OQzp1IaPbh6t0VkRERHxCs7ktQAP1SRewN/PwqhubWted44LZ8LTS1m37wT/W5rOjHO7mVidiIiIiPM0zaAV69AmlAeq5tM+/e1Oth/ON7kiEREREecozLZyVwztyLg+8ZRbbdz54UbKK21mlyQiIiLiMIXZVs5isfDo5f2JCQskLSuf53/YZXZJIiIiIg5TmBXiI0P456X9Afjvkj1s2H/C3IJEREREHKQwKwBcPCCRyQOTsNrs3DV3A6UVVrNLEhEREWmUwqzUeHhKX+Ijg9mTXcQTX+8wuxwRERGRRinMSo02YUE8PnUAAK//nMHK9OMmVyQiIiLSMIVZqWVs73iuHp6M3Q53f7SRwrJKs0sSERERqZfCrJzm75ek0jEmlAO5JTyycJvZ5YiIiIjUS2FWThMRHMATVwwE4P3V+1i846jJFYmIiIjUTWFW6jSqW1t+f2YKAPd8vIkTxeUmVyQiIiJyOoVZqddfJ/Sia7twjhaU8eD8rWaXIyIiInIahVmpV0igP09NG4SfBT7fcIgvN2eZXZKIiIhILQqz0qBByW3407ndAfh/n20mu6DM5IpERERETlKYlUbden4P+iRGkVtcwX2fbsZut5tdkoiIiAigMCsOCArw46lpAwn0t/DdtiN8su6g2SWJiIiIAAqz4qA+iVHcPq4nALPmb+XQiRKTKxIRERFRmBUn/HFMVwZ3akNBWSV//XgTNpumG4iIiIi5FGbFYQH+fjx55UBCAv1YtvsY767KNLskERERaeUUZsUpXdtFcO+E3gA8+uV29h4rMrkiERERac0UZsVp00d1YVTXtpRUWLnro41YNd1ARERETKIwK07z87PwxJUDiAgOYG1mLq/+lG52SSIiItJKKcxKk3SMCeOBS1IBePKbnew4XGByRSIiItIaKcxKk105rCPn946n3GrjzrkbqLDazC5JREREWhmFWWkyi8XC7Mv70yYskK2H8nn+h91mlyQiIiKtjMKsNEt8VAj/mNIPgBcW72bTgRPmFiQiIiKtisKsNNukgUlcMiARq83OnXM3UlphNbskERERaSUUZsUl/jGlH+0ig9l9tJAnv9lhdjkiIiLSSijMikvEhAfx2OX9AXh1WQar0o+bXJGIiIi0Bgqz4jLn92nPtGEdsdvh7o83UlRWaXZJIiIi0sIpzIpL3X9JKh3ahLI/p4RHvtxmdjkiIiLSwinMiktFhgTyxJUDAHhv1T5+3JltckUiIiLSkinMisuN7hbH9aO7APDXjzeSV1xhbkEiIiLSYinMilvcM6E3XePCOZJfxkNfbDW7HBEREWmhFGbFLUKD/Pn3tIH4WeCz9QdZtCXL7JJERESkBVKYFbcZ0imGm8/pBsDfPtvCscIykysSERGRlkZhVtzqtnE96J0QSU5ROX/7dDN2u93skkRERKQFUZgVtwoO8OepaYMI9LfwTdoRPlt/0OySREREpAVRmBW3S02K4rbzewDw4PytZOWVmFyRiIiItBQKs+IRN5/TjYHJbSgoreSvH2/SdAMRERFxCYVZ8YgAfz+evHIgwQF+/LTrGO+szGTFnuN8vuEgK/Ycx2pTuBURERHnWeytbIgsPz+f6Oho8vLyiIqKMrucVuf1ZRk8vCANC3Dqf3iJ0SE8OCmVCf0SzSpNREREvIQzeU0js+JRCVEhQO0gC3A4r5QZc9apH62IiIg4RWFWPMZqs/OPhWl1vlcdbmd9kaYpByIiIuIwhVnxmNUZOWTlldb7vh3IyitldUaO54oSERERn6YwKx5ztKD+INuU40REREQUZsVj4iNDHDru3ZWZ7D5a4OZqREREpCVQmBWPGZESS2J0CJZGjlu9N5cLn17KXz7ayMETWmBBRERE6qcwKx7j72fhwUmpAKcFWkvV4/5LUrkgtT02O3y09gBjn1jCrC+2cqywzNPlioiIiA9Qn1nxuEVbspj1RVqtm8F+3Wd23b5cnli0gxXpxwEID/LnhrNSuHFMV6JCAk2pW0RERDzDmbymMCumsNrsrM7I4WhBKfGRIYxIicXfr/Z4rd1uZ9nuY/xr0Q42H8wDoE1YILec251rR3UmJNDfjNJFRETEzRRmG6Aw63vsdjuLthzm39/sYE92EWAsvnDbuB5cObQjAf6aLSMiItKSKMw2QGHWd1VabXy67iDPfLeTQ1VTFFLiwrnzgp5c3D8RP7/Gbi0TERERX6Aw2wCFWd9XWmHl3VX7eGHxbnKKygHomxTF3eN7cW7PdlgsCrUiIiK+TGG2AQqzLUdhWSWv/ZTBKz+lU1hWCcCILrH8dUIvhnWJNbk6ERERaSqF2QYozLY8OUXlvLhkN2+tyKS80gbA+b3juXt8L/ok6s9YRETE1yjMNkBhtuU6dKKE577fxUdrD2C12bFYYPLAJO68oCed24abXZ6IiIg4SGG2AQqzLV96diFPfruThZuyAAjws3DV8GRuPb8H7aMcW1JXREREzKMw2wCF2dZjy8E8nvh6Bz/uzAYgJNCP60Z3YcY53WgTFlRznCM9b0VERMRzFGYboDDb+qxKP86/vt7B2sxcACJDArj5nG787swuLN2Z3ehqZCIiIuJZCrMNUJhtnex2Oz9sP8oTX+9g++ECwAi1BaWVpx1bPSb74jVDFGhFRERM4Exe84qlk1544QW6dOlCSEgIZ5xxBqtXr6732DfffBOLxVLrERKieZDSMIvFwvl92vPlrWfzzFWDSI4JrTPIAlT/dDfrizSstlb1s56IiIjPMT3Mfvjhh9x55508+OCDrFu3joEDBzJ+/HiOHj1a72eioqLIysqqeWRmZnqwYvFlfn4WLh3cgdmX92/wODuQlVfK6owczxQmIiIiTWJ6mH3qqae46aab+N3vfkdqaiovvfQSYWFhvP766/V+xmKxkJCQUPNo3769ByuWluB41cphjflk7QH2HityczUiIiLSVKaG2fLyctauXcu4ceNq9vn5+TFu3DhWrFhR7+cKCwvp3LkzycnJTJkyha1bt3qiXGlB4iMdm5ry8boDnPvvJZzzxGIe/HwLP2w/QnF53dMTRERExPMCzLz4sWPHsFqtp42stm/fnu3bt9f5mV69evH6668zYMAA8vLy+Pe//83o0aPZunUrHTt2PO34srIyysrKal7n5+e79osQnzQiJZbE6BAO55VS36zYyJAA+iZGsXZfLpnHi3lrRSZvrcgkyN+PESmxnNOzHef0akeP+AgsFrXyEhERMYOpYbYpRo0axahRo2pejx49mj59+vDyyy/zj3/847TjZ8+ezaxZszxZovgAfz8LD05KZcacdVigVqCtjqVPXDGACf0SKSyrZPnuY/y4M5slO7I5eKKEZbuPsWz3MR75chtJ0SGc06sd5/Rsx+jucUSFBJrwFYmIiLROprbmKi8vJywsjI8//phLL720Zv91113HiRMn+Pzzzx06z5VXXklAQADvv//+ae/VNTKbnJys1lwCwKItWU71mbXb7aQfK+LHHdn8uDOblenHKau01bzv72dhaKeYmnCbmhiFnxZgEBERcYpP9Zk944wzGDFiBM8//zwANpuNTp06MXPmTO69995GP2+1Wunbty8TJ07kqaeeavR49ZmVX2vOCmClFVZWph/nx51GuE3Prn2zWFxEMGN6xnFOz3ac3aMdseFB9Zyp+bWIiIi0FD4VZj/88EOuu+46Xn75ZUaMGMEzzzzD3Llz2b59O+3bt2f69Ol06NCB2bNnA/Dwww8zcuRIunfvzokTJ3jiiSeYN28ea9euJTU1tdHrKcyKO+3PKa4Jtst3H6Oo3FrznsUCAzq24Zye7Ti3VzsGdmxTK6g6O0osIiLSUjmT10yfM3vVVVeRnZ3NAw88wOHDhxk0aBCLFi2quSls3759+PmdbLqQm5vLTTfdxOHDh4mJiWHo0KEsX77coSAr4m7JsWFcM7Iz14zsTHmljTWZOUa43ZHN9sMFbNx/go37T/Dc97uIDg3k7B7GqK3NZufeTzefdjPa4bxSZsxZp9XIRERE6mH6yKynaWRWzHI4r5Slu4xR2592ZpNfzwpkv2YBEqJDWHbPeZpyICIirYJPTTPwNIVZ8QaVVhsbD5zgxx3ZLNyUxR4HFma44awunNk9joSoUBKjQ2gTFuj2lmCawysiImZQmG2Awqx4m883HOS2DzY4/bmQQD8So0NJiAohMTqEhGjjOTE6tGY7NjyoyYFXc3hFRMQsPjVnVqS1c3Q1siGdYii3WjmcV8qxwnJKK2xkHCsio4FR3aAAv5qwawTe0Jrgm1QVetuGB53WPmzRlixmzFmnObwiIuL1FGZFTNbYamTVc2Y/unlUza/4yyqtHMkrIyuvhMP5pWTllXI4r5RDJ06+PlZYRnmljX05xezLKa73+oH+FtpHnQy7CVHBfPjL/jprsVfVM+uLNC5ITdCUAxERMZ3CrIjJHFmN7MFJqbWCY3CAP53ahtGpbVi95y2vtHG0wAi2RtgtORl6q14fLSijwmrnQG4JB3JLgNxG67UDWXmlrM7IYVS3tk35kkVERFxGc2ZFvIQZc1QrrDayC8qqAm8Jh/NKWbb7GEt2ZDf62V4JkVzcP5FhXWIYnBxDaJC/W2oUEZHWRzeANUBhVryZN3QPWLHnOL95ZaVTnwnws9CvQzTDu8QwrEsswzrH0DYi2E0ViohIS6cw2wCFWZGGWW12znr8hwbn8LaNCGbmed1Ym3mCXzJyOJxfetpx3dqFM7xLLMO6xDK8SwydYsPc3kpMRERaBoXZBijMijSuupsB1D2H99RuBna7Med2TWYOv+zNZc3eHHYeKTztnPGRwVXhNobhXWLpnRBJgL/facfVxxtGrUVExDMUZhugMCvimObM4c0tKmdtZi6/ZOawZm8umw6coMJa+6+aiOAABndqUxNwG5p3q563IiKti8JsAxRmRRznqtHQ0gorG/efYE1mLr/szWHt3lwKymov5/vrebfDu8QSGx5Ub8/bukaJRUSkZVCYbYDCrIj5rDY7Ow4X1ExNqG/ebde4MLLyyiipsNZ5nuoevMvuOU9TDkREWhCtACYiXs3fz0JqUhSpSVFMH9Wl3nm36cfqX+wBTva8XbzjKOP6tPdM8SIi4lU0MisiXulEcTkvLN7NKz9lOHR8dGggnWLDSI4NJTk2jOSYsKrXYXRoE0pQgOM3mzVGN6OJiLiXRmZFxOe1CQvivN7tHQ6zeSUVbD6Yx+aDeae952eBhKgQkmNPBtxTg2+7iGCH24bpZjQREe+ikVkR8VqO9LxNiA7h69vHkJVXyr6cYvbnFNc87881tksrbA1eJyTQj+SYsFphNzkmlE5tjRHe8GDj537djCYi4hkamRWRFsHfz8KDk1KZMWcdFuruefvgpFSiQgOJCg2kV0Lkaeew2+0cKyxnX04xB3KL2Xf8ZMjdn1NCVl4JpRU2dh0tZNfR0/vjArQND6JDTCg7jxTUGartVfXM+iKNC1ITNOVARMSDNDIrIl7Pnb/aL6+0kZVXwr6ckwH31FHdE8UVTp3vvot6c+ngDsRHOj51QUREalNrrgYozIr4JrNuusovrWB/TjGfrj3Iaz87Nn8XICokgJ7tI+nRPpJe7SNqtuMighRyRUQaoWkGItLi+PtZGNWtrcevGxUSSN+kaPJLKh0Ks4nRIRwtKCO/tJI1mbmsycyt9X5seBA94o1w2zMhkp5V2zHhQU2qT50VRKS1U5gVEXHAiJRYEqNDGr0Zbdk951Fps5GeXcTOIwVVj0J2HSkgM6eYnKJyVmXksCojp9bn20UG07N9BD3iI+nZPpJeCRH0aB9JVEhgvTWps4KIiKYZmF2OiPiQ6m4GUPfNaI11Mygpt7Inu7Am4FaH3QO5JfV+JiEqpNYIbo/2RshdtitbnRVEpMXSnNkGKMyKSHO4YzS0qKySXUeNcLvrSAE7qkZyT73Gr/lZwFbP395a5ldEfJ3CbAMUZkWkuTw1TzW/tIJdp4zi7jpSyI4jBWQXlDn0+VemD+WC1ASX1yUi4m4Ksw1QmBURX/f+6kzu+3SLQ8d2axfO0M4xNY+ucRH4abRWRLycuhmIiLRgXdpGOHzsnuwi9mQXMXfNAQCiQwMZ0qkNQzvHMKRzDAM7tqlZ4UxExBfpbzARER/jaGeF+TPPYtOBE6zNzGVtZi4bD5wgr6SCxTuyWbwjGzBanvVJjGRoJyPcDu0cQ4c2oeqFKyI+Q9MMRER8UFM6K1RYbWzLyq8Jt+syczlUx01m7aOCjZHbTka47ZsUTVCAn0N1qe+tiLiC5sw2QGFWRFoKV3RWOHSihHX7clmXeYK1+3LZejCPyl+1SQgK8GNgx2iGVAXcIZ1iaBcZ7JZ6RERAYbZBCrMi0pK4eiS0pNzK5oN5J0dv9+WSU1R+2nGd24bVmpqQkV3ELe+p762IuIbCbAMUZkVEHGe329l7vLjW1ISdRwv49b8cFqhz/m71e+p7KyLOUDcDERFxCYvFQkpcOClx4VwxtCMAeSUVbNh/oibcrtmbQ2mlrd5z2IGsvFIWbjrEpIFJurlMRFxKI7MiItIsn60/yB0fbnDo2KiQAFKTouibFE3fqudu7cIJ8HfsBjMRaR00MisiIh6TEBXi0HH+fpBfWsnK9BxWpufU7A8K8KN3QiR9k6JITYomNTGKPomRhAXpnygRaZz+phARkWZxtO/tD3edS/qxQrYeyiftUD5bD+WxLauAwrJKNh3IY9OBPGA/AH4WSIkLp29SdNVIrjGKGxse5HBdahMm0jpomoGIiDRbU/reAthsdvblFBsBNyuPrYfy2Xoon+yCsjqvkxAVUhVsjVHcvklRdIw5fZEHtQkT8W3qZtAAhVkREfdwZYA8WlBaNXp7chR37/HiOo89dR5uamIUuSXlPLJgm9qEifgwhdkGKMyKiLiPO3+1X1BawfbDBWw9eHIEd9fRAiqsjv8zpjZhIr5BN4CJiIgp/P0sjOrW1i3njgwJZHiXWIZ3ia3ZV15pY9fRgpoR3JV7jrP9SEG956huE/b+6n1cNTyZQHVREPF5GpkVEZEW4/MNB7ntgw0OHRsW5M/gTm0Y1jmWESmxDEpuQ3iwxnhEvIFGZkVEpFWKj3SsTVh4kD9F5VZ+3n2cn3cfB4xR5b5JUVWjvzEM6xJLXESwO8sVERdQmBURkRbD0TZhS/8ylvRjRfyyN4df9uawZm8uB0+U1LQIe21ZBgBd48IZ1iWmZnpD57ZhWsFMxMtomoGIiLQoTW0TdvBECWuqwu0vGbnsqGPubbvIYGPUtmpqQu+ESIdWL1PPWxHnqJtBAxRmRURaPle0CTtRXM7azFx+2ZvLmr05bDqQR7nVVuuYiOAABndqUzNyOyi5DaFB/i6vRaS1UZhtgMKsiEjr4OrR0NIKK5sO5NVMTVi7N5eCsspaxwT4WejXIZoRKbEM6xxDXkkFf/14k3reijhJYbYBCrMiIuIKVpudHYcLasLtL3tzOJJf98pldVHPW5H6qZuBiIiIm/n7WUhNiiI1KYrrRnfBbrdzILekJtj+uDObQydK6/18dc/b1RnHGdUtznOFi7QwGpkVERFxA0d73rYJDWRs73jOSInljK5t6aKOCSIamRURETGboz1vT5RU8Nn6g3y2/iAA7aOCOSOlLSO7tuWMrrF0jQtXuBVpgMKsiIiIGzjS87Z9VAj/mjqAXzJzWJWew4b9JziSX8b8jYeYv/EQYLQDqx61HdU1lm7tIhRuRU6haQYiIiJu4mzP29IKK+v25bIyPYdV6cdZv/8E5ZW124HFRQQxIiXWGLlNaUuP+Aj8dAOZtDDqZtAAhVkREfGk5vSZLa2wsmH/CVal57Aq4zhrM3Mp+1W4jQ0PYkSXWM7oagTcXu0jGwy3WsBBfIHCbAMUZkVExNNcFSDLKo1etyv3HGdVRg5rM3MpqbDWOqZNWGBVuG3LGSmx9EmMqrmWFnAQX6Ew2wCFWRERaSnKK21sPniClek5rEw3Rm6Ly2uH26iQAEakxNImNJCP1x087RxawEG8kcJsAxRmRUSkpaqw2thyMM+Yc5txnDV7cyn81SplddECDuJt1JpLRESkFQr092NwpxgGd4phxrndqLTa2Hoon7lr9vHuqv31fu7kAg45jOrW1nMFi7iAn9kFiIiIiHsE+PsxMLkNI1IcC6jPfLeTFXuOY7O1ql/aio/TyKyIiEgL5+gCDqsycvjNKyvp0CaUywZ34LIhHejWLsLN1Yk0j0ZmRUREWrjqBRzqmw1rAdqGB3HV8I5EhgRw8EQJ/1m8m/Of/JFLX/iZd1bsJbeo3JMlizhMN4CJiIi0Ao4u4FBaYeW7bUf4dN1BftyZjbVqykGgv4Xzesdz+ZCOjO0VT1CAxsPEfdTNoAEKsyIi0lo522c2u8BYWvfTdQfYeii/Zn+bsEAmD0zi8iEdGdgxWsvrisspzDZAYVZERFqzpi7gsP1wPp+tO8hn6w9ytKCsZn/XduFMHdKRSwd3oEObUHeWLq2IwmwDFGZFRESazmqz8/PuY3y67gCLth6mtOLk8rqjurbl8iEduKh/IhHBusdcmk5htgEKsyIiIq5RWFbJV5uz+HTdQVakH6/ZHxLox4S+CVw+pCNndo+rd+TXVcv8SsujMNsAhVkRERHXO5BbzOcbDvHJ2gOkHyuq2d8+KphLB3Xg8iEd6ZUQWbPf2fm70roozDZAYVZERMR97HY7Gw/k8em6A8zfeIgTxRU17/VNiuLyIR2JCA7g3k828esA8uvOCtJ6Kcw2QGFWRETEM8orbSzecZRP1x3gh+1HqbA2HjksQEJ0CMvuOc/jUw407cF7OJPXNDtbRERE3CIowI/xfRMY3zeB3KJyFmw6xJvL97Inu6jez9iBrLxSPt9wkPN7tycqNMAjrb807cF3aWRWREREPObzDQe57YMNDh8f4GchJjyItuFBtI0IIjY8mLbhQcRWPYz9wTXb0aGB+Dk5mlq9oISmPXgPjcyKiIiIV4qPDHHouJBAP0orbFTa7GQXlJF9Sm/bhvj7WYgJCzwZdiNOhl/juSr4RhivI0MCmfVF2mlBFoxRYgsw64s0LkhN0JQDL6UwKyIiIh4zIiWWxOgQDueV1hkgT50zW2G1kVtczvHCco4XlZNTVMbxwnJyiozH8ernwjKOF5VTUFqJ1WbnWGE5xwrLXVJv9bSH1Rk5jOrW1iXnFNdSmBURERGP8fez8OCkVGbMWYcFagXa6nHPByel4u9nwd/Pn8ToUBKjHVtZrLzyZPg1wm5ZTfA9VmiE4VND8KmdFhqzO7tAYdZLac6siIiIeJw33HBVabXx3bYj3DxnnUPH90mMYkyPOM7u0Y5hXWIICfR3c4Wtl1pzNUBhVkRExDt4Qyssq83OWY//UO+0B4BAf8tpbcVCAv04I6UtZ/eIY0zPdvSIj/BI14XWQmG2AQqzIiIicqrqbgZQ97SHF68ZwvAusSzbfYyfdh3jp13ZHMmvfUNaQlQIZ/eI4+ye7Tirexyx4UGeKb6FUphtgMKsiIiI/Joz0x7sdjs7jxTy065slu46xqr045RV2mret1igX1I0Y3oaUxKGdIohKMDPY19LS6Aw2wCFWREREalLU6c9lFZYWbM3l6W7slm6M5vthwtqvR8W5M+orienJKTEhTc6JcEbpmCYSWG2AQqzIiIi4k5H80tZtvsYS3dms2z3sdPahHVoE1ozantmtziiwwJrve8NN8eZTWG2AQqzIiIi4ik2m51th/Nr5tr+kpFLufXklAQ/CwxMbsPZPdoxpkccR/JLmfne+la/GpnCbAMUZkVERMQsxeWVrMrI4aedRrjddbSw1vu/7r376/eqF5Tw5JQDM6Y8KMw2QGFWREREvMWhEyUs23WMpbuyWbzjKEVl1kY/M75ve/okRhEbHkSbsCBiw4JoU7WEb0xYEKFBrut/a9aUB4XZBijMioiIiDf6bP1B7vhwQ7PPExzgdzLohgfSJiyImLDAqtAbVPVeIDGnbEcEB5x2U1p1yzIzpjw4k9e0nK2IiIiIF0iICnHouCkDkwgLDiC3qJzcYmNZ3pzick4Ul1NhtVNWaSMrr7TWaGpjAv0tNaE3JiyINqGBLN19rM4pD3aMQDvrizQuSE0wvcuCwqyIiIiIFxiREktidEi9q5FVz5l96qpBdQZIu91OYVklJ4oryC0uJ6eoKugWGUE395TQm1NUUfVcTtn/b+/eg6Iq/z+Av5fbLq6AAcmyCi4linJVuQj41XHkFzWOSmaQoZI0zpiaoA5qGthoCdpQXsNsnGymDO1XWt5FEkoBSXAzjMSKAc2W1UkB8Ybs+f3Rz83NRfSru8+uvl8zZ4Y9++ye935mPfOZx+ecvWlAe4eE863Xcb71+p0HNkMC8GfzNVTW/4XYp70e6HM/KDazRERERDbA0UGGJWMG4rVPq++4EOxW67pkzMBOZ0JlMhncFM5wUzjDz7PbPR/36o0O0+b3yg18V3ce/1t1tsvX6lvvffbXUtjMEhEREdmIZ0N8UTBp8B0XXakseNGVq4sjXF1coe7hatz3ZHf5PTWzPd3ubWmEJbGZJSIiIrIhz4b44n8GqoT+Ati9LnmIDvC0WqbOsJklIiIisjGODjKha1EfdMmDNTmIDkBEREREtufWkgeVh+lSApWHwqZ+iYwzs0RERERkli0seegKm1kiIiIi6pToJQ9d4TIDIiIiIrJbbGaJiIiIyG6xmSUiIiIiu8VmloiIiIjsFptZIiIiIrJbbGaJiIiIyG7ZRDO7fv16aDQaKBQKxMTEoLKy8q7jv/jiCwQFBUGhUCA0NBR79uyxUlIiIiIisiXCm9mtW7di7ty5WLJkCaqrqxEeHo7ExETo9Xqz48vKyjBx4kS8+uqrOH78OJKSkpCUlISamhorJyciIiIi0WSSJEldD7OcmJgYREVFYd26dQAAg8EAPz8/vP7661i4cOEd41NSUtDW1oZdu3YZ9w0dOhQRERHYsGFDl8draWmBh4cHmpub4e7u/vA+CBERERE9FPfTrwmdmb1x4waqqqqQkJBg3Ofg4ICEhASUl5ebfU15ebnJeABITEzsdPz169fR0tJishERERHRo0FoM3vhwgV0dHTAx8fHZL+Pjw90Op3Z1+h0uvsan5ubCw8PD+Pm5+f3cMITERERkXDC18xa2htvvIHm5mbjdubMGdGRiIiIiOghcRJ5cG9vbzg6OqKpqclkf1NTE1QqldnXqFSq+xovl8shl8sfTmAiIiIisilCZ2ZdXFwwZMgQFBcXG/cZDAYUFxcjNjbW7GtiY2NNxgNAUVFRp+OJiIiI6NEldGYWAObOnYu0tDRERkYiOjoaq1atQltbG6ZOnQoAmDJlCnr16oXc3FwAQEZGBkaMGIH8/HyMHj0ahYWFOHbsGDZu3CjyYxARERGRAMKb2ZSUFJw/fx45OTnQ6XSIiIjAvn37jBd5NTY2wsHhnwnkuLg4bNmyBW+++SYWLVqEwMBA7NixAyEhIaI+AhEREREJIvw+s9bG+8wSERER2Ta7uc8sEREREdGDYDNLRERERHaLzSwRERER2S3hF4BZ260lwvxZWyIiIiLbdKtPu5dLux67Zra1tRUA+LO2RERERDautbUVHh4edx3z2N3NwGAw4Ny5c3Bzc4NMJrP48VpaWuDn54czZ87w7gn/wtqYx7p0jrUxj3XpHGtjHuvSOdbGPGvXRZIktLa2Qq1Wm9yi1ZzHbmbWwcEBvXv3tvpx3d3d+Y+iE6yNeaxL51gb81iXzrE25rEunWNtzLNmXbqakb2FF4ARERERkd1iM0tEREREdovNrIXJ5XIsWbIEcrlcdBSbw9qYx7p0jrUxj3XpHGtjHuvSOdbGPFuuy2N3ARgRERERPTo4M0tEREREdovNLBERERHZLTazRERERGS32MwSERERkd1iM2th69evh0ajgUKhQExMDCorK0VHEi43NxdRUVFwc3NDz549kZSUhFOnTomOZXPy8vIgk8mQmZkpOopwf/zxByZNmgQvLy+4uroiNDQUx44dEx1LuI6ODmRnZyMgIACurq54+umnsWzZsnv6LfNHyXfffYcxY8ZArVZDJpNhx44dJs9LkoScnBz4+vrC1dUVCQkJOH36tJiwVna32rS3t2PBggUIDQ2FUqmEWq3GlClTcO7cOXGBraSr78ztpk+fDplMhlWrVlktn0j3Upva2lqMHTsWHh4eUCqViIqKQmNjo/XD/j82sxa0detWzJ07F0uWLEF1dTXCw8ORmJgIvV4vOppQpaWlmDlzJioqKlBUVIT29nY888wzaGtrEx3NZvzwww/48MMPERYWJjqKcBcvXkR8fDycnZ2xd+9e/Pzzz8jPz8cTTzwhOppwK1asQEFBAdatW4fa2lqsWLECK1euxNq1a0VHs6q2tjaEh4dj/fr1Zp9fuXIl1qxZgw0bNuDo0aNQKpVITEzEtWvXrJzU+u5WmytXrqC6uhrZ2dmorq7GV199hVOnTmHs2LECklpXV9+ZW7Zv346Kigqo1WorJROvq9r89ttvGDZsGIKCglBSUoITJ04gOzsbCoXCyklvI5HFREdHSzNnzjQ+7ujokNRqtZSbmyswle3R6/USAKm0tFR0FJvQ2toqBQYGSkVFRdKIESOkjIwM0ZGEWrBggTRs2DDRMWzS6NGjpfT0dJN948ePl1JTUwUlEg+AtH37duNjg8EgqVQq6d133zXuu3TpkiSXy6XPP/9cQEJx/l0bcyorKyUAUkNDg3VC2YDO6nL27FmpV69eUk1NjdSnTx/p/ffft3o20czVJiUlRZo0aZKYQJ3gzKyF3LhxA1VVVUhISDDuc3BwQEJCAsrLywUmsz3Nzc0AAE9PT8FJbMPMmTMxevRok+/O4+ybb75BZGQkXnzxRfTs2RODBg3CRx99JDqWTYiLi0NxcTHq6uoAAD/++CMOHz6M5557TnAy21FfXw+dTmfy78nDwwMxMTE8F5vR3NwMmUyGHj16iI4ilMFgwOTJk5GVlYXg4GDRcWyGwWDA7t270a9fPyQmJqJnz56IiYm56zINa2AzayEXLlxAR0cHfHx8TPb7+PhAp9MJSmV7DAYDMjMzER8fj5CQENFxhCssLER1dTVyc3NFR7EZv//+OwoKChAYGIj9+/fjtddew+zZs/HJJ5+IjibcwoUL8dJLLyEoKAjOzs4YNGgQMjMzkZqaKjqazbh1vuW5uGvXrl3DggULMHHiRLi7u4uOI9SKFSvg5OSE2bNni45iU/R6PS5fvoy8vDw8++yzOHDgAJ5//nmMHz8epaWlwnI5CTsyEf6ehaypqcHhw4dFRxHuzJkzyMjIQFFRkdi1RzbGYDAgMjISy5cvBwAMGjQINTU12LBhA9LS0gSnE2vbtm347LPPsGXLFgQHB0Or1SIzMxNqtfqxrw3dn/b2diQnJ0OSJBQUFIiOI1RVVRVWr16N6upqyGQy0XFsisFgAACMGzcOc+bMAQBERESgrKwMGzZswIgRI4Tk4syshXh7e8PR0RFNTU0m+5uamqBSqQSlsi2zZs3Crl27cOjQIfTu3Vt0HOGqqqqg1+sxePBgODk5wcnJCaWlpVizZg2cnJzQ0dEhOqIQvr6+GDhwoMm+AQMGCL1y1lZkZWUZZ2dDQ0MxefJkzJkzhzP7t7l1vuW5uHO3GtmGhgYUFRU99rOy33//PfR6Pfz9/Y3n4oaGBsybNw8ajUZ0PKG8vb3h5ORkc+dkNrMW4uLigiFDhqC4uNi4z2AwoLi4GLGxsQKTiSdJEmbNmoXt27fj22+/RUBAgOhINmHUqFH46aefoNVqjVtkZCRSU1Oh1Wrh6OgoOqIQ8fHxd9y6ra6uDn369BGUyHZcuXIFDg6mp3FHR0fj7AkBAQEBUKlUJufilpYWHD169LE/FwP/NLKnT5/GwYMH4eXlJTqScJMnT8aJEydMzsVqtRpZWVnYv3+/6HhCubi4ICoqyubOyVxmYEFz585FWloaIiMjER0djVWrVqGtrQ1Tp04VHU2omTNnYsuWLfj666/h5uZmXLfm4eEBV1dXwenEcXNzu2PdsFKphJeX12O9nnjOnDmIi4vD8uXLkZycjMrKSmzcuBEbN24UHU24MWPG4J133oG/vz+Cg4Nx/PhxvPfee0hPTxcdzaouX76MX3/91fi4vr4eWq0Wnp6e8Pf3R2ZmJt5++20EBgYiICAA2dnZUKvVSEpKEhfaSu5WG19fX0yYMAHV1dXYtWsXOjo6jOdjT09PuLi4iIptcV19Z/7d1Ds7O0OlUqF///7Wjmp1XdUmKysLKSkpGD58OEaOHIl9+/Zh586dKCkpERda9O0UHnVr166V/P39JRcXFyk6OlqqqKgQHUk4AGa3jz/+WHQ0m8Nbc/1t586dUkhIiCSXy6WgoCBp48aNoiPZhJaWFikjI0Py9/eXFAqF9NRTT0mLFy+Wrl+/LjqaVR06dMjsOSUtLU2SpL9vz5WdnS35+PhIcrlcGjVqlHTq1Cmxoa3kbrWpr6/v9Hx86NAh0dEtqqvvzL89TrfmupfabNq0Serbt6+kUCik8PBwaceOHeICS5Ikk6TH7KdiiIiIiOiRwTWzRERERGS32MwSERERkd1iM0tEREREdovNLBERERHZLTazRERERGS32MwSERERkd1iM0tEREREdovNLBERGZWUlEAmk+HSpUuioxAR3RM2s0REVnL+/Hm4uLigra0N7e3tUCqVaGxsND6v0Wggk8nu2PLy8gSmJiKybU6iAxARPS7Ky8sRHh4OpVKJo0ePGn/r/HZLly7FtGnTTPa5ublZMyYRkV3hzCwRkZWUlZUhPj4eAHD48GHj37dzc3ODSqUy2ZRKJYB/lgDs3r0bYWFhUCgUGDp0KGpqakze48svv0RwcDDkcjk0Gg3y8/NNnr9+/ToWLFgAPz8/yOVy9O3bF5s2bTIZU1VVhcjISHTr1g1xcXE4derUwywFEdFDw5lZIiILamxsRFhYGADgypUrcHR0xObNm3H16lXIZDL06NEDL7/8Mj744IN7fs+srCysXr0aKpUKixYtwpgxY1BXVwdnZ2dUVVUhOTkZb731FlJSUlBWVoYZM2bAy8sLr7zyCgBgypQpKC8vx5o1axAeHo76+npcuHDB5BiLFy9Gfn4+nnzySUyfPh3p6ek4cuTIQ6sLEdHDIpMkSRIdgojoUXXz5k2cPXsWLS0tiIyMxLFjx6BUKhEREYHdu3fD398f3bt3h7e3NzQaDf788084OzubvMfevXvxn//8ByUlJRg5ciQKCwuRkpICAPjrr7/Qu3dvbN68GcnJyUhNTcX58+dx4MAB4+vnz5+P3bt34+TJk6irq0P//v1RVFSEhISEO/LeOsbBgwcxatQoAMCePXswevRoXL16FQqFwoLVIiK6f1xmQERkQU5OTtBoNPjll18QFRWFsLAw6HQ6+Pj4YPjw4dBoNPD29jaOz8rKglarNdkiIyNN3jM2Ntb4t6enJ/r374/a2loAQG1t7R3LF+Lj43H69Gl0dHRAq9XC0dERI0aMuGvuW7PJAODr6wsA0Ov1/10RiIgsiMsMiIgsKDg4GA0NDWhvb4fBYED37t1x8+ZN3Lx5E927d0efPn1w8uRJ43hvb2/07dvXYnlcXV3vadzts8MymQwAYDAYLJKJiOhBcGaWiMiC9uzZA61WC5VKhU8//RRarRYhISFYtWoVtFot9uzZc9/vWVFRYfz74sWLqKurw4ABAwAAAwYMuGNt65EjR9CvXz84OjoiNDQUBoMBpaWlD/bBiIhsBGdmiYgsqE+fPtDpdGhqasK4ceMgk8lw8uRJvPDCC8b/vr9da2srdDqdyb5u3brB3d3d+Hjp0qXw8vKCj48PFi9eDG9vbyQlJQEA5s2bh6ioKCxbtgwpKSkoLy/HunXrjBeYaTQapKWlIT093XgBWENDA/R6PZKTky1XCCIiC+HMLBGRhZWUlCAqKgoKhQKVlZXo3bu32UYWAHJycuDr62uyzZ8/32RMXl4eMjIyMGTIEOh0OuzcuRMuLi4AgMGDB2Pbtm0oLCxESEgIcnJysHTpUuOdDACgoKAAEyZMwIwZMxAUFIRp06ahra3NYp+fiMiSeDcDIiI7cetOAxcvXkSPHj1ExyEisgmcmSUiIiIiu8VmloiIiIjsFpcZEBEREZHd4swsEREREdktNrNEREREZLfYzBIRERGR3WIzS0RERER2i80sEREREdktNrNEREREZLfYzBIRERGR3WIzS0RERER2i80sEREREdmt/wNvD17QH6yuDgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resume best model\n",
        "net.load_state_dict(best_params)\n",
        "best_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2BLKpn1UdqC",
        "outputId": "194eb194-7117-41c2-ef77-8cdeabd763a9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.71153846153847"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    }
  ]
}