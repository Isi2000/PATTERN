{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNc9/QNPG/+A1josTr9Ovrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isi2000/PATTERN/blob/main/PATTERN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gZHWFXQdUh2r"
      },
      "outputs": [],
      "source": [
        "#data an and plotting \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#torch \n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms.functional as Ft\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "\n",
        "#working with dirs\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this line of code copies the files in the PATTERN repo\n",
        "#they are hd image so it may take a while\n",
        "!git clone https://github.com/Isi2000/PATTERN.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb-Q4uKaDccv",
        "outputId": "0b053f03-e00c-42f1-deba-a9f69d55eac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PATTERN'...\n",
            "remote: Enumerating objects: 5887, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET"
      ],
      "metadata": {
        "id": "JGZytXHIB7d0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First the image paths and labels need to be storged in a pd DataFrame object to later be put in a custom made dataset "
      ],
      "metadata": {
        "id": "5vV305iXChEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {0: 'NORMAL', 1:'BACTERIAL', 2:'VIRAL',}\n",
        "\n",
        "data_dir_train = '/content/PATTERN/images/t_all'\n",
        "data_dir_test = '/content/PATTERN/images/test_all'\n",
        "data_dir_val = '/content/PATTERN/images/val_all'\n",
        "\n",
        "d_train = {'img_path': os.listdir(data_dir_train)}\n",
        "d_test = {'img_path': os.listdir(data_dir_test)}\n",
        "d_val = {'img_path': os.listdir(data_dir_val)}\n",
        "\n",
        "df_train = pd.DataFrame(d_train)\n",
        "df_test= pd.DataFrame(d_test)\n",
        "df_val = pd.DataFrame(d_val)\n",
        "\n",
        "df_train['label'] = 0\n",
        "df_test['label'] = 0\n",
        "df_val['label'] = 0\n",
        "\n",
        "#these lines work really well, BE CAREFUL BECAUSE IT GIVES A PD WARNING \n",
        "\n",
        "#DIOCANE\n",
        "df_train.loc[df_train['img_path'].str.contains('virus'), 'label'] = 2\n",
        "df_train.loc[df_train['img_path'].str.contains('bacte'), 'label'] = 1\n",
        "df_test.loc[df_test['img_path'].str.contains('virus'), 'label'] = 2\n",
        "df_test.loc[df_test['img_path'].str.contains('bacte'), 'label'] = 1\n",
        "df_val.loc[df_val['img_path'].str.contains('virus'), 'label'] = 2\n",
        "df_val.loc[df_val['img_path'].str.contains('bacte'), 'label'] = 1\n",
        "\n",
        "print(df_train.head())\n",
        "print(df_test.head())\n",
        "print(df_val.head())\n"
      ],
      "metadata": {
        "id": "OXK3l7yuAH-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVpr6KN3l4YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation and Custome Image Dataset. The class is written following to pytorch standards \n",
        "(for more info see https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ],
      "metadata": {
        "id": "S3hgRXZyB3Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stands for pixels, it's for resizing\n",
        "\n",
        "pix = 512\n",
        "\n",
        "transform_1 = transforms.Compose(\n",
        "    [transforms.ToPILImage(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Grayscale(),    \n",
        "     #the resizing must be fixed according to training perf\n",
        "     #224 is the standard size for resnets\n",
        "     transforms.Resize(size = (pix, pix), antialias = False),\n",
        "     transforms.Normalize(mean=[0.0], std=[1.0]),\n",
        "     #this one is for resnet\n",
        "     transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
        "     ])\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, pd_df, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "train_ = CustomImageDataset(df_train, data_dir_train, transform = transform_1)\n",
        "test_ = CustomImageDataset(df_test, data_dir_test, transform = transform_1)\n",
        "val_ = CustomImageDataset(df_val, data_dir_val, transform = transform_1)"
      ],
      "metadata": {
        "id": "RnsmFhcMU8nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization of the dataset"
      ],
      "metadata": {
        "id": "uK_qEHGNDEWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotter(data):      \n",
        "    \"\"\"this plots 9 images from a dataset\"\"\"\n",
        "    figure = plt.figure(figsize = (8,8))\n",
        "    cols, rows = 5, 5\n",
        "    for i in range(1, cols * rows + 1):\n",
        "        sample_idx = torch.randint(len(data), size=(1,)).item()\n",
        "        img, label = Ft.to_pil_image(data[sample_idx][0]) , data[sample_idx][1]\n",
        "        print(img.size)\n",
        "        figure.add_subplot(rows, cols, i)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(labels_map[label], fontsize = 10)\n",
        "        plt.imshow(img)        \n",
        "    plt.show()\n",
        "\n",
        "plotter(train_)"
      ],
      "metadata": {
        "id": "rUUIrqVjBQGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network and GPU"
      ],
      "metadata": {
        "id": "-_XmqFODIAFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below tells the program to use gpu if available. All the code written is designed to work even if the machine doesn't have a gpu"
      ],
      "metadata": {
        "id": "Dn4lKp0SIvAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Congratulations, you have a GPU!\")\n",
        "else:\n",
        "    print(\"PyTorch cannot see your GPU :(\")\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "d66qn5oSDmuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Googlenet\n",
        "The Neural Network chosen is googlenet. It is a NN that performs really well on computer vision problem. It takes as input 3 rgb images and it classifies them into n classes "
      ],
      "metadata": {
        "id": "5LzGn9zEKDNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import googlenet\n",
        "\n",
        "net = googlenet(num_classes = 3, weights=False, aux_logits=False ).to(torch.device('cuda')) #weights=False implies that it isn't pretrained"
      ],
      "metadata": {
        "id": "IUjmX0jFOac9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input channels can be fixed with a simple transformation. For what concerns the output, the output of the net can be connected to a fully connected layer with label 3 outputs"
      ],
      "metadata": {
        "id": "Csg1-uPUv2vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(net, (3, pix, pix), device = 'cuda')"
      ],
      "metadata": {
        "id": "xdfK2Gl4-Vo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing"
      ],
      "metadata": {
        "id": "NXe9UXkJUToU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "m = nn.LogSoftmax(dim=1)\n",
        "\n",
        "#Adam may be too strong\n",
        "optimizer = optim.AdamW(net.parameters(), lr=1e-6, weight_decay=1e-5)\n",
        "\n",
        "train_loader = DataLoader(train_, batch_size=50, shuffle=True, num_workers = 2)\n",
        "test_loader = DataLoader(test_, batch_size=50, shuffle=True, num_workers = 2)\n",
        "val_loader = DataLoader(val_, batch_size=10, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, freq=20):\n",
        "    model.train()\n",
        "    epoch_loss = 0    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(m(output), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % freq == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Learning rate after epoch {epoch}: {optimizer.param_groups[0]['lr']}\")    \n",
        "    return epoch_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(m(output), target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)  \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    return test_loss, (100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "q9EfRI2INCXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOSS\n",
        "\n",
        "NOTE: as suggested by karphaty it is always usefull to check inital loss.\n",
        "With softmax and 3 classes we expect initial loss to be around -ln(0.33) = 1.1, this is fully compatible with what we observe with a pretrained net"
      ],
      "metadata": {
        "id": "aAlaAOEyjnDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "num_epochs = 15\n",
        "best_params = net.state_dict()\n",
        "best_accuracy = 0\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Train Epoch: {}'.format(epoch))\n",
        "    train_losses.append(train(net, device, train_loader, optimizer, epoch))\n",
        "    test_loss, test_accuracy = test(net, device, test_loader)\n",
        "\n",
        "    # model selection\n",
        "    if test_accuracy >= best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_params = net.state_dict()\n",
        "\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "metadata": {
        "id": "6Km7VZjCNQY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# RESULTS"
      ],
      "metadata": {
        "id": "9Wmid72nUZso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "a = []\n",
        "for i in test_accuracies:\n",
        "  a.append(i / 100)\n",
        "\n",
        "plt.plot(train_losses, marker='o', label='loss (train)')\n",
        "plt.plot(test_losses, marker='o', linestyle='dashed', label='loss (test)')\n",
        "plt.plot(a, marker='o', linestyle='dashed', label=' (test)')\n",
        "\n",
        "\n",
        "\n",
        "plt.xlabel('#Epoch')\n",
        "plt.ylabel('Negative Log-Likelihood')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OycAo2WDTuqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resume best model\n",
        "net.load_state_dict(best_params)\n",
        "best_accuracy"
      ],
      "metadata": {
        "id": "-2BLKpn1UdqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since this validaion is basically additional testing I think I can keep the same function\n",
        "test(net, device , val_loader )\n",
        "#this tells me that there might me something fishy"
      ],
      "metadata": {
        "id": "gUJIlTJxkjtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#u still need to do confusion matrix and use it maybe as a way to minimize something during training"
      ],
      "metadata": {
        "id": "7c848YMEHned"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import seaborn as sns\n",
        "\n"
      ],
      "metadata": {
        "id": "u8Z_qGz-Ghmt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}